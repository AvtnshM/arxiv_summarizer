category,title,summary,link,published,authors,summary_short
cs.LG,OmniX: From Unified Panoramic Generation and Perception to   Graphics-Ready 3D Scenes,"There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",http://arxiv.org/abs/2510.26800v1,2025-10-30T17:59:51Z,"Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu","**Breakthrough in 3D Scene Creation: OmniX Revolutionizes Virtual World Generation**

Imagine being able to create immersive and realistic virtual worlds with ease. Researchers have made a significant step towards achieving this goal with the development of OmniX, a unified framework that generates high-quality 3D scenes from 2D images. This innovation has the potential to transform industries such as gaming, architecture, and film production.

**The Problem with Current 3D Scene Creation Methods**

Currently, there are two main ways to construct 3D scenes: procedural generation and 2D lifting. Procedural generation involves using algorithms to create 3D models from scratch, while 2D lifting involves converting 2D images into 3D scenes. However, both methods have limitations. Procedural generation can be time-consuming and may not produce realistic results, while 2D lifting often focuses on appearance rather than the underlying geometry and materials of the scene.

**How OmniX Works**

OmniX addresses these limitations by leveraging powerful 2D generative models to produce 3D scenes that are not only visually stunning but also physically accurate. This means that the generated scenes can be lit, rendered, and simulated in a way that mimics real-world physics. The key insight behind OmniX is to repurpose 2D generative models for panoramic perception of geometry, textures, and materials.

**Key Features of OmniX**

* **Graphics-ready 3D scenes**: OmniX generates 3D scenes that are suitable for physically based rendering, relighting, and simulation.
* **Panoramic perception**: OmniX can perceive and understand the geometry, textures, and materials of a scene from a single 2D image.
* **Efficient and versatile**: OmniX uses a lightweight and efficient cross-modal adapter structure, making it suitable for a broad range of panoramic vision tasks.

**Impact and Future Possibilities**

The implications of this research are significant, enabling the creation of:

* Highly realistic and immersive virtual environments for gaming, training, and simulation
* Accurate and detailed 3D models for architecture, product design, and urban planning
* Efficient and cost-effective workflows for film and video production

The development of OmniX marks an exciting milestone in the field of computer vision and graphics, opening up new possibilities for the creation of virtual worlds that are indistinguishable from reality."
cs.LG,Scaling Image Geo-Localization to Continent Level,"Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (>100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.",http://arxiv.org/abs/2510.26795v1,2025-10-30T17:59:35Z,"Philipp Lindenberger, Paul-Edouard Sarlin, Jan Hosang, Matteo Balice, Marc Pollefeys, Simon Lynen, Eduard Trulls","**Advances in Image Geo-Localization: A Breakthrough in Identifying Locations Across Continents**

Imagine being able to pinpoint the exact location of a photo taken anywhere in the world, from a bustling city street to a scenic countryside road. Researchers have made a significant breakthrough in achieving this goal, developing a new method that can accurately identify the location of an image across a vast geographic area, such as a continent.

The challenge lies in the sheer volume of images available (over 100 million) and the limited coverage of certain areas. Existing techniques are either inefficient or provide coarse results, often accurate to within several kilometers. The new approach, however, combines two key strategies to achieve fine-grained geo-localization.

By leveraging a proxy classification task during training, the researchers learned rich feature representations that encode precise location information. They then combined these learned prototypes with aerial imagery embeddings to increase robustness to sparse ground-level data. This enables direct, fine-grained retrieval over large areas, spanning multiple countries.

In an extensive evaluation, the researchers demonstrated that their approach can localize images within 200 meters of their actual location more than 68% of the time, across a dataset covering a significant part of Europe. This achievement has significant implications for various applications, including image retrieval, mapping, and surveillance.

The code for this new approach is publicly available, making it a valuable resource for researchers and developers working on image geo-localization tasks. This breakthrough brings us closer to accurately identifying the location of images taken anywhere in the world, with potential applications in fields such as environmental monitoring, urban planning, and more."
cs.LG,"Learning Pseudorandom Numbers with Transformers: Permuted Congruential   Generators, Curricula, and Interpretability","We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.",http://arxiv.org/abs/2510.26792v1,2025-10-30T17:59:09Z,"Tao Tao, Maissam Barkeshli","**Can AI Learn to Generate Random Numbers?**

Researchers have been testing the abilities of a type of artificial intelligence (AI) called Transformer models to learn and predict sequences of numbers generated by complex algorithms called pseudo-random number generators (PRNGs). These algorithms are used to create seemingly random numbers for various applications, such as simulations, modeling, and cryptography.

The researchers found that Transformer models can successfully learn and predict sequences from different types of PRNGs, even when the output is limited to a single bit (a basic unit of digital information). This is impressive, as these PRNGs use complex operations like bit-wise shifts, XORs, rotations, and truncations to generate numbers.

The study also showed that when the AI model is trained on multiple PRNGs simultaneously, it can identify common structures and learn to predict sequences from each of them. However, as the complexity of the PRNGs increases, the model requires more data and computational power to learn effectively.

Interestingly, the researchers discovered that the AI model groups integer inputs into clusters based on their rotational symmetry, which allows it to transfer knowledge from smaller to larger PRNGs. This finding has implications for the design of more efficient training methods.

The study's results have significant implications for the development of AI models that can learn and generate complex patterns, and could potentially lead to breakthroughs in areas like cryptography, simulations, and data analysis."
cs.LG,Defeating the Training-Inference Mismatch via FP16,"Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",http://arxiv.org/abs/2510.26788v1,2025-10-30T17:58:11Z,"Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin","**Improving Stability in AI Training: A Simple yet Effective Solution**

Researchers have identified a major issue in fine-tuning large language models using reinforcement learning (RL). The problem, known as the ""training-inference mismatch,"" causes instability and inconsistent results. While previous solutions have attempted to fix this issue through complex algorithmic changes, this study reveals that the root cause lies in the way computers store and process numbers.

The researchers found that using a lower precision floating-point format, called FP16, can effectively eliminate this mismatch. Surprisingly, switching to FP16, which is a simple change that requires only a few lines of code, leads to more stable optimization, faster convergence, and better performance across various tasks and AI frameworks.

The implications of this study are significant, as it suggests that a simple change in the way computers process numbers can have a substantial impact on the stability and performance of AI models. The researchers hope that their findings will encourage a re-evaluation of the trade-offs between precision and performance in AI training, leading to more efficient and effective AI development."
cs.LG,Remote Labor Index: Measuring AI Automation of Remote Work,"AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",http://arxiv.org/abs/2510.26787v1,2025-10-30T17:58:04Z,"Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks","Here's a summary of the research paper for a general audience:

**Can AI Replace Human Workers Remotely?**

Researchers have made significant progress in developing artificial intelligence (AI) that can perform complex tasks. However, it's unclear how well these AI systems can actually automate real-world work, especially remote jobs. To find out, researchers created a new benchmark called the Remote Labor Index (RLI).

The RLI tests AI agents on real-world projects that require a range of skills, such as knowledge and reasoning. The results show that current AI agents are not very effective at automating remote work, with the best-performing AI achieving an automation rate of only 2.5%. This means that AI can only perform a small fraction of the tasks that humans do remotely.

These findings provide a reality check on the potential impact of AI on work and help set a baseline for tracking progress in AI automation. By understanding the current limitations of AI, stakeholders can better prepare for the changes that AI-driven automation may bring to the workforce."
cs.LG,HEIR: Learning Graph-Based Motion Hierarchies,"Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",http://arxiv.org/abs/2510.26786v1,2025-10-30T17:57:40Z,"Cheng Zheng, William Koch, Baiang Li, Felix Heide","**Unlocking the Secrets of Motion: A New Approach to Understanding Complex Movement**

Imagine you're watching a dancer perform a intricate routine. Their movement is made up of many smaller parts - the swing of their arms, the twirl of their body, and the tap of their feet. But how do we make sense of all these individual movements and how they work together to create the overall dance?

Researchers have long struggled to model complex motion, like the movement of a dancer or the deformation of a 3D object. Existing methods rely on pre-defined rules or assumptions about how the different parts of the motion fit together. But what if we could teach a computer to learn these relationships directly from data, without any preconceptions?

That's exactly what a team of researchers has done with their new method, called HEIR (Hierarchical Motion Modeling). By representing motion as a graph, where each node represents a simple movement and the edges represent the relationships between them, HEIR can learn to identify the underlying structure of complex motion.

In tests, HEIR was able to accurately reconstruct the motion of simple systems, like a ball moving in one or two dimensions. But it also showed promising results on more complex tasks, like deforming 3D objects. By providing a flexible and data-driven approach to motion modeling, HEIR has the potential to be applied to a wide range of fields, from computer vision and robotics to animation and beyond.

**Key Takeaways:**

* A new method, HEIR, learns to model complex motion by identifying the relationships between simple movement components.
* HEIR uses a graph-based approach to represent motion, allowing it to learn from data without pre-defined rules.
* The method shows promising results on a range of tasks, from simple motion to complex 3D deformations."
cs.LG,A Unified Theory for Causal Inference: Direct Debiased Machine Learning   via Bregman-Riesz Regression,"This note introduces a unified theory for causal inference that integrates Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted maximum likelihood estimation (TMLE), and the matching estimator in average treatment effect (ATE) estimation. In ATE estimation, the balancing weights and the regression functions of the outcome play important roles, where the balancing weights are referred to as the Riesz representer, bias-correction term, and clever covariates, depending on the context. Riesz regression, covariate balancing, DRE, and the matching estimator are methods for estimating the balancing weights, where Riesz regression is essentially equivalent to DRE in the ATE context, the matching estimator is a special case of DRE, and DRE is in a dual relationship with covariate balancing. TMLE is a method for constructing regression function estimators such that the leading bias term becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density Ratio Estimation and Riesz Regression.",http://arxiv.org/abs/2510.26783v1,2025-10-30T17:56:47Z,Masahiro Kato,"**Unlocking the Secrets of Causal Inference: A New Unified Theory**

Imagine you're trying to figure out whether a new medicine actually works, or if a certain policy change has a positive impact on society. To do this, researchers use a technique called causal inference. A new study introduces a unified theory that brings together several existing methods for causal inference into one cohesive framework.

**The Goal: Estimating Cause-and-Effect Relationships**

The goal of causal inference is to estimate the average treatment effect (ATE), which is the difference in outcomes between two groups: one that receives a treatment (e.g., a medicine) and one that doesn't. The challenge is to ensure that the comparison is fair and accurate.

**The Key Players: Balancing Weights and Regression Functions**

The study identifies two crucial components in ATE estimation: balancing weights and regression functions. Balancing weights help to create a fair comparison between the two groups, while regression functions predict the outcome of interest.

**The Unified Theory: Connecting Different Methods**

The new theory shows that several existing methods are connected and can be seen as different approaches to estimating balancing weights and regression functions. These methods include:

1. **Riesz regression**: a way to estimate balancing weights
2. **Covariate balancing**: another approach to estimating balancing weights
3. **Density-ratio estimation**: a method that's equivalent to Riesz regression
4. **Targeted maximum likelihood estimation**: a method for constructing regression function estimators
5. **Matching estimator**: a special case of density-ratio estimation

**Implications and Applications**

This unified theory has significant implications for researchers and practitioners in various fields, including medicine, social sciences, and policy-making. By providing a comprehensive framework for causal inference, it can help to:

* Improve the accuracy of cause-and-effect estimates
* Facilitate the comparison of different methods
* Guide the choice of methods for specific research questions

In summary, the study introduces a unified theory for causal inference that integrates multiple existing methods, providing a more comprehensive and coherent framework for understanding cause-and-effect relationships."
cs.LG,Clone Deterministic 3D Worlds with Geometrically-Regularized World   Models,"A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",http://arxiv.org/abs/2510.26782v1,2025-10-30T17:56:43Z,"Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen","**Breakthrough in Artificial Intelligence: Creating a More Accurate Virtual World**

Imagine a computer program that can predict and simulate the world around us, much like a video game. This program, called a ""world model,"" is crucial for developing intelligent agents that can think, plan, and make decisions in complex environments. However, current world models have limitations and can become inaccurate over time.

Researchers have made a significant step forward in creating a more accurate world model by developing a new approach called Geometrically-Regularized World Models (GRWM). This approach focuses on improving the way the program represents the world, making it more similar to the real world.

**The Problem: Inaccurate Representations**

The researchers identified that one of the main challenges in creating accurate world models is representing the world in a way that's easy for the program to understand. The world is made up of many complex and high-dimensional inputs, such as images, which can be difficult for the program to process.

**The Solution: GRWM**

GRWM addresses this challenge by ensuring that the program's representation of the world remains consistent and accurate over time. This is achieved by adding a geometric regularization technique that encourages the program to keep track of the relationships between different parts of the environment.

**The Results: Improved Accuracy and Stability**

The researchers tested GRWM in various 3D environments and found that it significantly improved the accuracy and stability of long-term predictions. This means that the program can now simulate the world more accurately and make more reliable predictions about what will happen in the future.

**The Impact: More Reliable AI Systems**

The findings of this study have important implications for the development of more reliable and accurate AI systems. By improving the representation of the world, GRWM provides a direct and useful path to creating more robust world models, which can be used in a wide range of applications, from robotics to autonomous vehicles."
cs.LG,Surpassing state of the art on AMD area estimation from RGB fundus   images through careful selection of U-Net architectures and loss functions   for class imbalance,"Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.",http://arxiv.org/abs/2510.26778v1,2025-10-30T17:55:46Z,"Valentyna Starodub, Mantas Lukoševičius","**Breakthrough in Detecting Age-Related Macular Degeneration from Retina Images**

Age-related macular degeneration (AMD) is a leading cause of vision loss in people over 60. A team of researchers has made significant progress in developing a computer-based system to detect AMD from retina images. They used a type of artificial intelligence called U-Net to analyze images of the retina taken with a non-invasive and cost-effective technique called RGB fundus imaging.

The researchers tested different approaches to improve the accuracy of their system, including pre-processing techniques, different types of deep learning models, and specialized algorithms to handle imbalances in the data. Their final system outperformed all previous submissions to the ADAM challenge, a comprehensive research competition and open dataset for AMD detection.

The achievement is significant because it could lead to earlier detection and treatment of AMD, potentially preventing vision loss. The researchers have made their source code freely available, which could facilitate further research and development in this area. Overall, this study demonstrates the potential of AI to improve the diagnosis and treatment of AMD and other eye diseases."
cs.LG,Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for   Time Series Classification,"Recent research on time series foundation models has primarily focused on forecasting, leaving it unclear how generalizable their learned representations are. In this study, we examine whether frozen pre-trained forecasting models can provide effective representations for classification. To this end, we compare different representation extraction strategies and introduce two model-agnostic embedding augmentations. Our experiments show that the best forecasting models achieve classification accuracy that matches or even surpasses that of state-of-the-art models pre-trained specifically for classification. Moreover, we observe a positive correlation between forecasting and classification performance. These findings challenge the assumption that task-specific pre-training is necessary, and suggest that learning to forecast may provide a powerful route toward constructing general-purpose time series foundation models.",http://arxiv.org/abs/2510.26777v1,2025-10-30T17:55:23Z,"Andreas Auer, Daniel Klotz, Sebastinan Böck, Sepp Hochreiter","Here's a summary of the research paper for a general audience:

**Can Forecasting Models Help with Time Series Classification?**

Researchers have been exploring the use of pre-trained models for making predictions about future events, such as stock prices or weather forecasts. But can these models also be used for other tasks, like classifying time series data (e.g., identifying patterns in sensor readings or medical data)?

In this study, the researchers found that pre-trained forecasting models can actually be very effective for time series classification, even if they weren't specifically designed for that task. They compared different methods for extracting useful information from these pre-trained models and introduced new techniques to improve their performance.

The surprising result was that the best forecasting models performed just as well, or even better, than models that were specifically pre-trained for classification. This suggests that learning to forecast can provide a strong foundation for understanding time series data, and could lead to the development of more general-purpose models that can be used for a wide range of tasks.

**In simple terms:** Pre-trained forecasting models, which are designed to predict future events, can also be used to classify time series data with high accuracy. This challenges the idea that task-specific pre-training is necessary, and opens up new possibilities for developing more versatile models."
cs.LG,Faithful and Fast Influence Function via Advanced Sampling,"How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.",http://arxiv.org/abs/2510.26776v2,2025-10-30T17:55:19Z,"Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang","**Explaining How Training Data Affects Black-Box Models**

Imagine you're training a machine learning model, like a chatbot or image classifier, on a large dataset. But have you ever wondered how individual data points affect the model's behavior? Researchers have developed a method called influence functions (IFs) to help answer this question. However, computing IFs for entire datasets can be computationally expensive.

To make IFs more efficient, researchers propose two new sampling techniques that select a small, representative subset of the training data. These techniques, based on features and logits, help reduce the variability in IF estimates and improve their accuracy.

In experiments, the new methods reduced computation time and memory usage by 30.1% and 42.2%, respectively, or improved the model's performance by 2.5%. This work enables faster and more accurate analysis of how training data influences black-box models, which can be useful for understanding and improving AI systems."
cs.LG,STaMP: Sequence Transformation and Mixed Precision for Low-Precision   Activation Quantization,"Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models. However, accuracy often degrades sharply when activations are quantized below eight bits. Recent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. In this paper, we propose \textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a novel strategy that applies linear transformations along the \textit{sequence} dimension to exploit the strong local correlation in language and visual data. By keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths. We evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.",http://arxiv.org/abs/2510.26771v1,2025-10-30T17:53:42Z,"Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel","**Breakthrough in AI Model Efficiency: STaMP Revolutionizes Low-Precision Activation Quantization**

Artificial intelligence (AI) models are becoming increasingly powerful, but they also require significant computational resources, memory, and energy. To make AI more efficient, researchers use a technique called quantization, which reduces the precision of the model's calculations. However, this often comes at the cost of accuracy. A new method, called Sequence Transformation and Mixed Precision (STaMP), has been developed to overcome this challenge.

STaMP applies a clever mathematical transformation to the data processed by AI models, taking advantage of the fact that many types of data, such as text and images, have strong local correlations. By keeping a small portion of the data at higher precision and applying transformations to the rest, STaMP enables AI models to maintain their accuracy even when using lower-precision calculations.

In tests, STaMP significantly improved the performance of AI models that process language and visual data, such as large language models and vision models. This innovation has the potential to make AI models more efficient, reducing their energy consumption, memory requirements, and computational needs, while maintaining their accuracy. This breakthrough could have significant implications for the development of more sustainable and accessible AI systems."
cs.LG,SteerVLM: Robust Model Control through Lightweight Activation Steering   for Vision Language Models,"This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",http://arxiv.org/abs/2510.26769v1,2025-10-30T17:52:39Z,"Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas","**Introducing SteerVLM: A New Way to Control AI Models**

Imagine being able to guide a powerful AI model to produce more accurate and relevant responses, without having to rewrite its underlying code. Researchers have developed a new technique called SteerVLM, which allows for fine-grained control over the output of Vision-Language Models (VLMs). These models are capable of understanding and generating text based on images and text inputs.

**What does SteerVLM do?**

SteerVLM is a lightweight module that can be added to existing VLMs to steer their outputs towards desired behaviors. It works by adjusting the model's internal activations, which are like the ""thoughts"" of the model, to better match the desired output. This approach requires minimal additional computational resources and can be applied at inference time, making it efficient and practical.

**Key benefits**

* **Improved control**: SteerVLM allows for precise control over the model's output semantics, enabling it to produce more accurate and relevant responses.
* **Lightweight**: The steering module requires only a tiny fraction (0.14%) of the original model's parameters, making it efficient and easy to implement.
* **No need for manual tuning**: SteerVLM adapts automatically to different situations, eliminating the need for manual tuning or pre-extracted static vectors.

**A new dataset for multimodal model control**

The researchers also introduced VNIA, a new dataset designed to help develop and evaluate VLM steering techniques. This dataset will facilitate further research and improvement in this area.

**Conclusion**

SteerVLM offers a robust solution for controlling AI models, enabling more accurate and relevant outputs without requiring significant changes to the underlying model. This technique has the potential to improve the performance and reliability of VLMs, which are increasingly used in applications such as image captioning, visual question answering, and more."
cs.LG,The Oversight Game: Learning to Cooperatively Balance an AI Agent's   Safety and Autonomy,"As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.",http://arxiv.org/abs/2510.26752v1,2025-10-30T17:46:49Z,"William Overman, Mohsen Bayati","**Making AI Safer: A New Approach to Balancing Autonomy and Control**

As AI agents become more capable, ensuring their safety and alignment with human values is crucial. Researchers have proposed a novel approach to achieve this balance by creating a control interface that allows humans and AI agents to work together. The goal is to enable AI agents to make decisions autonomously while still being overseen by humans to prevent potential harm.

The researchers modeled this interaction as a game where the AI agent decides whether to act on its own or defer to a human, and the human decides whether to trust the agent or oversee its actions. By analyzing this game, they found conditions under which the agent's autonomous decisions will not harm human values. This approach provides a formal guarantee that the agent's actions will align with human goals.

In a simulated environment, the researchers demonstrated that the AI agent and human can learn to work together effectively, with the agent deferring to the human when uncertain and the human overseeing the agent when necessary. This collaboration leads to safe decision-making without requiring changes to the underlying AI system.

**Key Takeaways:**

* A new approach to balancing AI autonomy and human control has been proposed.
* The approach provides a formal guarantee that AI agents will not harm human values.
* A simulated environment demonstrated the effectiveness of this approach in achieving safe decision-making.

This research offers a promising solution for making AI safer and more aligned with human values, without requiring significant modifications to existing AI systems."
cs.LG,Deep sequence models tend to memorize geometrically; it is unclear why,"In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task.   From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.   Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.",http://arxiv.org/abs/2510.26745v1,2025-10-30T17:40:22Z,"Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar","**Unlocking the Secrets of How AI Models Store Information**

Imagine you're trying to remember where you put your keys. You might recall that you last saw them near the couch, or on the kitchen counter. This process of remembering is similar to how artificial intelligence (AI) models store and retrieve information. Researchers have long thought that AI models use a simple, brute-force approach to store information, essentially memorizing pairs of things that occur together.

However, a new study suggests that AI models, specifically those using a type of model called Transformers, may be storing information in a more complex and elegant way. This ""geometric"" approach allows the model to understand relationships between many different pieces of information, not just those that are directly connected.

The study found that even when the model is only trained on local associations between things, it can still learn to represent information in a geometric way. This is surprising, as it's not clear why the model would choose to do so. The researchers also found that this geometric approach can simplify complex reasoning tasks, making them easier to learn.

The study's findings have implications for how we understand how AI models acquire and store knowledge. By understanding how models like Transformers store information, researchers can develop more efficient and effective AI systems. The study's results also suggest that there may be ways to improve the performance of AI models by making their memory more geometric.

Overall, the study provides new insights into the inner workings of AI models and encourages researchers to rethink their assumptions about how these models store and retrieve information."
cs.LG,Bridging the Gap between Empirical Welfare Maximization and Conditional   Average Treatment Effect Estimation in Policy Learning,"The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a novel regularization method for policy learning. Our findings yield a convex and computationally efficient training procedure that avoids the NP-hard combinatorial step typically required in EWM.",http://arxiv.org/abs/2510.26723v1,2025-10-30T17:23:40Z,Masahiro Kato,"**Unlocking Better Decision-Making: A New Approach to Policy Learning**

Imagine you're a healthcare professional trying to decide which treatment to prescribe to a patient. You want to choose the treatment that will have the best outcome for the patient, based on their individual characteristics. This is a classic problem in policy learning, where the goal is to develop a decision-making system that recommends the best course of action.

There are two main approaches to policy learning: Empirical Welfare Maximization (EWM) and the plug-in approach. EWM works by estimating the overall benefit of different treatments and choosing the one that maximizes that benefit. The plug-in approach, on the other hand, estimates the individual effect of each treatment and recommends the one with the highest estimated effect.

Researchers have now discovered that these two approaches are actually equivalent, meaning they are solving the same underlying problem. This breakthrough finding has important implications. It means that the two approaches can be used interchangeably, and that they have the same level of accuracy under certain conditions.

The researchers also propose a new method for training decision-making systems, which combines the strengths of both approaches. This new method is more efficient and easier to compute, avoiding a complex mathematical problem that was previously required.

Overall, this research has the potential to improve decision-making in a wide range of fields, from healthcare to education and beyond. By developing more accurate and efficient decision-making systems, we can make better choices that lead to better outcomes for individuals and society."
cs.LG,Non-Convex Over-the-Air Heterogeneous Federated Learning: A   Bias-Variance Trade-off,"Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.",http://arxiv.org/abs/2510.26722v2,2025-10-30T17:22:57Z,"Muhammad Faraz Ul Abrar, Nicolò Michelusi","**Summary: A New Approach to Federated Learning over Wireless Networks**

Imagine a future where many devices, like smartphones and laptops, can learn from each other and improve their performance on tasks like image recognition without sharing their data. This is the idea behind federated learning (FL). However, when devices have different connections to the internet, some may struggle to participate, slowing down the learning process.

Researchers have proposed a new approach to overcome this challenge, called over-the-air (OTA) federated learning. Their method allows devices to send updates to a central server simultaneously, using the same wireless channel. This approach can speed up the learning process, but it can also introduce errors, or ""bias,"" that can affect the accuracy of the model.

The researchers developed a new way to update the model that allows for some bias, but reduces errors caused by the variability of the wireless connections. They also created a mathematical formula that shows how to balance the trade-off between bias and variance. This formula helps optimize the performance of the model.

The researchers tested their approach on a task of image classification and found that it accelerates convergence and improves generalization compared to previous methods. This new approach could enable faster and more accurate learning on a wide range of devices, even in areas with varying wireless connectivity.

**Key Takeaways:**

* A new approach to federated learning over wireless networks that allows devices to learn from each other without sharing data.
* The approach balances the trade-off between bias and variance to optimize performance.
* The method was tested on an image classification task and showed improved convergence and generalization."
cs.LG,On Purely Private Covariance Estimation,"We present a simple perturbation mechanism for the release of $d$-dimensional covariance matrices $\Sigma$ under pure differential privacy. For large datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers the provably optimal Frobenius norm error guarantees of \cite{nikolov2023private}, while simultaneously achieving best known error for all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is information-theoretically optimal for all $p\ge 2$, in particular, our mechanism is the first purely private covariance estimator that achieves optimal error in spectral norm.   For small datasets $n< d^2/\varepsilon$, we further show that by projecting the output onto the nuclear norm ball of appropriate radius, our algorithm achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$, improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private} and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of \cite{dong2022differentially}.",http://arxiv.org/abs/2510.26717v1,2025-10-30T17:18:53Z,"Tommaso d'Orsi, Gleb Novikov","**Protecting Sensitive Data while Estimating Covariance**

Imagine you have a large dataset with many variables, and you want to understand how these variables relate to each other. A crucial step in this process is estimating the covariance matrix, which shows how much each variable changes when another variable changes. However, if your dataset contains sensitive information about individuals, you need to ensure that any analysis you do doesn't compromise their privacy.

Researchers have developed a method to estimate covariance matrices while keeping individual data private. This method, called a perturbation mechanism, adds a small amount of random noise to the estimated covariance matrix to protect individual data. The researchers showed that their method works well for large datasets and provides the best possible accuracy for measuring how similar the variables are.

The key benefits of this method are:

* **Optimal accuracy**: For large datasets, the method achieves the best possible accuracy in measuring the relationships between variables.
* **Improved accuracy for small datasets**: For smaller datasets, the method still provides good accuracy by adjusting the output to fit within a certain range.
* **Stronger privacy guarantees**: The method ensures that individual data remains private, even when releasing the estimated covariance matrix.

This research has important implications for fields like data analysis, machine learning, and statistics, where protecting sensitive data is crucial. By providing a reliable and private way to estimate covariance matrices, this method can help researchers and analysts make more informed decisions while safeguarding individual privacy."
cs.LG,LSM-MS2: A Foundation Model Bridging Spectral Identification and   Biological Interpretation,"A vast majority of mass spectrometry data remains uncharacterized, leaving much of its biological and chemical information untapped. Recent advances in machine learning have begun to address this gap, particularly for tasks such as spectral identification in tandem mass spectrometry data. Here, we present the latest generation of LSM-MS2, a large-scale deep learning foundation model trained on millions of spectra to learn a semantic chemical space. LSM-MS2 achieves state-of-the-art performance in spectral identification, improving on existing methods by 30% in accuracy of identifying challenging isomeric compounds, yielding 42% more correct identifications in complex biological samples, and maintaining robustness under low-concentration conditions. Furthermore, LSM-MS2 produces rich spectral embeddings that enable direct biological interpretation from minimal downstream data, successfully differentiating disease states and predicting clinical outcomes across diverse translational applications.",http://arxiv.org/abs/2510.26715v1,2025-10-30T17:13:58Z,"Gabriel Asher, Devesh Shah, Amy A. Caudy, Luke Ferro, Lea Amar, Ana S. H. Costa, Thomas Patton, Niall O'Connor, Jennifer M. Campbell, Jack Geremia","**Breakthrough in Mass Spectrometry: AI Model Unlocks Hidden Biological Insights**

Scientists have developed a powerful artificial intelligence (AI) model called LSM-MS2, which can analyze large amounts of data from mass spectrometry, a technique used to identify the chemical composition of biological samples. Mass spectrometry generates vast amounts of data, but much of it remains uncharacterized, limiting our understanding of the biological and chemical information it contains.

The LSM-MS2 model, trained on millions of spectra, has achieved state-of-the-art performance in identifying chemical compounds in biological samples. Specifically, it has:

* Improved the accuracy of identifying complex compounds by 30%
* Identified 42% more correct compounds in complex biological samples
* Maintained accuracy even when compound concentrations are low

What's more, LSM-MS2 can also provide direct insights into the biological meaning of the data, allowing researchers to:

* Differentiate between different disease states
* Predict clinical outcomes

This breakthrough has the potential to unlock new discoveries in fields such as medicine, biology, and chemistry, and could lead to a better understanding of complex biological systems. The LSM-MS2 model is a significant step forward in harnessing the power of mass spectrometry data to drive innovation and improve human health."
cs.LG,On the limitation of evaluating machine unlearning using only a single   training seed,"Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comparisons of MU algorithms should also reflect the variability across different model training seeds.",http://arxiv.org/abs/2510.26714v2,2025-10-30T17:13:42Z,"Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma","Here's a summary of the research paper for a general audience:

**The Limitations of Testing Machine Unlearning**

Machine unlearning is a technique that aims to remove the influence of certain data points from a trained artificial intelligence (AI) model without having to retrain the entire model from scratch. This can be useful for ensuring that AI models don't use sensitive or outdated information.

However, testing the effectiveness of machine unlearning algorithms can be tricky. Researchers often run these algorithms multiple times on the same trained model to see how well they work. But, a new study shows that this approach can be flawed.

The study found that some machine unlearning methods can produce very different results depending on the random numbers used to train the AI model in the first place. This means that if you only test an algorithm with one training seed (or set of random numbers), you might get results that aren't representative of how the algorithm will perform in real life.

The researchers recommend that testing machine unlearning algorithms should take into account the variability that comes from different training seeds. This will give a more accurate picture of how well these algorithms work and help researchers compare them more effectively."
cs.CV,Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with   the MME-CoF Benchmark,"Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",http://arxiv.org/abs/2510.26802v1,2025-10-30T17:59:55Z,"Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng","**Can Video Models Think on Their Own?**

Researchers investigated whether video generation models, like those used to create realistic videos, can also reason and think critically on their own. They tested a popular video model called Veo-3 on its ability to reason in various visual scenarios, such as understanding spatial relationships, physics, and logic.

The researchers created a benchmark, called MME-CoF, to evaluate the model's reasoning abilities across 12 different areas. They found that while Veo-3 showed promising results in some areas, such as short-term spatial reasoning and fine-grained details, it struggled with more complex tasks, like long-term causal reasoning and abstract logic.

Overall, the study suggests that current video models are not yet reliable as standalone ""zero-shot reasoners,"" meaning they can't think critically on their own without additional guidance. However, they do show potential as complementary visual tools that can work alongside specialized reasoning models to improve visual understanding."
cs.CV,OmniX: From Unified Panoramic Generation and Perception to   Graphics-Ready 3D Scenes,"There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",http://arxiv.org/abs/2510.26800v1,2025-10-30T17:59:51Z,"Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu","**Breakthrough in 3D Scene Creation: OmniX Revolutionizes Virtual World Generation**

Imagine being able to create immersive and realistic 3D environments with ease, similar to those found in video games or virtual reality experiences. Researchers have made a significant advancement in this field with the development of OmniX, a unified framework that generates high-quality 3D scenes from 2D images.

**The Challenge of 3D Scene Creation**

Creating 3D scenes can be done in two main ways: using algorithms to generate scenes from scratch (procedural generation) or lifting 2D images into 3D environments (2D lifting). While both methods have their strengths, panorama-based 2D lifting has shown great promise in producing realistic and diverse 3D environments.

**How OmniX Works**

OmniX takes this technique to the next level by using powerful 2D generative models to not only create realistic 3D scenes but also to perceive and understand the geometry, textures, and materials of the scene. This allows for the generation of ""graphics-ready"" 3D scenes that can be used for physically based rendering, relighting, and simulation.

**Key Innovations**

The OmniX framework is built on a lightweight and efficient adapter structure that enables the reuse of 2D generative priors for a wide range of panoramic vision tasks. This includes panoramic perception, generation, and completion. Additionally, a large-scale synthetic panorama dataset has been created, containing high-quality multimodal panoramas from diverse indoor and outdoor scenes.

**Impact and Applications**

The OmniX framework has been extensively tested and has shown impressive results in panoramic visual perception and 3D scene generation. This breakthrough technology opens up new possibilities for creating immersive and physically realistic virtual worlds, with potential applications in fields such as:

* Video game development
* Virtual reality experiences
* Architecture and urban planning
* Film and animation production

Overall, OmniX represents a significant advancement in 3D scene creation, enabling the generation of high-quality, realistic, and immersive virtual environments with unprecedented ease and flexibility."
cs.CV,Masked Diffusion Captioning for Visual Feature Learning,"We learn visual features by captioning images with an image-conditioned masked diffusion language model, a formulation we call masked diffusion captioning (MDC). During training, text tokens in each image-caption pair are masked at a randomly chosen ratio, and a decoder conditioned on visual features is trained to reconstruct the original text. After training, the learned visual features can be applied to downstream vision tasks. Unlike autoregressive captioning, the strength of the visual learning signal in MDC does not depend on each token's position in the sequence, reducing the need for auxiliary objectives. Linear probing experiments across a variety of academic-scale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches.",http://arxiv.org/abs/2510.26799v1,2025-10-30T17:59:46Z,"Chao Feng, Zihao Wei, Andrew Owens","Here's a summary of the research paper for a general audience:

**Improving Computer Vision with a New Training Method**

Researchers have developed a new way to train computers to understand visual information from images. The method, called Masked Diffusion Captioning (MDC), involves teaching a computer to generate captions for images, but with a twist. Instead of trying to generate the entire caption at once, the computer is shown an image and a caption with some words randomly removed. The computer then tries to fill in the missing words based on the image and the remaining words in the caption.

This approach allows the computer to learn more effective visual features, which are like building blocks that help the computer understand what's in an image. These features can then be used for a variety of tasks, such as object recognition, image classification, and more.

The researchers tested their method on several datasets and found that it produces visual features that are just as good as those produced by other state-of-the-art methods. The advantage of MDC is that it's a more straightforward and efficient way to learn visual features, which could lead to improvements in computer vision applications such as self-driving cars, facial recognition, and medical image analysis."
cs.CV,SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting,"Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.",http://arxiv.org/abs/2510.26796v1,2025-10-30T17:59:39Z,"Dongyue Lu, Ao Liang, Tianxin Huang, Xiao Fu, Yuyang Zhao, Baorui Ma, Liang Pan, Wei Yin, Lingdong Kong, Wei Tsang Ooi, Ziwei Liu","**Breakthrough in 4D Content Generation: SEE4D**

Imagine being able to generate immersive 4D content from ordinary videos, without the need for expensive 3D modeling or manual camera pose annotations. Researchers have made a significant step towards achieving this goal with the introduction of SEE4D, a pose-free, 4D generation framework.

**The Problem with Current Methods**

Current methods for generating 4D content from videos rely on manually annotated camera poses, which are time-consuming and prone to errors. Recent approaches have attempted to mitigate this issue by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions. However, these methods often entangle camera motion with scene dynamics, making it challenging to model and infer 4D scenes.

**How SEE4D Works**

SEE4D takes a different approach by separating camera control from scene modeling. Instead of predicting camera trajectories, it renders input frames to a set of fixed virtual cameras, allowing for more flexible and accurate 4D generation. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising synthesized warped images and filling in occluded or missing regions across virtual viewpoints.

**Key Advantages**

The SEE4D framework offers several key advantages:

* **Pose-free**: No manual camera pose annotations are required, making it more practical for in-the-wild footage.
* **Improved performance**: SEE4D achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines.
* **Coherent generation**: The autoregressive inference pipeline enables coherent generation of 4D content at bounded per-step complexity.

**Implications and Future Directions**

The SEE4D framework has significant implications for various applications, including:

* **Immersive experiences**: SEE4D can enable the creation of immersive 4D content for virtual reality, augmented reality, and other applications.
* **Video analysis**: SEE4D can improve video analysis tasks, such as object recognition and tracking, by providing more accurate 4D representations of scenes.

Overall, SEE4D represents a significant advancement in 4D content generation, enabling the creation of immersive and dynamic 4D scenes from ordinary videos without the need for extensive manual annotations or 3D modeling."
cs.CV,Scaling Image Geo-Localization to Continent Level,"Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (>100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.",http://arxiv.org/abs/2510.26795v1,2025-10-30T17:59:35Z,"Philipp Lindenberger, Paul-Edouard Sarlin, Jan Hosang, Matteo Balice, Marc Pollefeys, Simon Lynen, Eduard Trulls","**Advancing Image Geo-Localization: A Breakthrough in Identifying Locations from Photos**

Imagine being able to pinpoint the exact location of a photo taken anywhere in the world, from a bustling city street to a serene countryside landscape. Researchers have made a significant step towards achieving this goal with a new approach that can accurately identify the location of an image on a continental scale.

The challenge lies in the vast number of images available (over 100 million) and the limited coverage of certain areas, making traditional image retrieval techniques inefficient. Previous solutions have either provided coarse results or been limited to smaller regions.

The innovative approach introduced in this study combines two techniques:

1. **Learning rich feature representations**: By training a model on a proxy task, researchers can extract detailed information from images that implicitly encodes precise location data.
2. **Combining ground and aerial imagery**: By integrating aerial imagery with ground-level images, the model becomes more robust and can handle areas with limited ground-level data.

The results are impressive: the new approach can localize images within 200 meters of their actual location more than 68% of the time, across a vast area covering a significant part of Europe. This achievement has the potential to revolutionize various applications, such as mapping, surveillance, and environmental monitoring.

The code for this approach is publicly available, paving the way for further development and applications in the field of image geo-localization."
cs.CV,"The Quest for Generalizable Motion Generation: Data, Model, and   Evaluation","Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",http://arxiv.org/abs/2510.26794v1,2025-10-30T17:59:27Z,"Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu","Here's a summary of the research paper for a general audience:

**Improving Artificial Intelligence's Ability to Generate Human Motion**

Researchers have made significant progress in creating artificial intelligence (AI) models that can generate 3D human motion, such as walking, running, or dancing. However, these models often struggle to generalize to new situations and environments. To overcome this limitation, the researchers drew inspiration from a related field: video generation. They developed a comprehensive framework that transfers knowledge from video generation to motion generation.

**Key Contributions**

The researchers created:

1. **A large dataset**: A massive collection of 228,000 high-quality motion samples, including data from motion capture technology, web videos, and synthesized samples. This dataset provides a diverse range of human motions and contexts.
2. **A new AI model**: A flow-matching-based diffusion transformer that combines the strengths of motion capture data and video generation models. This model can generate high-quality human motion that generalizes well to new situations.
3. **An efficient variant**: A distilled version of the model that eliminates the need for video generation dependencies while maintaining strong generalization capabilities.
4. **A benchmark for evaluation**: A hierarchical benchmark that assesses motion quality, prompt fidelity, and generalization ability.

**Breakthroughs**

The researchers' framework significantly outperforms existing approaches in both automatic and human evaluations. Their work has the potential to improve AI's ability to generate realistic and diverse human motion, which can be applied to various fields, such as animation, robotics, and virtual reality. The code, data, and benchmark will be made publicly available, enabling further research and development in this area."
cs.CV,HEIR: Learning Graph-Based Motion Hierarchies,"Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",http://arxiv.org/abs/2510.26786v1,2025-10-30T17:57:40Z,"Cheng Zheng, William Koch, Baiang Li, Felix Heide","**Breakthrough in Motion Modeling: A New Way to Understand Complex Movement**

Imagine trying to understand a complex dance routine. Instead of looking at the individual steps, researchers have developed a new method to analyze the relationships between different movements. This approach, called HEIR, uses a graph-based system to learn the hierarchical structure of motion from data.

In simple terms, HEIR breaks down complex movements into smaller components and identifies how they relate to each other. For example, in a dance routine, the movement of a single arm might be influenced by the movement of the entire body. HEIR's graph-based system represents these relationships as a tree-like structure, where each node represents a movement and the edges represent the connections between them.

The researchers tested HEIR on three different types of motion: simple 1D movements, 2D rotations, and complex 3D scene deformations. The results showed that HEIR was able to accurately reconstruct the underlying motion hierarchy in the 1D and 2D cases. In the 3D case, HEIR produced more realistic and interpretable results compared to existing methods.

The key innovation of HEIR is that it learns the motion hierarchy directly from data, rather than relying on pre-defined or heuristic structures. This makes it a flexible and adaptable approach that can be applied to a wide range of motion-centric tasks, from computer vision and graphics to robotics.

**What does this mean?**

* HEIR provides a new way to understand complex movements by breaking them down into smaller components and analyzing their relationships.
* This approach has the potential to improve our ability to model and analyze motion in various fields, from computer vision to robotics.
* By learning the motion hierarchy directly from data, HEIR offers a more flexible and adaptable approach than existing methods.

**Why is this important?**

* Understanding complex movements is crucial in many areas, such as robotics, computer vision, and graphics.
* HEIR's approach has the potential to enable more accurate and efficient motion modeling, which could lead to breakthroughs in areas like robotics, animation, and video analysis."
cs.CV,Clone Deterministic 3D Worlds with Geometrically-Regularized World   Models,"A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",http://arxiv.org/abs/2510.26782v1,2025-10-30T17:56:43Z,"Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen","**Breakthrough in Artificial Intelligence: Creating a More Accurate Virtual World**

Imagine a computer program that can simulate a virtual world, predicting what will happen next based on past experiences. This is known as a ""world model,"" and it's essential for developing intelligent agents that can think, plan, and reason in complex environments. However, current world models have limitations, degrading over time and struggling to make accurate long-term predictions.

Researchers have made a significant step forward in addressing this challenge by developing a new approach called Geometrically-Regularized World Models (GRWM). GRWM improves the way the program represents the virtual world, allowing it to learn a more accurate and stable model of the environment.

**Key Innovation:**

The GRWM approach ensures that consecutive points in a virtual trajectory (e.g., a sequence of images) remain close in a latent representation space. This means that the program can better understand the relationships between different parts of the environment, leading to more accurate predictions.

**Impact:**

The GRWM approach has been tested in various 3D environments and has shown significant improvements in long-term prediction tasks. The benefits of GRWM come from learning a latent manifold with superior geometric structure, which enables more accurate and stable predictions.

**Takeaway:**

The study demonstrates that improving representation learning is a direct and useful path to robust world models. By developing more accurate virtual worlds, researchers can create intelligent agents that can think, plan, and reason more effectively in complex environments. This breakthrough has the potential to advance various applications, including robotics, autonomous vehicles, and video games."
cs.CV,ChartAB: A Benchmark for Chart Grounding & Dense Alignment,"Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel ""ChartAlign Benchmark (ChartAB)"" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.",http://arxiv.org/abs/2510.26781v1,2025-10-30T17:56:31Z,"Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou","Here's a summary of the research paper for a general audience:

**Understanding Charts: A New Benchmark for AI Models**

Charts are a great way to visualize data and help us understand complex information. However, AI models that can understand and interpret charts are still in their early stages. These models struggle to extract detailed information from charts, which makes it hard for them to compare multiple charts and make sense of them.

To address this challenge, researchers have created a new benchmark called ChartAB. This benchmark is a tool that tests how well AI models can understand charts by asking them to perform tasks such as:

* Extracting data from charts
* Identifying different parts of a chart (e.g. labels, titles)
* Recognizing specific attributes of a chart (e.g. colors, shapes)

The researchers used ChartAB to evaluate several recent AI models and found that they have limitations in understanding charts. For example, some models are biased towards certain types of charts or make mistakes when trying to extract information. These findings highlight the need for further research and development to improve AI models' ability to understand and interpret charts.

The creation of ChartAB is an important step towards developing more accurate and reliable AI models that can help us make sense of complex data visualizations."
cs.CV,Surpassing state of the art on AMD area estimation from RGB fundus   images through careful selection of U-Net architectures and loss functions   for class imbalance,"Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.",http://arxiv.org/abs/2510.26778v1,2025-10-30T17:55:46Z,"Valentyna Starodub, Mantas Lukoševičius","Here's a summary of the research paper for a general audience:

**Advancing Detection of Age-Related Macular Degeneration**

Age-related macular degeneration (AMD) is a leading cause of vision loss in people over 60. Researchers have made progress in detecting AMD using a non-invasive and cost-effective imaging technique called RGB fundus imaging. This technique takes pictures of the back of the eye to identify lesions that can cause vision loss.

In this study, researchers tested different approaches to improve the accuracy of a computer model called U-Net in detecting AMD lesions from RGB fundus images. They compared various techniques, such as data pre-processing, different types of neural networks, and specialized loss functions to handle imbalances in the data.

The researchers found that by carefully selecting the right combination of techniques, they could create a model that outperforms previous state-of-the-art models in detecting different types of AMD lesions. This breakthrough has the potential to improve the diagnosis and treatment of AMD, and the researchers have made their code freely available to others.

**In simple terms:** This study used computer algorithms to analyze images of the eye to detect age-related macular degeneration (AMD). By testing different approaches, the researchers developed a more accurate model that can help doctors diagnose and treat AMD more effectively."
cs.CV,SteerVLM: Robust Model Control through Lightweight Activation Steering   for Vision Language Models,"This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",http://arxiv.org/abs/2510.26769v1,2025-10-30T17:52:39Z,"Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas","**Controlling AI Models with a Lightweight Steering Module**

Imagine being able to guide a powerful AI model to produce more accurate and relevant responses, without having to retrain or modify the model itself. Researchers have developed a novel approach called SteerVLM, which introduces a lightweight ""steering module"" that can be added to Vision-Language Models (VLMs) to control their outputs.

**What does it do?**

SteerVLM allows for fine-grained control over the model's responses, ensuring they align with desired instructions. This is achieved by dynamically adjusting the model's internal activations, which connect language and image information. The best part? This steering module is extremely lightweight, requiring only 0.14% of the original model's parameters.

**How does it work?**

The steering module learns from paired prompts that encode target and converse behaviors. This enables it to adaptively steer the model's activations across different layers, without needing manual tuning or pre-extracted static vectors. The result is a more robust and flexible solution for controlling VLMs.

**The Impact**

The researchers also created a new dataset, VNIA, to evaluate the effectiveness of VLM steering techniques. Their method, SteerVLM, outperformed existing techniques in benchmarks for steering and hallucination mitigation. This work has significant implications for developing more controllable and reliable AI models, which can be used in a variety of applications, from chatbots to image captioning.

**In Simple Terms**

Think of SteerVLM as a ""remote control"" for AI models. It allows researchers to guide the model's responses, making them more accurate and relevant, without having to change the model itself. This lightweight steering module has the potential to improve the performance and reliability of AI models, making them more useful in a wide range of applications."
cs.CV,MORE: Multi-Organ Medical Image REconstruction Dataset,"CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page https://more-med.github.io/",http://arxiv.org/abs/2510.26759v1,2025-10-30T17:49:49Z,"Shaokai Wu, Yapan Guo, Yanbiao Ji, Jing Tong, Yuxiang Lu, Mei Li, Suizhi Huang, Yue Ding, Hongtao Lu","**Advancing Medical Imaging: Introducing the MORE Dataset**

Medical imaging plays a crucial role in diagnosing and treating diseases. A major challenge in using artificial intelligence (AI) for medical imaging is that current AI models are often limited to specific parts of the body and types of injuries or diseases. This makes it difficult for these models to accurately interpret images of other parts of the body or unfamiliar conditions.

To address this issue, researchers have created a new dataset called Multi-Organ Medical Image REconstruction (MORE). This dataset contains a wide variety of CT scans from 9 different parts of the body and 15 types of injuries or diseases. The goal of the MORE dataset is to help train AI models to be more robust and generalizable, allowing them to accurately interpret a broader range of medical images.

The researchers also developed a new AI model that outperforms existing models on the MORE dataset. Their results show that using a comprehensive dataset like MORE can significantly improve the ability of AI models to generalize to new and unfamiliar images. This advancement has the potential to improve the accuracy and reliability of medical imaging, ultimately leading to better patient care. The MORE dataset is now freely available to researchers, and its use is expected to drive further innovation in medical imaging."
cs.CV,ProstNFound+: A Prospective Study using Medical Foundation Models for   Prostate Cancer Detection,"Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.",http://arxiv.org/abs/2510.26703v1,2025-10-30T17:07:04Z,"Paul F. R. Wilson, Mohamed Harmanani, Minh Nguyen Nhat To, Amoon Jamzad, Tarek Elghareb, Zhuoxin Guo, Adam Kinnaird, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi","Here's a summary of the research paper for a general audience:

**Breakthrough in Prostate Cancer Detection**

Researchers have made a significant step forward in detecting prostate cancer using artificial intelligence (AI). They developed a new AI model called ProstNFound+ that analyzes micro-ultrasound images to identify prostate cancer. This model uses a type of AI called a ""foundation model"" that can learn from large amounts of data and adapt to new situations.

**How it works**

ProstNFound+ was trained on data from multiple centers and then tested on new data from a different clinical site, collected five years later. The model generates a heatmap that highlights areas of potential cancer and provides a risk score for clinically significant prostate cancer.

**Promising results**

The results are promising, with ProstNFound+ performing well on the new data, with no decrease in performance compared to its initial testing. The model's predictions align closely with standard clinical scoring protocols, and it produces interpretable heatmaps that match biopsy-confirmed lesions.

**What it means**

This study suggests that ProstNFound+ has the potential to be a valuable tool for doctors in detecting prostate cancer. Its ability to generalize to new data and produce accurate results makes it a scalable and interpretable alternative to traditional methods. This could lead to earlier and more accurate diagnoses, and ultimately, better treatment outcomes for patients with prostate cancer."
cs.CV,The Impact and Outlook of 3D Gaussian Splatting,"Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.",http://arxiv.org/abs/2510.26694v1,2025-10-30T17:01:18Z,Bernhard Kerbl,"**Breakthrough in 3D Technology: 3D Gaussian Splatting**

Imagine being able to capture and recreate 3D scenes with unprecedented accuracy and detail. A recent innovation called 3D Gaussian Splatting (3DGS) has made this possible, and it's transforming the field of 3D vision and graphics.

**What is 3DGS?**

3DGS is a way to represent 3D scenes using a mathematical model. It allows for efficient and detailed rendering of complex environments, making it a game-changer for applications such as virtual reality, video games, and architectural visualization.

**Advances and Applications**

Researchers have been actively exploring ways to improve 3DGS, leading to several exciting developments:

* **Faster and more efficient**: New methods enable faster training and rendering of 3D scenes, making it possible to use 3DGS on lower-powered devices.
* **Dynamic scenes**: 3DGS can now be used to represent dynamic scenes, such as videos or moving objects, opening up new possibilities for applications like video production and simulation.
* **Mobile and VR**: 3DGS is being adapted for use on mobile devices and virtual reality platforms, enabling more immersive experiences.
* **Large-scale environments**: Researchers have made progress in applying 3DGS to massive-scale environments, such as cities or landscapes.

**The Future of 3DGS**

As 3DGS continues to evolve, we can expect to see even more innovative applications in fields like:

* **Virtual reality and gaming**: More realistic and immersive experiences
* **Architecture and construction**: Detailed and interactive 3D models of buildings and environments
* **Video production and special effects**: Faster and more realistic rendering of complex scenes

Overall, 3D Gaussian Splatting has revolutionized the field of 3D vision and graphics, and its continued development is likely to have a significant impact on various industries and applications."
cs.CV,Process Integrated Computer Vision for Real-Time Failure Prediction in   Steel Rolling Mill,"We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.",http://arxiv.org/abs/2510.26684v1,2025-10-30T16:54:16Z,"Vaibhav Kurrey, Sivakalyan Pujari, Gagan Raj Gupta","**Predicting Equipment Failures in Steel Mills with Computer Vision**

A recent study has successfully developed and deployed a computer vision system to predict equipment failures in a steel rolling mill. The system uses cameras to monitor the mill's equipment and production process in real-time, and applies deep learning algorithms to detect potential issues before they occur.

By analyzing video feeds and sensor data, the system can identify the location and likely cause of potential failures, allowing for proactive maintenance and reducing the risk of unplanned breakdowns. This integrated approach has the potential to improve operational reliability, productivity, and profitability in industrial manufacturing environments.

The innovative aspect of this system is that it can be easily scaled up and applied to other production lines with minimal additional resources, making it a cost-effective solution for industries looking to improve their equipment maintenance and reduce downtime."
cs.CV,Improving Classification of Occluded Objects through Scene Context,"The presence of occlusions has provided substantial challenges to typically-powerful object recognition algorithms. Additional sources of information can be extremely valuable to reduce errors caused by occlusions. Scene context is known to aid in object recognition in biological vision. In this work, we attempt to add robustness into existing Region Proposal Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks through two distinct scene-based information fusion techniques. We present one algorithm under each methodology: the first operates prior to prediction, selecting a custom object network to use based on the identified background scene, and the second operates after detection, fusing scene knowledge into initial object scores output by the RPN. We demonstrate our algorithms on challenging datasets featuring partial occlusions, which show overall improvement in both recall and precision against baseline methods. In addition, our experiments contrast multiple training methodologies for occlusion handling, finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others. Our method is interpretable and can easily be adapted to other datasets, offering many future directions for research and practical applications.",http://arxiv.org/abs/2510.26681v1,2025-10-30T16:51:18Z,"Courtney M. King, Daniel D. Leeds, Damian Lyons, George Kalaitzis","**Improving Object Recognition through Context**

Imagine trying to identify an object that's partially hidden from view. This can be a challenge for computers, just like it is for humans. Researchers have made significant progress in developing powerful object recognition algorithms, but occlusions - when objects are blocked by other things - can still cause errors.

In a recent study, scientists explored how to improve object recognition by using ""scene context"" - information about the surroundings. For example, if you're in a kitchen, you're more likely to see a toaster than a bicycle. The researchers developed two new techniques to incorporate scene context into existing object detection algorithms.

The first technique selects a customized object recognition network based on the background scene. The second technique adjusts the initial object detection scores based on what the algorithm knows about the scene.

The researchers tested their methods on challenging datasets with partial occlusions and found that they improved both recall (finding all the objects) and precision (avoiding false positives). They also discovered that training the algorithm on a mix of occluded and unoccluded images leads to better performance.

This work has important implications for applications such as self-driving cars, surveillance systems, and robotics, where accurate object recognition is crucial. The researchers' approach is interpretable and adaptable to other datasets, opening up new avenues for research and practical applications."
cs.CV,BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric   Brain MRI,"Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.",http://arxiv.org/abs/2510.26661v1,2025-10-30T16:29:09Z,"Alya Almsouti, Ainur Khamitova, Darya Taratynova, Mohammad Yaqub","**Improving Image Quality Assessment in Pediatric Brain MRI**

Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool for pediatric brain conditions. However, the quality of MRI images can be affected by various types of artifacts, which can lead to inaccurate diagnoses. Assessing the severity of these artifacts is a time-consuming and subjective process, which is why researchers have developed an automated solution called BRIQA.

BRIQA aims to accurately classify the severity of artifacts in pediatric brain MRI images, addressing a common problem in medical imaging: class imbalance. This means that some types of artifacts are much more common than others, making it challenging for algorithms to learn to detect the less common ones.

The BRIQA system uses a novel approach called gradient-based loss reweighting, which dynamically adjusts the importance of each type of artifact during training. This allows the algorithm to focus on the less common artifacts and improve its overall performance.

In experiments, BRIQA outperformed existing methods, particularly for underrepresented artifact types such as Noise, Zipper, and Positioning. The system's performance improved significantly, with an average increase of 5% in accuracy.

The development of BRIQA has the potential to improve the accuracy of pediatric brain MRI diagnoses, especially in low-field systems where image quality can be compromised. The code for BRIQA is now publicly available, allowing researchers and clinicians to use and build upon this innovative technology."
cs.CV,Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning   Optical Flow on RADARSAT-2,"Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",http://arxiv.org/abs/2510.26653v1,2025-10-30T16:20:28Z,"Daniela Martin, Joseph Gallego","**Accurate Sea Ice Drift Estimation in the Arctic: A Breakthrough with Deep Learning**

Scientists have made a significant advancement in estimating the movement of sea ice in the Arctic, which is crucial for navigation, climate research, and weather forecasting. They used a technique called optical flow, which analyzes satellite images to track the movement of objects. In this case, they applied optical flow to RADARSAT-2 satellite images of sea ice.

The researchers tested 48 different deep learning models, a type of artificial intelligence, to see how well they could estimate sea ice drift. They compared the results to actual measurements from GPS-tracked buoys on the sea ice. The best models achieved an accuracy of 300-400 meters, which is a significant improvement over previous methods.

This breakthrough has several important implications:

* **Improved navigation**: Accurate sea ice drift estimation can help ships and other vessels navigate safely through the Arctic.
* **Climate modeling**: The new method provides a more detailed and accurate picture of sea ice movement, which can help scientists understand and predict climate change.
* **Continuous motion tracking**: Unlike traditional methods, which only provide motion estimates at specific buoy locations, optical flow produces a spatially continuous drift field, providing motion estimates for every image pixel.

Overall, this study demonstrates the potential of deep learning techniques for accurately estimating sea ice drift, which can have a significant impact on our understanding of the Arctic and its role in the Earth's climate system."
cs.CV,"All You Need for Object Detection: From Pixels, Points, and Prompts to   Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles","Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.",http://arxiv.org/abs/2510.26641v1,2025-10-30T16:08:25Z,"Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi","**Advances in Object Detection for Autonomous Vehicles**

Autonomous vehicles (AVs) are revolutionizing transportation, but their success relies on accurately detecting objects in complex environments. A recent research paper explores the current state of object detection in AVs, highlighting emerging trends and technologies that could shape the future of autonomous driving.

**The Challenge of Object Detection**

Object detection is a critical component of AVs, as it enables vehicles to perceive and respond to their surroundings. However, detecting objects in complex environments, such as urban streets or highways, is a challenging task. Current object detection systems rely on a combination of sensors, including cameras, ultrasonic sensors, LiDAR, and radar.

**Emerging Trends and Technologies**

The paper highlights several emerging trends and technologies that could improve object detection in AVs:

1. **Vision-Language Models (VLMs) and Large Language Models (LLMs)**: These AI models can process and understand vast amounts of data, including images, text, and sensor data. By integrating VLMs and LLMs with AV sensors, researchers aim to create more robust and accurate object detection systems.
2. **Sensor Fusion**: Combining data from multiple sensors, such as cameras, LiDAR, and radar, can provide a more comprehensive understanding of the environment. The paper explores various sensor fusion strategies and their potential to improve object detection.
3. **Generative AI**: Generative AI models, such as Generative Adversarial Networks (GANs), can generate synthetic data, which can be used to train and test object detection systems.

**Datasets and Methodologies**

The paper also discusses the importance of high-quality datasets for training and testing object detection systems. It introduces a structured categorization of AV datasets, including ego-vehicle, infrastructure-based, and cooperative datasets. Additionally, the paper analyzes cutting-edge detection methodologies, including 2D and 3D pipelines, hybrid sensor fusion, and transformer-driven approaches.

**Roadmap for Future Research**

The paper provides a clear roadmap for future research in object detection for AVs, highlighting current capabilities, open challenges, and future opportunities. By synthesizing these perspectives, the authors aim to accelerate progress in this field and enable the development of more reliable and efficient AV systems.

Overall, the paper offers a comprehensive overview of the current state of object detection in AVs, highlighting emerging trends and technologies that could shape the future of autonomous driving."
cs.CV,SAMRI: Segment Anything Model for MRI,"Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.",http://arxiv.org/abs/2510.26635v1,2025-10-30T16:04:00Z,"Zhao Wang, Wei Dai, Thuy Thanh Dao, Steffen Bollmann, Hongfu Sun, Craig Engstrom, Shekhar S. Chandra","**Breakthrough in MRI Analysis: AI Model Achieves High Accuracy**

Magnetic Resonance Imaging (MRI) is a crucial tool for diagnosing and monitoring various medical conditions. However, accurately segmenting MRI images - identifying specific areas of interest - is a time-consuming and labor-intensive process. Researchers have now developed a new AI model, called SAMRI, which can efficiently and accurately segment MRI images.

SAMRI is an adaptation of the Segment Anything Model (SAM), a powerful AI model that has shown impressive results in analyzing natural images. By fine-tuning SAM for MRI images, the researchers achieved a high level of accuracy, with a mean Dice score of 0.87. This score indicates that SAMRI can accurately identify specific areas in MRI images, even in cases where the images vary in quality or protocol.

The significance of SAMRI lies in its ability to generalize well across different types of MRI images, including those of various organs and pathologies. This is particularly important for identifying small and clinically important structures, which can be challenging to segment accurately. By leveraging SAMRI, clinicians can potentially make more accurate diagnoses and develop more effective treatment plans.

Notably, the researchers were able to adapt SAM to MRI images with a significant reduction in training time and computational resources. This breakthrough has the potential to accelerate the adoption of AI in clinical settings, ultimately improving patient care."
cs.AI,Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with   the MME-CoF Benchmark,"Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",http://arxiv.org/abs/2510.26802v1,2025-10-30T17:59:55Z,"Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng","**Can Video Models Think for Themselves?**

Researchers investigated whether video generation models, like those used to create realistic videos, can also reason and make logical connections on their own. They tested a popular video model called Veo-3 with a new benchmark called MME-CoF, which evaluates its ability to reason in 12 different areas, such as understanding spatial relationships, physics, and logic.

The results showed that while video models can generate coherent and realistic videos, they are not yet reliable as standalone ""zero-shot reasoners,"" meaning they can't think for themselves in complex visual reasoning scenarios. However, they did demonstrate promising reasoning patterns in certain areas, such as short-term spatial coherence and local dynamics.

The study suggests that video models can still be useful as complementary visual engines, working alongside dedicated reasoning models to improve overall performance. In other words, video models can provide valuable visual information, but may need to be paired with other models that specialize in logical reasoning to achieve more complex tasks.

Overall, the study provides a comprehensive evaluation of video models as zero-shot reasoners and highlights areas where they need improvement."
cs.AI,Gistify! Codebase-Level Understanding via Runtime Execution,"As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.",http://arxiv.org/abs/2510.26790v1,2025-10-30T17:58:26Z,"Hyunji Lee, Minseon Kim, Chinmay Singh, Matheus Pereira, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal, Zhengyan Shi, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan, Lucas Caccia","Here's a summary of the research paper for a general audience:

**Introducing Gistify: A New Challenge for AI Coding Tools**

Imagine you have a huge library of code with thousands of lines, and you want to teach an artificial intelligence (AI) tool to understand how it works. Researchers have proposed a new task called Gistify, which tests an AI tool's ability to distill the essential parts of a large codebase into a single, small file that can still perform a specific task.

The goal of Gistify is to see if an AI tool can analyze a large codebase, identify the crucial components, and create a minimal file that can replicate the output of a specific command. This requires the AI tool to have a deep understanding of the codebase's structure, how it executes, and how to produce new code.

The researchers found that even the best AI coding tools struggle with Gistify, especially when the code has complex execution paths. This highlights the need for further research and development to create more advanced AI tools that can truly understand and work with large codebases.

In simple terms, Gistify is a new benchmark for AI coding tools that challenges them to extract the essence of a large codebase and recreate a specific functionality in a tiny file. This task has important implications for the development of more sophisticated AI tools that can assist with coding and software development."
cs.AI,Defeating the Training-Inference Mismatch via FP16,"Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",http://arxiv.org/abs/2510.26788v1,2025-10-30T17:58:11Z,"Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin","**Improving Stability in AI Training: A Simple yet Effective Solution**

Researchers have identified a major issue in fine-tuning large language models using reinforcement learning (RL), a technique that helps AI systems learn from trial and error. The problem, known as the ""training-inference mismatch,"" causes instability in the training process, leading to suboptimal performance. 

The root cause of this issue lies in the way computers store and process numbers, specifically in the use of a type of numerical precision called BF16. While BF16 has a wide range of values, it can introduce significant rounding errors that disrupt the consistency between training and inference (the process of making predictions).

Surprisingly, the researchers found that switching to a different numerical precision, FP16, resolves this issue. This change is easy to implement, requires no modifications to the AI model or learning algorithm, and can be done with just a few lines of code.

The results show that using FP16 leads to more stable optimization, faster convergence, and better performance across various tasks, algorithms, and frameworks. This simple yet effective solution has the potential to improve the efficiency and effectiveness of AI training, and may lead to a re-evaluation of numerical precision in RL fine-tuning."
cs.AI,Remote Labor Index: Measuring AI Automation of Remote Work,"AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",http://arxiv.org/abs/2510.26787v1,2025-10-30T17:58:04Z,"Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks","**The Future of Remote Work: How Well Can AI Automate Jobs?**

Imagine a world where artificial intelligence (AI) can do your job from anywhere. Sounds convenient, but how close are we to that reality? A new study introduces the Remote Labor Index (RLI), a tool that measures how well AI can automate real-world work tasks remotely.

The researchers tested various AI agents on a range of practical projects and found that they performed poorly, with the best AI agent able to automate only 2.5% of the tasks. This means that AI is not yet ready to replace human workers, at least not for most jobs.

The study provides a realistic snapshot of AI's capabilities and limitations, helping to separate hype from reality. By tracking AI's progress over time, the RLI can help businesses, policymakers, and workers prepare for the changes that AI-driven automation may bring to the workforce. In short, the study gives us a clearer picture of how AI can and can't be used to automate remote work, and what that means for the future of work."
cs.AI,LLMs Process Lists With General Filter Heads,"We investigate the mechanisms underlying a range of list-processing tasks in LLMs, and we find that LLMs have learned to encode a compact, causal representation of a general filtering operation that mirrors the generic ""filter"" function of functional programming. Using causal mediation analysis on a diverse set of list-processing tasks, we find that a small number of attention heads, which we dub filter heads, encode a compact representation of the filtering predicate in their query states at certain tokens. We demonstrate that this predicate representation is general and portable: it can be extracted and reapplied to execute the same filtering operation on different collections, presented in different formats, languages, or even in tasks. However, we also identify situations where transformer LMs can exploit a different strategy for filtering: eagerly evaluating if an item satisfies the predicate and storing this intermediate result as a flag directly in the item representations. Our results reveal that transformer LMs can develop human-interpretable implementations of abstract computational operations that generalize in ways that are surprisingly similar to strategies used in traditional functional programming patterns.",http://arxiv.org/abs/2510.26784v1,2025-10-30T17:57:17Z,"Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, David Bau","**Unlocking How Large Language Models Process Lists**

Imagine you're searching for specific items on a shopping list or filtering out irrelevant information from a long text. Large language models (LLMs), like those used in chatbots and virtual assistants, are surprisingly good at these tasks. But have you ever wondered how they do it?

Researchers investigated how LLMs process lists and found that they've learned to use a simple, yet powerful, ""filter"" function, similar to those used in computer programming. This function allows LLMs to quickly identify and extract specific information from lists.

The study discovered that a small part of the LLM, called ""filter heads,"" plays a crucial role in this process. These filter heads can be thought of as a compact representation of the filtering rules, which can be applied to different lists, even if they're in different formats or languages.

However, the researchers also found that LLMs can use a different strategy in certain situations, where they evaluate each item individually and store the results. This shows that LLMs can develop clever and human-understandable ways to solve complex tasks, similar to those used in traditional computer programming.

Overall, this research provides new insights into how LLMs work and how they can be used to improve various applications, from virtual assistants to language translation tools."
cs.AI,Clone Deterministic 3D Worlds with Geometrically-Regularized World   Models,"A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",http://arxiv.org/abs/2510.26782v1,2025-10-30T17:56:43Z,"Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen","**Breakthrough in Artificial Intelligence: Creating a More Accurate Virtual World**

Imagine a computer program that can simulate a 3D world, like a video game, and predict what will happen next. This is called a ""world model,"" and it's essential for developing intelligent agents that can think, plan, and make decisions in complex environments. However, current world models have limitations and can become inaccurate over time.

Researchers have made a significant step forward in creating a more accurate world model. They introduced a new approach called Geometrically-Regularized World Models (GRWM), which improves the way the model represents the environment. By ensuring that similar points in the environment are close together in the model's internal representation, GRWM creates a more accurate and stable simulation.

The results are impressive: GRWM significantly improves the model's ability to predict what will happen next in a 3D environment, even over long periods. This is achieved without making the model more complex or changing its overall architecture. The researchers found that the key to this success is learning a better internal representation of the environment, which allows the model to make more accurate predictions.

This breakthrough has important implications for the development of intelligent agents that can interact with complex environments, such as robots or autonomous vehicles. With more accurate world models, these agents can make better decisions and navigate their surroundings more effectively."
cs.AI,Faithful and Fast Influence Function via Advanced Sampling,"How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.",http://arxiv.org/abs/2510.26776v2,2025-10-30T17:55:19Z,"Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang","Here's a summary of the research paper for a general audience:

**Understanding How Training Data Affects AI Models**

Imagine you're training a self-driving car to recognize different types of cars. You show it many pictures of various cars, and it learns to identify them. But have you ever wondered which specific pictures helped the car learn to recognize a particular type of car? This is a challenge in machine learning, especially with complex models that are difficult to understand.

Researchers have developed a tool called influence functions (IFs) to help explain how individual pieces of training data, like a single picture, affect the model's behavior. However, computing IFs for large datasets can be time-consuming and requires a lot of computer resources.

To solve this problem, the researchers proposed two new methods for selecting a small, representative subset of the training data. These methods help reduce the computational resources needed to compute IFs while maintaining their accuracy.

**Key Benefits**

The new methods:

* Reduce computation time by 30%
* Decrease memory usage by 42%
* Improve the accuracy of IF estimates by 2.5%

**Why it Matters**

By making it faster and more efficient to understand how training data affects AI models, this research can help:

* Improve the transparency and trustworthiness of AI models
* Identify and mitigate potential biases in AI decision-making
* Enhance the overall performance and reliability of AI systems

Overall, this research provides a significant advancement in understanding how training data influences AI models, making it a valuable contribution to the field of machine learning."
cs.AI,STaMP: Sequence Transformation and Mixed Precision for Low-Precision   Activation Quantization,"Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models. However, accuracy often degrades sharply when activations are quantized below eight bits. Recent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. In this paper, we propose \textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a novel strategy that applies linear transformations along the \textit{sequence} dimension to exploit the strong local correlation in language and visual data. By keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths. We evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.",http://arxiv.org/abs/2510.26771v1,2025-10-30T17:53:42Z,"Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel","Here's a summary of the research paper for a general audience:

**Making AI Models More Efficient**

Artificial intelligence (AI) models are becoming increasingly powerful, but they also require a lot of computing power and memory to run. One way to make them more efficient is to reduce the amount of data they use to make decisions. This process is called ""quantization.""

**The Problem with Quantization**

However, when AI models use very low-precision data (like 4 or 5 bits), they often make less accurate predictions. This is especially true for certain types of data, like language and images.

**A New Solution: STaMP**

Researchers have proposed a new method called Sequence Transformation and Mixed Precision (STaMP) to improve the accuracy of AI models that use low-precision data. STaMP applies a mathematical transformation to the data along a specific dimension (like time or sequence) to take advantage of the strong correlations in language and visual data.

**How STaMP Works**

The key innovation of STaMP is to keep a small portion of the data at higher precision, while still using lower precision for the rest of the data. This approach allows AI models to maintain their accuracy even when using lower precision data.

**Results**

The researchers tested STaMP on several recent AI architectures and found that it significantly improves the accuracy of AI models that use low-precision data. This method can be used in combination with other techniques to further improve the efficiency of AI models.

Overall, STaMP is a promising new approach to making AI models more efficient and accurate, which could have significant implications for applications like natural language processing, computer vision, and more."
cs.AI,AMO-Bench: Large Language Models Still Struggle in High School Math   Competitions,"We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/",http://arxiv.org/abs/2510.26768v1,2025-10-30T17:52:02Z,"Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou","**Large Language Models Struggle with Advanced Math**

Researchers have created a new benchmark, called AMO-Bench, to test the math skills of large language models (LLMs). The benchmark consists of 50 challenging math problems from high-level competitions, such as the International Mathematical Olympiad. The goal is to assess how well LLMs can reason and solve complex math problems.

The results show that even the best LLMs struggle with these advanced math problems, achieving an accuracy of only 52.4%. Most models scored below 40%, indicating significant room for improvement. The researchers also found that increasing the computational power and time allowed to solve the problems can lead to better performance, but it's still not enough to match human-level math skills.

The AMO-Bench benchmark is designed to push the limits of LLMs' mathematical reasoning abilities and encourage further research in this area. The findings highlight the need for more advanced and effective methods to improve the math skills of LLMs, which is essential for their application in various fields, such as science, engineering, and education."
cs.AI,The Oversight Game: Learning to Cooperatively Balance an AI Agent's   Safety and Autonomy,"As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.",http://arxiv.org/abs/2510.26752v1,2025-10-30T17:46:49Z,"William Overman, Mohsen Bayati","**Making AI Safer: A New Approach to Balancing Autonomy and Control**

As AI agents become more capable, ensuring their safety and alignment with human values is crucial. Researchers have proposed a new approach to achieve this balance by creating a control interface that allows humans and AI agents to work together. The goal is to enable the AI agent to make decisions autonomously while still allowing humans to intervene when necessary.

Imagine a game where the AI agent decides whether to act on its own or ask for human input, while the human decides whether to trust the agent or oversee its actions. This interaction is modeled as a game, where both players learn to cooperate and balance the agent's autonomy with human oversight.

The researchers found that under certain conditions, this game can be designed to ensure that the AI agent's autonomous decisions benefit both itself and the human. In other words, as the agent becomes more autonomous, it will not harm the human's interests.

The approach was tested in a simulated environment, where the AI agent and human learned to work together through trial and error. The results showed that the agent learned to ask for human input when uncertain and the human learned when to oversee the agent's actions. This collaboration led to safe and efficient decision-making, without requiring changes to the agent's underlying system.

This research provides a promising solution for making AI agents safer and more aligned with human values, even after they have been deployed. By creating a transparent control layer, humans and AI agents can work together to achieve common goals while minimizing the risk of safety violations."
cs.AI,Deep sequence models tend to memorize geometrically; it is unclear why,"In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task.   From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.   Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.",http://arxiv.org/abs/2510.26745v1,2025-10-30T17:40:22Z,"Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar","Here's a summary of the research paper for a general audience:

**Researchers Discover That AI Models Can Memorize in a Surprising Way**

When we train artificial intelligence (AI) models to process sequences of information, such as text or images, we often think of their memory as a simple lookup table that stores relationships between nearby pieces of information. However, a new study suggests that these models may be capable of memorizing in a more complex and geometric way.

The researchers found that a type of AI model called a Transformer can learn to represent relationships between entities in a way that goes beyond simple associations. Instead of just storing local connections between nearby pieces of information, the model seems to create a geometric map of relationships between all entities, including those that are not directly connected.

What's surprising is that this geometric representation emerges even when the model is only trained on local associations, and it's not clear why this happens. The researchers analyzed the model's behavior and found that it's related to a phenomenon called spectral bias, which is a tendency for the model to favor certain types of solutions.

This discovery has implications for how we understand AI models and how we design them. It suggests that these models may be capable of more complex and nuanced representations of information than we previously thought, and that we may be able to improve their performance by encouraging them to learn more geometric representations. Ultimately, this research challenges our current understanding of how AI models work and encourages us to rethink our assumptions about their capabilities."
cs.AI,A General Incentives-Based Framework for Fairness in Multi-agent   Resource Allocation,"We introduce the General Incentives-based Framework for Fairness (GIFF), a novel approach for fair multi-agent resource allocation that infers fair decision-making from standard value functions. In resource-constrained settings, agents optimizing for efficiency often create inequitable outcomes. Our approach leverages the action-value (Q-)function to balance efficiency and fairness without requiring additional training. Specifically, our method computes a local fairness gain for each action and introduces a counterfactual advantage correction term to discourage over-allocation to already well-off agents. This approach is formalized within a centralized control setting, where an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.   Empirical evaluations across diverse domains, including dynamic ridesharing, homelessness prevention, and a complex job allocation task-demonstrate that our framework consistently outperforms strong baselines and can discover far-sighted, equitable policies. The framework's effectiveness is supported by a theoretical foundation; we prove its fairness surrogate is a principled lower bound on the true fairness improvement and that its trade-off parameter offers monotonic tuning. Our findings establish GIFF as a robust and principled framework for leveraging standard reinforcement learning components to achieve more equitable outcomes in complex multi-agent systems.",http://arxiv.org/abs/2510.26740v1,2025-10-30T17:37:51Z,"Ashwin Kumar, William Yeoh","**Fair Resource Allocation: A New Approach**

Imagine a world where resources are scarce and multiple individuals or agents are competing for them. How can we ensure that the allocation of these resources is fair and doesn't favor some individuals over others? Researchers have proposed a new framework called the General Incentives-based Framework for Fairness (GIFF) to address this challenge.

**The Problem with Current Methods**

Current methods often prioritize efficiency, which can lead to unequal outcomes. For example, in a ridesharing system, the most efficient solution might be to prioritize routes that serve the most passengers, but this could leave some areas or individuals underserved.

**The GIFF Solution**

GIFF is a novel approach that balances efficiency and fairness without requiring additional training. It works by:

1. Computing a ""fairness gain"" for each possible action
2. Introducing a correction term to discourage over-allocation to already well-off agents

This approach is tested in various domains, including:

* Dynamic ridesharing
* Homelessness prevention
* Job allocation

**Key Findings**

The results show that GIFF consistently outperforms existing methods and can discover equitable policies that benefit all agents. The framework is also backed by a strong theoretical foundation, which proves that its fairness surrogate is a reliable indicator of fairness improvement.

**Implications**

The GIFF framework offers a robust and principled approach to achieving more equitable outcomes in complex multi-agent systems. Its applications can be far-reaching, from optimizing resource allocation in transportation systems to ensuring fairness in social services. By leveraging standard reinforcement learning components, GIFF provides a promising solution for creating more just and equitable societies."
cs.AI,Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models,"This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).   We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.   The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.",http://arxiv.org/abs/2510.26732v1,2025-10-30T17:31:03Z,"J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot","**Evaluating the Reasoning Abilities of AI Models Across Different Platforms**

Researchers have conducted a comprehensive study to evaluate the reasoning capabilities of advanced AI models, known as foundation models, across various platforms and academic domains. The study aimed to assess the performance of these models in solving complex problems in fields such as physics, mathematics, chemistry, economics, and biology.

The researchers evaluated 15 AI models on 79 problems using three different computational platforms: a supercomputer, a cloud platform, and a university cluster. The study consisted of three phases: establishing a baseline performance, validating the results across different platforms, and conducting an extended evaluation of the models' performance.

The findings of the study challenge the common assumption that larger AI models perform better. Instead, the researchers found that the quality of the training data is more critical than the size of the model in determining its reasoning abilities. The study provides guidelines for selecting the most suitable AI models for various applications, including education, research, and production.

The study's methodology and benchmark can be used to track the evolution of AI models' reasoning capabilities over time, enabling the development of more advanced and effective AI systems. Overall, the study provides valuable insights into the strengths and limitations of current AI models and highlights the importance of high-quality training data in developing robust AI systems."
cs.AI,ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for   Efficient MoE Inference,"The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.   To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.",http://arxiv.org/abs/2510.26730v1,2025-10-30T17:29:27Z,"Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang","**Improving Efficiency in Large Language Models**

Large language models are becoming increasingly powerful, but they require a lot of memory to run. To address this issue, researchers have developed a technique called Mixture-of-Experts (MoE), which only activates a small portion of the model's parameters during inference. However, current methods for implementing MoE can still be slow due to the need to constantly transfer data between different types of memory.

To solve this problem, a team of researchers has developed a new system called ExpertFlow. ExpertFlow is a smart scheduler that predicts which parts of the model will be needed next and prepares them in advance, reducing the need for slow memory transfers. It also uses a clever routing system to minimize the number of times data needs to be transferred.

The result is a significant improvement in efficiency, with ExpertFlow reducing the time spent waiting for data to be transferred to less than 0.1% of the time taken by previous methods. This breakthrough has the potential to enable faster and more efficient deployment of large language models on devices with limited memory, paving the way for more widespread adoption of AI technologies."
cs.AI,Non-Convex Over-the-Air Heterogeneous Federated Learning: A   Bias-Variance Trade-off,"Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.",http://arxiv.org/abs/2510.26722v2,2025-10-30T17:22:57Z,"Muhammad Faraz Ul Abrar, Nicolò Michelusi","Here's a summary of the research paper for a general audience:

**Title:** Improving Wireless Machine Learning with a Trade-off

**What it's about:** Machine learning models are getting better and better, but they often require a lot of data and computing power. One way to speed up the process is to use a technique called federated learning, where many devices (like phones or computers) work together to train a model. However, when these devices communicate with each other over wireless networks, their signals can interfere with each other, making it harder to train the model.

**The problem:** Previous solutions to this problem assumed that all devices had equal communication conditions, which isn't always the case. For example, some devices might be farther away from the base station or have weaker signals. This can slow down the training process and make the model less accurate.

**The breakthrough:** Researchers have developed a new approach that allows for a controlled amount of ""bias"" in the model updates, which helps to reduce the impact of interference. They also developed a new algorithm that optimizes the trade-off between bias and variance (a measure of how much the model updates vary). This approach works for complex, non-convex machine learning models, like those used for image classification.

**The result:** The new approach accelerates the training process and improves the accuracy of the model. In experiments, it outperformed previous methods, especially in situations where devices had different communication conditions. This research has the potential to improve the efficiency and accuracy of machine learning models in wireless networks."
cs.AI,Unveiling Intrinsic Text Bias in Multimodal Large Language Models   through Attention Key-Space Analysis,"Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.",http://arxiv.org/abs/2510.26721v1,2025-10-30T17:22:22Z,"Xinhan Zheng, Huyu Wu, Xueting Wang, Haiyun Jiang","**Uncovering the Hidden Bias in Multimodal AI Models**

Multimodal large language models (MLLMs) are AI systems that can process both text and images. However, researchers have noticed that these models tend to favor text over images when making decisions, which can limit their ability to reason effectively from visual evidence.

A new study suggests that this ""text bias"" is not due to external factors such as imbalanced data or training methods, but rather a fundamental issue with the model's internal architecture. Specifically, the researchers found that the model's attention mechanism, which helps it focus on relevant information, treats visual and textual inputs differently.

By analyzing the model's internal workings, the researchers discovered that visual and textual inputs occupy distinct subspaces within the attention space, leading to a systematic under-utilization of visual information. This finding reveals that text bias arises from an intrinsic misalignment within the attention key space, rather than solely from external data factors.

The study's results have important implications for the development of more balanced and effective multimodal AI models. By understanding the root causes of text bias, researchers can design better models that can effectively integrate visual and textual information."
cs.AI,On the limitation of evaluating machine unlearning using only a single   training seed,"Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comparisons of MU algorithms should also reflect the variability across different model training seeds.",http://arxiv.org/abs/2510.26714v2,2025-10-30T17:13:42Z,"Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma","Here's a summary of the research paper for a general audience:

**The Limitations of Testing Machine Unlearning**

Imagine you've trained a computer model to recognize pictures, but you later realize that one of the pictures used to train it was incorrect. You'd want to remove the influence of that picture from the model without having to retrain it from scratch. This is called ""machine unlearning"" (MU).

Researchers have developed methods to approximate machine unlearning, but these methods aren't perfect and can only be evaluated through experiments. A common way to test these methods is to run them multiple times on the same trained model. However, a new study shows that this approach can be flawed.

The study found that some machine unlearning methods can produce very different results depending on the random numbers used to train the model in the first place. This means that if you only test a method on one trained model, you might get misleading results.

The researchers recommend that future studies test machine unlearning methods more thoroughly by training multiple models with different random numbers and evaluating the methods on each one. This will give a more accurate picture of how well these methods really work."
cs.AI,Delegated Authorization for Agents Constrained to Semantic Task-to-Scope   Matching,"Authorizing Large Language Model driven agents to dynamically invoke tools and access protected resources introduces significant risks, since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope. We introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources, and issue access tokens constrained to the minimal set of scopes necessary for the agents' assigned tasks. Given the unavailability of datasets centered on delegated authorization flows, particularly including both semantically appropriate and inappropriate scope requests for a given task, we introduce ASTRA, a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes. Our experiments show both the potential and current limitations of model-based matching, particularly as the number of scopes needed for task completion increases. Our results highlight the need for further research into semantic matching techniques enabling intent-aware authorization for multi-agent and tool-augmented applications, including fine-grained control, such as Task-Based Access Control (TBAC).",http://arxiv.org/abs/2510.26702v1,2025-10-30T17:07:00Z,"Majed El Helou, Chiara Troiani, Benjamin Ryder, Jean Diaconu, Hervé Muyal, Marcelo Yannuzzi","Here's a summary of the research paper for a general audience:

**Title:** Safer AI Agents: Limiting Access to Sensitive Information

**Summary:** As AI-powered agents become more autonomous, they need to access various tools and resources to perform tasks. However, giving them too much access can lead to risks, such as operating outside their intended scope. Researchers have developed a new authorization system that helps limit AI agents' access to only the resources they need to complete a specific task.

The system uses a semantic matching approach to ensure that agents only receive access tokens that are relevant to their assigned tasks. To test this system, the researchers created a dataset called ASTRA, which includes examples of both suitable and unsuitable access requests for various tasks.

The study found that while the system shows promise, it also has limitations, particularly when agents require access to multiple resources to complete a task. The results highlight the need for further research into developing more sophisticated techniques for controlling access to sensitive information, enabling safer and more secure AI applications.

**In simple terms:** Imagine a smart home assistant that can control lights, thermostats, and locks. Currently, such agents might have broad access to all these devices, which could lead to unintended consequences. This research aims to create a more fine-grained system that only allows the agent to access the specific devices it needs to perform a task, reducing potential risks and improving overall safety."
cs.AI,The End of Manual Decoding: Towards Truly End-to-End Language Models,"The ""end-to-end"" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly ""end-to-end"" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.   Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from ""hacking the test set""-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., ""generate with low randomness"") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.",http://arxiv.org/abs/2510.26697v2,2025-10-30T17:01:43Z,"Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang","**Breakthrough in Language Models: Towards Truly End-to-End Generation**

Large Language Models (LLMs) have become increasingly popular, but their ""end-to-end"" label is misleading. Currently, these models rely on a manual decoding process that requires tedious fine-tuning of settings, such as temperature and top-p. A new architecture, called AutoDeco, has been developed to overcome this limitation.

AutoDeco enables truly ""end-to-end"" generation by learning to control its own decoding strategy. This approach allows the model to dynamically adjust its settings on a token-by-token basis, eliminating the need for manual tuning. In extensive experiments, AutoDeco outperformed default decoding strategies and achieved performance comparable to an ""oracle-tuned"" baseline, which is a practical upper bound for any static method.

The most exciting aspect of AutoDeco is its ability to interpret natural language commands, such as ""generate with low randomness."" This allows for steerable and interactive LLM decoding, opening up new possibilities for controlling the output of language models. With AutoDeco, the future of language models looks more efficient, flexible, and user-friendly."
cs.AI,Process Integrated Computer Vision for Real-Time Failure Prediction in   Steel Rolling Mill,"We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.",http://arxiv.org/abs/2510.26684v1,2025-10-30T16:54:16Z,"Vaibhav Kurrey, Sivakalyan Pujari, Gagan Raj Gupta","Here's a summary of the research paper for a general audience:

**Predicting Equipment Failures in Steel Mills with Computer Vision**

Researchers have developed a system that uses computer vision and artificial intelligence to predict equipment failures in steel rolling mills. The system uses cameras to monitor the equipment and production process in real-time, and analyzes the video feeds using deep learning models. This allows for early detection of potential problems, enabling proactive maintenance and reducing costly unplanned breakdowns.

The system is designed to be scalable and easy to deploy, with minimal additional resources required. By combining visual data with sensor data, the system can not only predict failures but also identify the location and likely cause of the problem. This integrated approach aims to improve the reliability, productivity, and profitability of industrial manufacturing environments, such as steel mills.

In simple terms, the system is like a ""predictive maintenance"" tool that helps steel mills avoid equipment failures and stay operational, by using cameras and AI to monitor and analyze the production process in real-time."
cs.CL,Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with   the MME-CoF Benchmark,"Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",http://arxiv.org/abs/2510.26802v1,2025-10-30T17:59:55Z,"Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng","**Can Video Models Think for Themselves?**

Researchers investigated whether video generation models, like those used to create realistic videos, can also reason and think critically about visual information. They tested a popular video model called Veo-3 on its ability to reason in various scenarios, such as understanding spatial relationships, physics, and logic.

The study found that while Veo-3 can produce impressive results in some areas, such as short-term spatial coherence and fine-grained details, it struggles with more complex reasoning tasks, like long-term causal relationships and abstract logic. The researchers created a benchmark called MME-CoF to evaluate the model's performance and identified both its strengths and weaknesses.

Overall, the study suggests that current video models are not yet reliable as standalone ""zero-shot reasoners,"" meaning they can't think for themselves without additional guidance. However, they show promise as complementary tools that can work alongside dedicated reasoning models to enhance visual understanding."
cs.CL,Gistify! Codebase-Level Understanding via Runtime Execution,"As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.",http://arxiv.org/abs/2510.26790v1,2025-10-30T17:58:26Z,"Hyunji Lee, Minseon Kim, Chinmay Singh, Matheus Pereira, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal, Zhengyan Shi, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan, Lucas Caccia","**Simplifying Complex Code: A New AI Challenge**

Imagine you're trying to understand a huge, complex software project with millions of lines of code. How would you distill it down to just the essential parts that make it work? Researchers have proposed a new challenge called ""Gistify"" to test the abilities of artificial intelligence (AI) models that write code.

The goal of Gistify is to take a large codebase and a specific command, and then generate a tiny, self-contained file that can replicate the exact output of that command. This requires the AI model to have a deep understanding of the codebase's structure, how it executes, and what parts are crucial to its functionality.

The researchers found that even the best AI models today struggle with Gistify, especially when the code has long execution paths. This highlights the need for more advanced AI models that can truly comprehend complex codebases and simplify them into their essential components. The Gistify challenge has the potential to drive innovation in AI-powered coding and improve the way we interact with complex software systems."
cs.CL,Defeating the Training-Inference Mismatch via FP16,"Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",http://arxiv.org/abs/2510.26788v1,2025-10-30T17:58:11Z,"Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin","Here's a summary of the research paper for a general audience:

**Improving AI Training with a Simple Switch**

Researchers have identified a problem with training large language models, a type of artificial intelligence (AI) used for tasks like chatbots and language translation. The issue arises from a mismatch between how the AI is trained and how it's used in real-world applications. This mismatch can cause instability and slow down the training process.

The researchers found that the root cause of the problem lies in the way computers store and process numbers, specifically with the use of a type of numerical precision called BF16. They discovered that switching to a different type of numerical precision, called FP16, can eliminate this mismatch.

The good news is that this change is easy to implement and requires no major modifications to the AI model or training process. The researchers tested their approach and found that it leads to more stable training, faster convergence, and better performance across a range of tasks and AI frameworks.

In simple terms, the researchers propose a simple ""fix"" that can improve the training of large language models, making them more efficient and effective. This finding could have significant implications for the development of AI systems and could lead to more widespread adoption of these technologies."
cs.CL,Remote Labor Index: Measuring AI Automation of Remote Work,"AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",http://arxiv.org/abs/2510.26787v1,2025-10-30T17:58:04Z,"Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks","**The Future of Remote Work: How Well Can AI Automate Tasks?**

Researchers have made significant progress in developing artificial intelligence (AI) that can perform complex tasks, but it's unclear how much these advancements can actually replace human workers. To get a better sense of AI's capabilities, a team of researchers created the Remote Labor Index (RLI), a test that evaluates how well AI agents can complete real-world tasks that people do remotely.

The results show that AI agents are not yet very good at automating these tasks, with the best AI agent able to complete only 2.5% of the tasks on its own. This suggests that while AI has made progress, it still has a long way to go before it can significantly replace human workers.

The Remote Labor Index provides a new way to measure the impact of AI on work and will help researchers, policymakers, and business leaders understand how AI is changing the job market. By tracking AI's progress over time, stakeholders can prepare for the changes that AI-driven automation will bring and make informed decisions about how to navigate its effects on the workforce."
cs.CL,AMO-Bench: Large Language Models Still Struggle in High School Math   Competitions,"We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/",http://arxiv.org/abs/2510.26768v1,2025-10-30T17:52:02Z,"Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou","**Large Language Models Struggle with Advanced Math**

Researchers have created a new benchmark called AMO-Bench to test the math skills of large language models (LLMs), like those used in chatbots and virtual assistants. The benchmark consists of 50 challenging math problems from high-level competitions, verified by experts to be as difficult as those in the International Mathematical Olympiad.

The results are surprising: even the best-performing LLMs got only about 52% of the problems correct, with most models scoring below 40%. This shows that current LLMs still have a long way to go in terms of mathematical reasoning.

The good news is that the researchers found that LLMs can improve with more computing power and time to think. This suggests that there's potential for future advancements in LLM math skills.

The AMO-Bench benchmark is now available to help researchers and developers improve the math abilities of LLMs, which could have significant implications for fields like education, science, and engineering."
cs.CL,Deep sequence models tend to memorize geometrically; it is unclear why,"In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task.   From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.   Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.",http://arxiv.org/abs/2510.26745v1,2025-10-30T17:40:22Z,"Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar","**Unlocking the Secrets of How AI Models Store Information**

Imagine you're trying to remember a phone number. You might recall it by associating it with a specific person or event. This is similar to how AI models, like those used in language processing, store information. They often rely on simple connections between words or entities to remember facts.

However, researchers have discovered that some AI models, specifically those using a type of neural network called Transformers, can store information in a more complex and geometric way. This means that the model creates a mental map of relationships between all entities, not just those that are directly connected.

The surprising thing is that this geometric approach emerges even when the model is only trained on local associations between entities, and not explicitly taught to create a global map. The researchers found that this phenomenon simplifies complex reasoning tasks and leads to the creation of an elegant geometry that is hard to explain.

The study also reveals that this geometric approach is not just a result of the model's architecture or optimization techniques. Rather, it arises from a natural bias in the model's learning process, which favors smooth and continuous representations.

The findings have implications for areas like knowledge acquisition, capacity, and discovery, and may lead to the development of more efficient and effective AI models. By understanding how AI models store information, researchers can design better models that can learn and reason more effectively."
cs.CL,Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models,"This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).   We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.   The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.",http://arxiv.org/abs/2510.26732v1,2025-10-30T17:31:03Z,"J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot","**Evaluating the Reasoning Abilities of AI Models Across Different Platforms**

Researchers have conducted a comprehensive study to assess the reasoning capabilities of advanced AI models, known as foundation models, across various platforms and academic domains. The study evaluated 15 AI models on 79 problems in subjects like physics, mathematics, chemistry, and economics, using three different computing infrastructures: a supercomputer, a cloud platform, and a university cluster.

The study's key findings:

1. **Training data quality matters more than model size**: The researchers discovered that the quality of the data used to train the AI models is more important than the size of the model itself. This challenges the common assumption that larger models always perform better.
2. **Reasoning capabilities vary across models and platforms**: The study showed that different AI models and computing platforms can affect the performance of the models. This highlights the need for careful model selection and evaluation.
3. **Guidelines for model selection**: The researchers provided actionable guidelines for choosing the right AI model for various applications, including education, production, and research.

The study's methodology and benchmark can be used to track the evolution of AI models' reasoning capabilities over time, enabling longitudinal evaluation and improvement of these models. Overall, the study provides valuable insights into the strengths and limitations of current AI models and can inform the development of more effective AI systems."
cs.CL,Value Drifts: Tracing Value Alignment During LLM Post-Training,"As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.",http://arxiv.org/abs/2510.26707v1,2025-10-30T17:09:09Z,"Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Vered Shwartz, Siva Reddy","Here's a summary of the research paper for a general audience:

**Understanding How AI Models Learn Human Values**

As AI models become more integrated into our lives, it's essential that they align with human values and ethics. Researchers have been studying how well AI models reflect human values, but most studies focus on fully trained models. This new study explores how AI models learn and adopt human values during their training process.

The researchers investigated two popular AI models, Llama-3 and Qwen-3, and found that the values of these models are primarily established during a phase called supervised fine-tuning (SFT). Interestingly, further training using preference optimization, which aims to refine the model's values, rarely changes the values established during SFT.

The study also discovered that different algorithms used for preference optimization can lead to varying outcomes in terms of value alignment, even when the same data is used. These findings provide valuable insights into how AI models learn human values and can inform the development of more value-aligned AI systems.

**Key Takeaways:**

* AI models learn human values primarily during the supervised fine-tuning phase.
* Further training using preference optimization rarely changes the values established during supervised fine-tuning.
* Different algorithms used for preference optimization can lead to varying outcomes in terms of value alignment.

This research has important implications for the development of AI systems that align with human values and ethics. By understanding how AI models learn and adopt human values, we can create more responsible and trustworthy AI systems."
cs.CL,The End of Manual Decoding: Towards Truly End-to-End Language Models,"The ""end-to-end"" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly ""end-to-end"" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.   Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from ""hacking the test set""-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., ""generate with low randomness"") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.",http://arxiv.org/abs/2510.26697v2,2025-10-30T17:01:43Z,"Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang","**Breakthrough in AI Language Models: Towards Truly End-to-End Generation**

Large Language Models (LLMs) have become increasingly popular, but they still rely on manual fine-tuning to produce coherent text. This manual process, known as decoding, requires experts to adjust settings like temperature and top-p to get the desired output. A new research paper introduces AutoDeco, a novel architecture that enables LLMs to generate text in a truly ""end-to-end"" manner, without manual intervention.

AutoDeco works by adding lightweight components to the standard transformer model, which dynamically predict the optimal decoding settings for each token. This approach allows the model to self-regulate its sampling strategy, eliminating the need for manual tuning. The results are impressive: AutoDeco outperforms default decoding strategies and achieves performance comparable to an ""oracle-tuned"" baseline, which is a practical upper bound for any static method.

The most exciting aspect of AutoDeco is its ability to interpret natural language commands, such as ""generate with low randomness."" The model can adjust its decoding settings on a token-by-token basis, opening up new possibilities for steerable and interactive LLM decoding. This breakthrough has the potential to revolutionize the way we interact with AI language models, making them more efficient, flexible, and user-friendly."
cs.CL,"Kimi Linear: An Expressive, Efficient Attention Architecture","We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.",http://arxiv.org/abs/2510.26692v1,2025-10-30T16:59:43Z,"Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du","**Breakthrough in AI Architecture: Kimi Linear Revolutionizes Attention Mechanism**

Researchers have introduced Kimi Linear, a novel attention architecture that outperforms traditional ""full attention"" methods in various scenarios, including short and long context lengths, and reinforcement learning. This innovation is significant because it enables more efficient and effective processing of complex data.

**What makes Kimi Linear special?**

* It uses a unique attention module called Kimi Delta Attention (KDA), which allows for more efficient use of memory and computation.
* Kimi Linear achieves high hardware efficiency through a specialized algorithm that reduces computation while maintaining accuracy.
* The researchers pre-trained a massive Kimi Linear model with 3 billion activated parameters and demonstrated its superior performance over traditional attention architectures.

**Key benefits:**

* Kimi Linear outperforms traditional attention architectures by a significant margin across all evaluated tasks.
* It reduces memory usage by up to 75% and achieves up to 6 times faster decoding speed for long input sequences (1 million context).
* The Kimi Linear architecture can be easily integrated into existing AI models as a drop-in replacement, making it a promising solution for a wide range of applications.

**What's next?**

The researchers have open-sourced the KDA kernel and implementation, making it accessible to the broader research community. They have also released pre-trained model checkpoints, which can be used to build upon this innovation. This breakthrough has the potential to accelerate progress in natural language processing, computer vision, and other areas of AI research."
cs.CL,Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models,"Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs.",http://arxiv.org/abs/2510.26683v1,2025-10-30T16:53:45Z,"Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang","**Improving Large Language Models with Ontology Rules**

Large language models (LLMs) have shown impressive abilities in various fields, but they often struggle in specialized areas like healthcare due to a lack of high-quality training data. To address this issue, researchers have proposed a new framework called Evontree. Evontree uses a set of rules, known as ontology rules, to help LLMs learn and refine domain-specific knowledge without needing extensive external data.

The Evontree framework works by:

1. Extracting domain knowledge from the LLM
2. Identifying inconsistencies using ontology rules
3. Refining the knowledge through self-distilled fine-tuning

In tests on medical question-answering benchmarks, Evontree improved the performance of LLMs, achieving up to a 3.7% increase in accuracy. This approach shows promise for adapting LLMs to specialized domains with limited training data, making it a valuable tool for applications like healthcare.

**Key Takeaways:**

* Evontree is a novel framework that leverages ontology rules to improve LLMs in specialized domains
* It extracts, validates, and enhances domain knowledge within LLMs without requiring extensive external data
* Evontree demonstrates improved performance on medical QA benchmarks, achieving up to a 3.7% increase in accuracy."
cs.CL,The Era of Agentic Organization: Learning to Organize with Language   Models,"We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.",http://arxiv.org/abs/2510.26658v1,2025-10-30T16:25:10Z,"Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei","**The Future of AI: Collaborative Problem-Solving with Language Models**

Imagine a future where artificial intelligence (AI) systems can work together to solve complex problems, much like a team of humans. Researchers are now envisioning a new era of AI, called ""agentic organization,"" where AI agents collaborate and work together concurrently to achieve outcomes that surpass individual intelligence.

To make this vision a reality, the researchers have developed a new approach called ""asynchronous thinking"" (AsyncThink). This approach allows large language models to organize their internal thinking process into concurrently executable structures, similar to a team working on a project. An ""organizer"" assigns tasks to individual ""workers,"" which then merge their findings to produce a coherent solution.

The researchers tested AsyncThink and found that it not only improved the accuracy of mathematical reasoning but also reduced the time it took to solve problems by 28% compared to traditional parallel thinking approaches. Moreover, AsyncThink was able to generalize its learning to tackle new, unseen tasks without requiring additional training.

This breakthrough has the potential to revolutionize the way AI systems solve complex problems, enabling them to work together more efficiently and effectively. The possibilities for applications are vast, from improving decision-making in healthcare and finance to enhancing problem-solving in education and science."
cs.CL,Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large   Language Model,"Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially \textit{from the scaling perspective}, raising concerns that the potential of encoder-decoder models may have been overlooked. To fill this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison between RedLLM, pretrained with prefix language modeling (LM), and DecLLM, pretrained with causal LM, at different model scales, ranging from $\sim$150M to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for instruction tuning, our experiments show that RedLLM produces compelling scaling properties and surprisingly strong performance. While DecLLM is overall more compute-optimal during pretraining, RedLLM demonstrates comparable scaling and context length extrapolation capabilities. After instruction tuning, RedLLM achieves comparable and even better results on various downstream tasks while enjoying substantially better inference efficiency. We hope our findings could inspire more efforts on re-examining RedLLM, unlocking its potential for developing powerful and efficient LLMs.",http://arxiv.org/abs/2510.26622v1,2025-10-30T15:48:28Z,"Biao Zhang, Yong Cheng, Siamak Shakeri, Xinyi Wang, Min Ma, Orhan Firat","**Revisiting Encoder-Decoder Large Language Models: A Surprising Comeback**

Large language models (LLMs) are a type of artificial intelligence designed to process and generate human-like language. Recently, most LLMs have adopted a ""decoder-only"" architecture, abandoning the traditional ""encoder-decoder"" approach. However, a new study questions whether this shift was premature.

The researchers revived the encoder-decoder model, called RedLLM, and upgraded it with modern techniques used in decoder-only models (DecLLM). They compared the performance of both models at various scales, from 150 million to 8 billion parameters, using a massive dataset of 1.6 trillion tokens.

The surprising findings:

* Encoder-decoder models (RedLLM) can scale just as well as decoder-only models (DecLLM) and achieve comparable performance on many tasks.
* RedLLM models are more efficient during inference, meaning they can generate responses faster and with less computational power.
* After fine-tuning, RedLLM models performed equally well or even better than DecLLM models on various tasks.

These results suggest that encoder-decoder models may have been overlooked, and their potential for developing powerful and efficient LLMs warrants further exploration. The study's findings could inspire a re-examination of encoder-decoder models and lead to the creation of more efficient and effective language models."
cs.CL,SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual   Document Understanding,"Multi-page visual documents such as manuals, brochures, presentations, and posters convey key information through layout, colors, icons, and cross-slide references. While large language models (LLMs) offer opportunities in document understanding, current systems struggle with complex, multi-page visual documents, particularly in fine-grained reasoning over elements and pages. We introduce SlideAgent, a versatile agentic framework for understanding multi-modal, multi-page, and multi-layout documents, especially slide decks. SlideAgent employs specialized agents and decomposes reasoning into three specialized levels-global, page, and element-to construct a structured, query-agnostic representation that captures both overarching themes and detailed visual or textual cues. During inference, SlideAgent selectively activates specialized agents for multi-level reasoning and integrates their outputs into coherent, context-aware answers. Extensive experiments show that SlideAgent achieves significant improvement over both proprietary (+7.9 overall) and open-source models (+9.8 overall).",http://arxiv.org/abs/2510.26615v1,2025-10-30T15:41:15Z,"Yiqiao Jin, Rachneet Kaur, Zhen Zeng, Sumitra Ganesh, Srijan Kumar","Here's a summary of the research paper for a general audience:

**Understanding Complex Documents Just Got Easier**

Imagine trying to make sense of a long, visually complex document like a manual, brochure, or presentation. Current computer systems struggle to understand the relationships between different parts of the document, especially when it spans multiple pages.

Researchers have developed a new framework called SlideAgent, which uses a team of specialized ""agents"" to work together to understand complex documents. These agents focus on different levels of the document, from the overall theme to individual elements like text and images.

SlideAgent has been tested on a variety of documents, including slide decks, and has shown significant improvements in understanding over existing systems - both commercial and open-source ones. This breakthrough has the potential to enable computers to better comprehend and summarize complex documents, making it easier for people to extract the information they need."
cs.CL,Normative Reasoning in Large Language Models: A Comparative Benchmark   from Logical and Modal Perspectives,"Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.",http://arxiv.org/abs/2510.26606v2,2025-10-30T15:35:13Z,"Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada","**Large Language Models Put to the Test: Can They Reason with Rules and Obligations?**

Large language models (LLMs) have become incredibly good at understanding and generating human-like text, but researchers are still exploring their limitations. One important area of reasoning is ""normative reasoning,"" which involves understanding rules, obligations, and permissions. For example, if someone says ""it's obligatory to wear a seatbelt while driving,"" a model should be able to infer that it's not allowed to drive without a seatbelt.

A new study evaluated the ability of LLMs to reason with normative statements, comparing their performance to reasoning with statements about knowledge and belief (epistemic modals). The researchers created a dataset that covers various formal patterns of reasoning in both areas, as well as factors that influence human thinking.

The results showed that while LLMs generally follow valid reasoning patterns, they struggle with specific types of normative reasoning and exhibit biases similar to those found in human reasoning studies. These findings highlight the challenges of achieving logical consistency in LLMs' normative reasoning and provide insights for improving their reliability.

**In simple terms:** LLMs are good at understanding text, but they still struggle with reasoning about rules and obligations. This study helps identify areas where LLMs need improvement, which is essential for developing more reliable and trustworthy language models. The researchers have made their data and code publicly available, which can help others build on their work."
cs.CL,Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference   in Large Language Models,"Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.   Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%.",http://arxiv.org/abs/2510.26577v1,2025-10-30T15:04:36Z,"Yinrong Hong, Zhiquan Tan, Kai Hu","**Breakthrough in Large Language Model Efficiency**

Researchers have made a significant advancement in speeding up Large Language Models (LLMs), which are AI systems that process and generate human-like language. The challenge with LLMs is that they can be slow due to their complex design and large size. To address this, the researchers developed a new method called CAST, which dynamically adjusts how the model generates text to make it more efficient.

**What makes CAST unique?**

Unlike previous methods, CAST takes into account important factors such as the type of computer hardware (like GPU devices) and how many tasks are being processed at once (batch sizes). By considering these factors, CAST builds a customized ""tree"" structure that allows the model to generate and validate multiple text tokens simultaneously, reducing the time it takes to produce output.

**Impressive results**

The researchers tested CAST on various tasks and LLMs, and the results are impressive: CAST can process text up to 5.2 times faster than traditional methods. Additionally, it outperforms existing state-of-the-art techniques by 5-20%. This breakthrough has the potential to significantly improve the efficiency and responsiveness of LLMs, making them more practical for real-world applications."
cs.CL,InfoFlow: Reinforcing Search Agent Via Reward Density Optimization,"Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.",http://arxiv.org/abs/2510.26575v1,2025-10-30T15:03:21Z,"Kun Luo, Hongjin Qian, Zheng Liu, Ziyi Xia, Shitao Xiao, Siqi Bao, Jun Zhao, Kang Liu","Here's a summary of the research paper ""InfoFlow: Reinforcing Search Agent Via Reward Density Optimization"" for a general audience:

**Improving Search Agents with InfoFlow**

Imagine you're searching for a specific piece of information online. You might use a search engine to find relevant results, but the process can be time-consuming and frustrating. Researchers have been working on developing ""search agents"" that can help automate this process, but these agents often struggle to find what they're looking for efficiently.

The main challenge is that search agents typically only receive rewards or feedback when they finally find what they're looking for, which can be rare and far apart. This makes it hard for them to learn and improve over time.

To address this issue, researchers have developed a new framework called InfoFlow. InfoFlow aims to provide search agents with more frequent and relevant feedback, allowing them to learn and improve more efficiently.

InfoFlow works in three key ways:

1. **Breaking down tasks**: InfoFlow breaks down long search tasks into smaller, more manageable sub-tasks, providing the agent with more frequent rewards and feedback.
2. **Corrective guidance**: When the agent gets stuck, InfoFlow provides corrective guidance to help it get back on track.
3. **Dual-agent architecture**: InfoFlow uses two agents working together: one agent explores and searches, while the other agent helps refine and summarize the search history, making it easier to find what the agent is looking for.

The results are impressive: InfoFlow has been tested on several search benchmarks and has significantly outperformed existing methods. In fact, it has enabled lightweight language models to perform as well as more advanced, proprietary models.

Overall, InfoFlow has the potential to improve the efficiency and effectiveness of search agents, making it easier for us to find what we're looking for online."
cs.CL,The Structure of Relation Decoding Linear Operators in Large Language   Models,"This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.",http://arxiv.org/abs/2510.26543v1,2025-10-30T14:36:09Z,"Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga","**Unlocking the Secrets of Large Language Models: How They Understand Relationships**

Imagine you're asking a virtual assistant to tell you the capital of France. The assistant uses complex algorithms to retrieve this information, but have you ever wondered how it actually works? A recent study delved into the inner workings of large language models, like those used in chatbots and virtual assistants, to understand how they decode specific relationships between words.

The researchers found that these models use simple mathematical operators to extract relationships, such as ""capital of"" or ""food from,"" from vast amounts of text data. Surprisingly, they discovered that these operators are not unique to each relationship, but rather focus on broader semantic properties, like ""country of."" This means that the model is not specifically looking for the ""capital of"" relationship, but rather for information related to a country's properties.

The study also showed that these operators can be highly compressed, meaning they can be simplified without losing much accuracy. This is because they rely on these coarse-grained properties, which are shared across multiple relationships. Think of it like a filing system, where information is organized into broad categories rather than specific folders.

The findings have significant implications for how we understand large language models. They suggest that these models are not as nuanced as we thought, and that their ability to generalize to new relationships is limited to those that are semantically close to the ones they were trained on. In other words, if a model is trained on information about countries and capitals, it may struggle to understand relationships that are very different, like those between people and their favorite foods.

Overall, this study provides a deeper understanding of how large language models work, and highlights the importance of considering their limitations when designing and training these models."
cs.CL,Hebrew Diacritics Restoration using Visual Representation,"Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task.   In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation.   Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization.",http://arxiv.org/abs/2510.26521v1,2025-10-30T14:15:16Z,"Yair Elboher, Yuval Pinter","Here's a summary of the research paper for a general audience:

**Restoring Accents to Hebrew Text**

Hebrew text often appears without vowel markings, known as diacritics, which can make it difficult to read and understand. Researchers have developed a new system called DIVRIT to automatically add these diacritics back to the text. Unlike previous methods, DIVRIT uses a unique approach by treating the text as an image, rather than just a sequence of characters. This allows the system to ""see"" the text in a more holistic way and make more accurate predictions about which diacritics are needed.

The researchers tested DIVRIT on a variety of Hebrew texts and found that it performed well, even in challenging situations where the correct diacritics were not immediately obvious. By using visual representations of the text, DIVRIT was able to accurately restore diacritics without relying on complex linguistic analysis. This breakthrough has the potential to improve the readability and understanding of Hebrew text, and could be useful for applications such as language learning, text-to-speech synthesis, and more."
cs.CL,Inside CORE-KG: Evaluating Structured Prompting and Coreference   Resolution for Knowledge Graphs,"Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.",http://arxiv.org/abs/2510.26512v1,2025-10-30T14:05:55Z,"Dipak Meher, Carlotta Domeniconi","**Breaking Down Complex Legal Texts: A New Approach to Understanding Human Smuggling Networks**

Human smuggling networks are complex and constantly evolving, making it difficult for authorities to track and analyze them. One way to gain insight into these networks is by analyzing legal case documents, but these documents are often unstructured and hard to understand. Researchers have been working on ways to automatically extract information from these documents and create knowledge graphs, which are visual representations of the relationships between different entities.

A new framework called CORE-KG has been developed to improve the accuracy of these knowledge graphs. CORE-KG uses two key components: structured prompts, which guide the extraction of information, and coreference resolution, which helps to identify and link duplicate references to the same entity.

In a recent study, researchers conducted an experiment to see how important each of these components is to the overall performance of CORE-KG. They found that removing coreference resolution led to a significant increase in duplicate nodes (28.32%) and noisy nodes (4.32%), while removing structured prompts resulted in a large increase in noisy nodes (73.33%).

These findings provide valuable insights into how to design more effective systems for extracting structured information from complex legal texts. By using CORE-KG and similar approaches, researchers hope to improve our understanding of human smuggling networks and help authorities to better track and disrupt them."
stat.ML,A Unified Theory for Causal Inference: Direct Debiased Machine Learning   via Bregman-Riesz Regression,"This note introduces a unified theory for causal inference that integrates Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted maximum likelihood estimation (TMLE), and the matching estimator in average treatment effect (ATE) estimation. In ATE estimation, the balancing weights and the regression functions of the outcome play important roles, where the balancing weights are referred to as the Riesz representer, bias-correction term, and clever covariates, depending on the context. Riesz regression, covariate balancing, DRE, and the matching estimator are methods for estimating the balancing weights, where Riesz regression is essentially equivalent to DRE in the ATE context, the matching estimator is a special case of DRE, and DRE is in a dual relationship with covariate balancing. TMLE is a method for constructing regression function estimators such that the leading bias term becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density Ratio Estimation and Riesz Regression.",http://arxiv.org/abs/2510.26783v1,2025-10-30T17:56:47Z,Masahiro Kato,"**Unlocking the Secrets of Cause-and-Effect: A New Unified Theory**

Imagine you're trying to figure out whether a new exercise routine really leads to weight loss, or if a certain medicine actually helps people recover from an illness. This is known as causal inference, and it's a crucial question in many fields, from medicine to social sciences.

Researchers have developed various methods to estimate the effect of a treatment or intervention, but these methods have often been separate and distinct. A new study introduces a unified theory that brings together several of these methods under one umbrella. This unified approach, called Direct Debiased Machine Learning via Bregman-Riesz Regression, provides a comprehensive framework for understanding the relationships between different methods.

**The Key Players: Balancing Weights and Regression Functions**

In causal inference, there are two main components: balancing weights and regression functions. Balancing weights help ensure that the groups being compared are similar in terms of relevant characteristics. Regression functions, on the other hand, help predict the outcome of interest.

The new study shows that several popular methods, including Riesz regression, covariate balancing, and density-ratio estimation, are all connected and can be used to estimate balancing weights. Additionally, the study reveals that targeted maximum likelihood estimation and matching estimators are also related to these methods.

**What Does This Mean?**

This unified theory has important implications for researchers and practitioners. By understanding the connections between different methods, scientists can:

* Choose the best approach for their specific problem
* Develop new, more accurate methods for estimating cause-and-effect relationships
* Apply these methods to a wide range of fields, from medicine to social sciences

In short, this study provides a major breakthrough in the field of causal inference, enabling researchers to better understand the complex relationships between variables and make more accurate predictions about the effects of interventions."
stat.ML,Deep sequence models tend to memorize geometrically; it is unclear why,"In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task.   From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.   Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.",http://arxiv.org/abs/2510.26745v1,2025-10-30T17:40:22Z,"Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar","**Unlocking the Secrets of How AI Models Store Information**

Researchers have made a fascinating discovery about how artificial intelligence (AI) models, specifically those using ""deep sequence models"" like Transformers, store and process information. For a long time, it's been thought that these models work by simply memorizing pairs of related entities, like a lookup table. However, this study suggests that these models may actually be doing something much more complex.

The researchers found that these models can create a kind of ""geometric map"" of relationships between entities, even if they haven't been explicitly trained on those relationships. This map allows the model to make connections between entities that it has never seen together before. This is surprising because it's not clear why the model would create such a map, especially since it's not a more efficient way of storing information.

The study also explored why this happens, and found that it's due to a phenomenon called ""spectral bias."" This is a natural tendency of the model to create a geometric representation of the data, even when it's not explicitly trained to do so.

The implications of this research are significant, as it challenges our current understanding of how AI models store and process information. It also suggests that there may be ways to improve the performance of these models by making their memory more ""geometric."" Overall, this study opens up new avenues for research into the inner workings of AI models, and could lead to breakthroughs in areas like knowledge acquisition and discovery."
stat.ML,Bridging the Gap between Empirical Welfare Maximization and Conditional   Average Treatment Effect Estimation in Policy Learning,"The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a novel regularization method for policy learning. Our findings yield a convex and computationally efficient training procedure that avoids the NP-hard combinatorial step typically required in EWM.",http://arxiv.org/abs/2510.26723v1,2025-10-30T17:23:40Z,Masahiro Kato,"**New Research Bridges the Gap in Policy Learning**

Imagine you're trying to decide which treatment will work best for a patient based on their individual characteristics. Policy learning is a field of research that aims to develop algorithms that can make these decisions to maximize overall well-being. There are two main approaches to policy learning: one focuses on maximizing overall welfare (Empirical Welfare Maximization, or EWM) and the other estimates the average effect of a treatment on an individual (Conditional Average Treatment Effect, or CATE).

A recent study has found that these two approaches are more similar than previously thought. In fact, the researchers proved that EWM and CATE estimation are essentially solving the same mathematical problem, just in different ways. This breakthrough finding means that the two approaches are interchangeable and have the same theoretical guarantees.

The study also proposes a new method for policy learning that combines the benefits of both approaches. This new method allows for a more efficient and computationally friendly way to train algorithms, avoiding complex calculations that were previously required.

The implications of this research are significant, as it could lead to more effective and personalized treatment recommendations in a variety of fields, such as healthcare and social policy. By bridging the gap between EWM and CATE estimation, researchers can develop more robust and efficient algorithms that ultimately lead to better outcomes for individuals and society."
stat.ML,Budgeted Multiple-Expert Deferral,"Learning to defer uncertain predictions to costly experts offers a powerful strategy for improving the accuracy and efficiency of machine learning systems. However, standard training procedures for deferral algorithms typically require querying all experts for every training instance, an approach that becomes prohibitively expensive when expert queries incur significant computational or resource costs. This undermines the core goal of deferral: to limit unnecessary expert usage. To overcome this challenge, we introduce the budgeted deferral framework, which aims to train effective deferral algorithms while minimizing expert query costs during training. We propose new algorithms for both two-stage and single-stage multiple-expert deferral settings that selectively query only a subset of experts per training example. While inspired by active learning, our setting is fundamentally different: labels are already known, and the core challenge is to decide which experts to query in order to balance cost and predictive performance. We establish theoretical guarantees for both of our algorithms, including generalization bounds and label complexity analyses. Empirical results across several domains show that our algorithms substantially reduce training costs without sacrificing prediction accuracy, demonstrating the practical value of our budget-aware deferral algorithms.",http://arxiv.org/abs/2510.26706v1,2025-10-30T17:08:52Z,"Giulia DeSalvo, Clara Mohri, Mehryar Mohri, Yutao Zhong","**Improving Machine Learning Efficiency: A New Approach to Expert Deferral**

Machine learning systems can make predictions with the help of human experts, but querying these experts can be costly and time-consuming. A new approach called ""budgeted deferral"" aims to reduce these costs while maintaining prediction accuracy. Researchers have developed algorithms that selectively query experts only when necessary, rather than querying all experts for every prediction.

The traditional approach to training machine learning systems requires querying all experts for every example, which can be expensive. The new budgeted deferral framework trains algorithms to defer uncertain predictions to experts while minimizing the number of expert queries during training. This approach is different from active learning, where the goal is to gather more information, whereas in budgeted deferral, the labels are already known, and the goal is to balance cost and predictive performance.

The researchers propose two algorithms for different settings and provide theoretical guarantees for their performance. In tests across several domains, these algorithms significantly reduced training costs without sacrificing prediction accuracy. This breakthrough has the potential to make machine learning systems more efficient and cost-effective in a wide range of applications."
stat.ML,Assessment of the conditional exchangeability assumption in causal   machine learning models: a simulation study,"Observational studies developing causal machine learning (ML) models for the prediction of individualized treatment effects (ITEs) seldom conduct empirical evaluations to assess the conditional exchangeability assumption. We aimed to evaluate the performance of these models under conditional exchangeability violations and the utility of negative control outcomes (NCOs) as a diagnostic. We conducted a simulation study to examine confounding bias in ITE estimates generated by causal forest and X-learner models under varying conditions, including the presence or absence of true heterogeneity. We simulated data to reflect real-world scenarios with differing levels of confounding, sample size, and NCO confounding structures. We then estimated and compared subgroup-level treatment effects on the primary outcome and NCOs across settings with and without unmeasured confounding. When conditional exchangeability was violated, causal forest and X-learner models failed to recover true treatment effect heterogeneity and, in some cases, falsely indicated heterogeneity when there was none. NCOs successfully identified subgroups affected by unmeasured confounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it remained informative, flagging potential bias in subgroup level estimates, though not always pinpointing the subgroup with the largest confounding. Violations of conditional exchangeability substantially limit the validity of ITE estimates from causal ML models in routinely collected observational data. NCOs serve a useful empirical diagnostic tool for detecting subgroup-specific unmeasured confounding and should be incorporated into causal ML workflows to support the credibility of individualized inference.",http://arxiv.org/abs/2510.26700v1,2025-10-30T17:05:57Z,"Gerard T. Portela, Jason B. Gibbons, Sebastian Schneeweiss, Rishi J. Desai","**Can Machine Learning Models Accurately Predict Personalized Treatment Effects?**

Machine learning models are increasingly being used to predict how individuals will respond to different treatments. However, these models rely on a crucial assumption: that the data used to train them is unbiased. A recent study tested what happens when this assumption is violated, and found that the models can produce inaccurate results.

The researchers simulated real-world scenarios with varying levels of bias and sample sizes, and used two popular machine learning models (causal forest and X-learner) to estimate treatment effects. They found that when the assumption of unbiased data was not met, the models failed to accurately identify which subgroups of people would benefit from treatment. In some cases, the models even suggested that certain subgroups would benefit from treatment when they wouldn't.

The good news is that the researchers also found a way to detect when the models might be producing biased results. They used something called ""negative control outcomes"" (NCOs), which are outcomes that are not affected by the treatment. By analyzing NCOs, the researchers were able to identify subgroups that were likely to be affected by bias.

The study's findings have important implications for the use of machine learning models in healthcare. They suggest that these models should be used with caution, and that NCOs should be used to validate their results. By doing so, researchers and clinicians can increase confidence in the accuracy of personalized treatment recommendations.

**In simple terms:** Machine learning models can be useful for predicting how individuals will respond to different treatments, but they require high-quality, unbiased data. A new study found that when this data is not available, the models can produce inaccurate results. However, a simple diagnostic tool called negative control outcomes can help detect when the models might be off track."
stat.ML,Action-Driven Processes for Continuous-Time Control,"At the heart of reinforcement learning are actions -- decisions made in response to observations of the environment. Actions are equally fundamental in the modeling of stochastic processes, as they trigger discontinuous state transitions and enable the flow of information through large, complex systems. In this paper, we unify the perspectives of stochastic processes and reinforcement learning through action-driven processes, and illustrate their application to spiking neural networks. Leveraging ideas from control-as-inference, we show that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for a suitably defined action-driven process is equivalent to maximum entropy reinforcement learning.",http://arxiv.org/abs/2510.26672v1,2025-10-30T16:42:09Z,"Ruimin He, Shaowei Lin","**Unlocking Efficient Control Systems: A Breakthrough in Reinforcement Learning**

Imagine you're playing a video game where you need to make quick decisions to navigate through a complex environment. Your brain is constantly processing information and making choices to achieve your goals. Researchers have made a significant breakthrough in understanding how to optimize such decision-making processes, which could lead to more efficient control systems in various fields.

The study combines two key areas: reinforcement learning (a type of artificial intelligence that learns from trial and error) and stochastic processes (which model random events and systems). By merging these perspectives, the researchers introduced the concept of ""action-driven processes."" These processes describe how actions trigger changes in a system and facilitate the flow of information.

The researchers applied their new approach to spiking neural networks, a type of artificial neural network inspired by the human brain. They found that optimizing the decision-making process in these networks is equivalent to a well-known reinforcement learning method called maximum entropy reinforcement learning.

In simple terms, the study provides a new framework for understanding and optimizing control systems, which could lead to advancements in areas like robotics, autonomous vehicles, and smart homes. By leveraging this new approach, researchers and engineers can develop more efficient and effective control systems that can adapt to complex environments."
stat.ML,On Measuring Localization of Shortcuts in Deep Networks,"Shortcuts, spurious rules that perform well during training but fail to generalize, present a major challenge to the reliability of deep networks (Geirhos et al., 2020). However, the impact of shortcuts on feature representations remains understudied, obstructing the design of principled shortcut-mitigation methods. To overcome this limitation, we investigate the layer-wise localization of shortcuts in deep models. Our novel experiment design quantifies the layer-wise contribution to accuracy degradation caused by a shortcut-inducing skew by counterfactual training on clean and skewed datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find that shortcut learning is not localized in specific layers but distributed throughout the network. Different network parts play different roles in this process: shallow layers predominantly encode spurious features, while deeper layers predominantly forget core features that are predictive on clean data. We also analyze the differences in localization and describe its principal axes of variation. Finally, our analysis of layer-wise shortcut-mitigation strategies suggests the hardness of designing general methods, supporting dataset- and architecture-specific approaches instead.",http://arxiv.org/abs/2510.26560v1,2025-10-30T14:51:03Z,"Nikita Tsoy, Nikola Konstantinov","**Understanding Shortcuts in Deep Learning: A New Study**

Deep learning models can sometimes learn shortcuts, or simple rules that work well during training but fail when faced with new, real-world data. This can lead to unreliable performance. Researchers have been trying to understand how shortcuts affect the way these models learn and represent features.

In a recent study, researchers investigated how shortcuts are distributed throughout deep learning models. They developed a new method to measure the impact of shortcuts on different layers of the model. They tested this method on several popular datasets and models, including images of animals, people, and objects.

The study found that shortcuts are not confined to specific layers of the model, but are instead spread throughout. The researchers also discovered that different parts of the model play different roles in learning shortcuts. Shallow layers (early in the model) tend to focus on simple, spurious features, while deeper layers (later in the model) tend to forget the important features that are learned from clean data.

The study's findings suggest that it's challenging to design a general method to mitigate shortcuts, and that a more tailored approach may be needed for each dataset and model architecture. This research provides new insights into the nature of shortcuts in deep learning and could help improve the reliability of these models."
stat.ML,Approximating Heavy-Tailed Distributions with a Mixture of Bernstein   Phase-Type and Hyperexponential Models,"Heavy-tailed distributions, prevalent in a lot of real-world applications such as finance, telecommunications, queuing theory, and natural language processing, are challenging to model accurately owing to their slow tail decay. Bernstein phase-type (BPH) distributions, through their analytical tractability and good approximations in the non-tail region, can present a good solution, but they suffer from an inability to reproduce these heavy-tailed behaviors exactly, thus leading to inadequate performance in important tail areas. On the contrary, while highly adaptable to heavy-tailed distributions, hyperexponential (HE) models struggle in the body part of the distribution. Additionally, they are highly sensitive to initial parameter selection, significantly affecting their precision.   To solve these issues, we propose a novel hybrid model of BPH and HE distributions, borrowing the most desirable features from each for enhanced approximation quality. Specifically, we leverage an optimization to set initial parameters for the HE component, significantly enhancing its robustness and reducing the possibility that the associated procedure results in an invalid HE model. Experimental validation demonstrates that the novel hybrid approach is more performant than individual application of BPH or HE models. More precisely, it can capture both the body and the tail of heavy-tailed distributions, with a considerable enhancement in matching parameters such as mean and coefficient of variation. Additional validation through experiments utilizing queuing theory proves the practical usefulness, accuracy, and precision of our hybrid approach.",http://arxiv.org/abs/2510.26524v1,2025-10-30T14:16:43Z,"Abdelhakim Ziani, András Horváth, Paolo Ballarini","**Improving Modeling of Heavy-Tailed Distributions**

Heavy-tailed distributions are common in many fields, such as finance, telecommunications, and natural language processing. They are characterized by a slow decline in their tail, making them challenging to model accurately. Researchers have proposed various models to approximate these distributions, but each has its limitations.

A new study introduces a hybrid model that combines the strengths of two existing models: Bernstein phase-type (BPH) and hyperexponential (HE) models. The BPH model is good at approximating the main part of the distribution, but struggles with the tail. The HE model, on the other hand, is adaptable to heavy-tailed distributions but has limitations in the main part of the distribution and can be sensitive to initial parameters.

The hybrid model, called BPH-HE, leverages the best features of both models to provide a more accurate approximation of heavy-tailed distributions. The researchers also propose an optimization method to set initial parameters for the HE component, making the model more robust and reliable.

**Key Findings:**

* The hybrid BPH-HE model outperforms individual BPH and HE models in approximating heavy-tailed distributions.
* The model can accurately capture both the main part and the tail of the distribution.
* The hybrid approach shows significant improvements in matching key parameters, such as mean and coefficient of variation.
* The model's practical usefulness and accuracy are validated through experiments in queuing theory.

Overall, the study presents a promising new approach for modeling heavy-tailed distributions, which can have significant implications for various fields, including finance, telecommunications, and natural language processing."
stat.ML,LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection,"Model and hyperparameter selection are critical but challenging in machine learning, typically requiring expert intuition or expensive automated search. We investigate whether large language models (LLMs) can act as in-context meta-learners for this task. By converting each dataset into interpretable metadata, we prompt an LLM to recommend both model families and hyperparameters. We study two prompting strategies: (1) a zero-shot mode relying solely on pretrained knowledge, and (2) a meta-informed mode augmented with examples of models and their performance on past tasks. Across synthetic and real-world benchmarks, we show that LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search, and that improvements from meta-informed prompting demonstrate their capacity for in-context meta-learning. These results highlight a promising new role for LLMs as lightweight, general-purpose assistants for model selection and hyperparameter optimization.",http://arxiv.org/abs/2510.26510v1,2025-10-30T14:04:25Z,"Youssef Attia El Hili, Albert Thomas, Malik Tiomoko, Abdelhakim Benechehab, Corentin Léger, Corinne Ancourt, Balázs Kégl","**Can AI Help Choose the Right Machine Learning Model?**

Researchers have made a breakthrough in machine learning by exploring whether large language models (LLMs) can help choose the best model and settings (hyperparameters) for a specific task. Typically, selecting the right model and settings requires expert knowledge or a time-consuming and costly search process.

In this study, the researchers converted dataset information into a format that LLMs can understand and then asked the LLMs to recommend models and settings. They tested two approaches: one relying solely on the LLM's pre-trained knowledge and another that provided examples of past tasks and their results.

The results showed that LLMs can effectively recommend competitive models and settings without the need for a costly search. The approach that used past task examples (meta-informed mode) performed even better, demonstrating the LLM's ability to learn from context. This research highlights the potential for LLMs to act as helpful assistants in machine learning, making it easier to select the right models and settings for a specific task."
stat.ML,Statistical Inference for Matching Decisions via Matrix Completion under   Dependent Missingness,"This paper studies decision-making and statistical inference for two-sided matching markets via matrix completion. In contrast to the independent sampling assumed in classical matrix completion literature, the observed entries, which arise from past matching data, are constrained by matching capacity. This matching-induced dependence poses new challenges for both estimation and inference in the matrix completion framework. We propose a non-convex algorithm based on Grassmannian gradient descent and establish near-optimal entrywise convergence rates for three canonical mechanisms, i.e., one-to-one matching, one-to-many matching with one-sided random arrival, and two-sided random arrival. To facilitate valid uncertainty quantification and hypothesis testing on matching decisions, we further develop a general debiasing and projection framework for arbitrary linear forms of the reward matrix, deriving asymptotic normality with finite-sample guarantees under matching-induced dependent sampling. Our empirical experiments demonstrate that the proposed approach provides accurate estimation, valid confidence intervals, and efficient evaluation of matching policies.",http://arxiv.org/abs/2510.26478v1,2025-10-30T13:31:32Z,"Congyuan Duan, Wanteng Ma, Dong Xia, Kan Xu","**Unlocking Insights in Matching Markets through Advanced Statistics**

Imagine a world where making informed decisions in matching markets, such as pairing students with schools or workers with jobs, becomes much easier. A recent research paper proposes a groundbreaking approach to tackle this challenge. By using a technique called matrix completion, the authors develop a method to analyze data from past matching experiences and make accurate predictions about future matches.

The twist? The data from past matches isn't independent; it's connected by the limited capacity of each match. This dependence poses significant challenges for statistical analysis. To overcome this, the researchers created a non-convex algorithm and a debiasing framework that provides reliable estimates, confidence intervals, and hypothesis testing for matching decisions.

**Key Breakthroughs:**

1. **Accurate Estimation**: The proposed approach provides near-optimal estimates for three common matching mechanisms.
2. **Valid Uncertainty Quantification**: The method offers a way to quantify uncertainty and test hypotheses about matching decisions, ensuring reliable conclusions.
3. **Efficient Evaluation of Matching Policies**: The approach enables efficient evaluation of different matching policies, facilitating informed decision-making.

**What does this mean?**

This research has the potential to revolutionize decision-making in various matching markets, such as:

* Education: improving student-school matches
* Labor markets: enhancing job-worker matches
* Healthcare: optimizing patient-doctor or patient-treatment matches

By providing a robust statistical framework, this study paves the way for more informed and effective decision-making in these critical areas."
stat.ML,Multi-Output Robust and Conjugate Gaussian Processes,"Multi-output Gaussian process (MOGP) regression allows modelling dependencies among multiple correlated response variables. Similarly to standard Gaussian processes, MOGPs are sensitive to model misspecification and outliers, which can distort predictions within individual outputs. This situation can be further exacerbated by multiple anomalous response variables whose errors propagate due to correlations between outputs. To handle this situation, we extend and generalise the robust and conjugate Gaussian process (RCGP) framework introduced by Altamirano et al. (2024). This results in the multi-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and jointly captures correlations across outputs. We thoroughly evaluate our approach through applications in finance and cancer research.",http://arxiv.org/abs/2510.26401v1,2025-10-30T11:41:19Z,"Joshua Rooijakkers, Leiv Rønneberg, François-Xavier Briol, Jeremias Knoblauch, Matias Altamirano","**Improving Predictions with Robust Multi-Output Gaussian Processes**

Imagine you're trying to predict multiple related things at the same time, like stock prices or the growth of different types of cancer cells. A common approach is to use a statistical model called a Gaussian process, which can capture the relationships between these different variables. However, these models can be sensitive to errors or unusual data points, which can lead to inaccurate predictions.

To address this issue, researchers have developed a new method called the multi-output robust and conjugate Gaussian process (MO-RCGP). This approach extends a previous method called the robust and conjugate Gaussian process (RCGP) to handle multiple outputs. The key benefits of MO-RCGP are:

* **Robustness**: MO-RCGP is less affected by errors or unusual data points, providing more reliable predictions.
* **Conjugacy**: This property makes it easier to update predictions as new data becomes available.
* **Joint modeling**: MO-RCGP captures the correlations between multiple outputs, allowing for more accurate predictions.

The researchers tested MO-RCGP on real-world applications in finance and cancer research, and found that it provides more accurate and reliable predictions compared to existing methods. This new approach has the potential to improve predictions in a wide range of fields where multiple related variables need to be modeled."
stat.ML,Posterior Sampling by Combining Diffusion Models with Annealed Langevin   Dynamics,"Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.   To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.",http://arxiv.org/abs/2510.26324v1,2025-10-30T10:17:27Z,"Zhiyang Xun, Shivam Gupta, Eric Price","**Unlocking Efficient Posterior Sampling with AI**

Imagine trying to restore a blurry image or reconstruct a medical scan. This task is a classic problem in computer science and engineering, known as posterior sampling. The goal is to generate accurate and detailed images by combining a noisy measurement with a good understanding of what the original image should look like.

Researchers have proposed various methods to tackle this challenge, but most are computationally expensive and often rely on approximations. A new approach, described in a recent research paper, combines the strengths of two powerful techniques: diffusion models and Langevin dynamics.

**The Breakthrough**

The researchers focused on a specific type of problem where the prior distribution (the expected structure of the image) is log-concave, meaning it has a smooth and well-behaved shape. They discovered that by combining diffusion models with an annealed variant of Langevin dynamics, they can sample from the posterior distribution (the updated image) much more efficiently.

**The Key Advantage**

The key advantage of this approach is that it can tolerate a higher level of error in the estimation of the score (a measure of the image's features). Specifically, it requires an $L^4$ bound on the score error, which is a significant improvement over previous methods that required a much stricter sub-exponential error bound.

**The Impact**

This breakthrough has significant implications for various applications, including:

* Image restoration (e.g., deblurring, inpainting)
* Medical imaging (e.g., MRI reconstruction)
* Computer vision

By enabling efficient posterior sampling, this new approach can lead to more accurate and detailed images, which can improve diagnosis, treatment, and decision-making in various fields.

**In Simple Terms**

In simple terms, this research proposes a new method for restoring images and reconstructing medical scans by combining two powerful techniques. The approach is more efficient and can tolerate a higher level of error, making it a significant step forward in the field of computer science and engineering."
stat.ML,Implicit Bias of Per-sample Adam on Separable Data: Departure from the   Full-batch Regime,"Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\ell_\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\ell_2$-max-margin classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.",http://arxiv.org/abs/2510.26303v1,2025-10-30T09:41:33Z,"Beomhan Baek, Minhak Song, Chulhee Yun","**The Hidden Bias of a Popular AI Optimizer**

Researchers have been studying the behavior of a widely used algorithm in artificial intelligence (AI) called Adam. Adam is a ""optimizer"" that helps AI models learn from data. While it's known to work well in practice, its theoretical foundations are not well understood.

In this study, the researchers investigated how Adam behaves when it's trained on data one sample at a time, rather than on the entire dataset at once. They found that, surprisingly, Adam's behavior can be different when trained on one sample at a time compared to when trained on the entire dataset.

Specifically, when trained on one sample at a time, Adam tends to favor solutions that are more ""balanced"" or ""fair"" in some sense, whereas when trained on the entire dataset, it favors solutions that are more extreme or ""sparse"". The researchers also showed that this behavior can depend on the specific data being used.

The study's findings have implications for the development of AI models, as they highlight the importance of considering the training procedure and data when using optimizers like Adam. Additionally, the researchers found that another optimizer, called Signum, behaves consistently regardless of the training procedure, which could make it a more reliable choice in certain situations.

Overall, this research provides new insights into the behavior of Adam and other optimizers, which can help improve the development of AI models."
stat.ML,Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning,"Physics-informed machine learning (PIML) integrates prior physical information, often in the form of differential equation constraints, into the process of fitting machine learning models to physical data. Popular PIML approaches, including neural operators, physics-informed neural networks, neural ordinary differential equations, and neural discrete equilibria, are typically fit to objectives that simultaneously include both data and physical constraints. However, the multi-objective nature of this approach creates ambiguity in the measurement of model quality. This is related to a poor understanding of epistemic uncertainty, and it can lead to surprising failure modes, even when existing statistical metrics suggest strong fits. Working within a Gaussian process regression framework, we introduce the Physics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test losses, the PILE score is a single, uncertainty-aware metric that provides a selection principle for hyperparameters of a PIML model. We show that PILE minimization yields excellent choices for a wide variety of model parameters, including kernel bandwidth, least squares regularization weights, and even kernel function selection. We also show that, even prior to data acquisition, a special 'data-free' case of the PILE score identifies a priori kernel choices that are 'well-adapted' to a given PDE. Beyond the kernel setting, we anticipate that the PILE score can be extended to PIML at large, and we outline approaches to do so.",http://arxiv.org/abs/2510.26121v1,2025-10-30T04:05:49Z,"Mara Daniels, Liam Hodgkinson, Michael Mahoney","Here's a summary of the research paper for a general audience:

**Improving Machine Learning Models with Physics**

Machine learning models are great at analyzing data, but sometimes they don't take into account the underlying physical laws that govern the world. To address this, researchers have developed a new approach called Physics-Informed Machine Learning (PIML). PIML combines machine learning with physical laws, such as differential equations, to create more accurate models.

However, a major challenge with PIML is evaluating how well these models are performing. Current methods use multiple objectives, which can lead to confusion and unexpected errors. To tackle this issue, researchers have introduced a new metric called the Physics-Informed Log Evidence (PILE) score.

**What is the PILE score?**

The PILE score is a single, comprehensive metric that takes into account both the data and physical constraints of a PIML model. It provides a way to evaluate the model's performance while also considering the uncertainty associated with the model's predictions. This is important because it allows researchers to make more informed decisions about how to improve the model.

**How does the PILE score work?**

The researchers tested the PILE score on a range of PIML models and found that it consistently identified the best model parameters, such as kernel bandwidth and regularization weights. They also found that the PILE score can be used to select the best kernel function for a given problem, even before any data is collected.

**What's next?**

The PILE score has the potential to be widely adopted in PIML, and the researchers outline approaches to extend it to other PIML models beyond the Gaussian process regression framework. By improving the evaluation and selection of PIML models, the PILE score can help researchers and engineers develop more accurate and reliable models that combine machine learning with physical laws."
stat.ML,Data-driven Projection Generation for Efficiently Solving Heterogeneous   Quadratic Programming Problems,"We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.",http://arxiv.org/abs/2510.26061v1,2025-10-30T01:32:21Z,"Tomoharu Iwata, Futoshi Futami","**Efficiently Solving Complex Optimization Problems with AI**

Researchers have developed a new framework that uses artificial intelligence (AI) to efficiently solve complex optimization problems, known as quadratic programming (QP) problems. These problems involve finding the best solution among a large number of possible solutions, and are used in a wide range of applications, from finance to logistics.

The AI framework, based on a graph neural network, generates customized ""projections"" that reduce the number of variables in these complex problems, making them easier to solve. This approach allows the framework to produce high-quality solutions quickly, even for problems it has never seen before.

The researchers trained the AI model on a diverse set of QP problems and demonstrated that it outperforms existing methods in producing feasible solutions with reduced computation time. This breakthrough has the potential to accelerate the solution of complex optimization problems in various fields, leading to more efficient and effective decision-making.

**Key benefits:**

* Faster solution times
* High-quality solutions
* Ability to handle complex, heterogeneous problems
* Potential applications in finance, logistics, and other fields

This innovative approach has the potential to transform the way we solve complex optimization problems, enabling faster and more efficient decision-making in a wide range of industries."
stat.ML,Bias-Corrected Data Synthesis for Imbalanced Learning,"Imbalanced data, where the positive samples represent only a small proportion compared to the negative samples, makes it challenging for classification problems to balance the false positive and false negative rates. A common approach to addressing the challenge involves generating synthetic data for the minority group and then training classification models with both observed and synthetic data. However, since the synthetic data depends on the observed data and fails to replicate the original data distribution accurately, prediction accuracy is reduced when the synthetic data is naively treated as the true data. In this paper, we address the bias introduced by synthetic data and provide consistent estimators for this bias by borrowing information from the majority group. We propose a bias correction procedure to mitigate the adverse effects of synthetic data, enhancing prediction accuracy while avoiding overfitting. This procedure is extended to broader scenarios with imbalanced data, such as imbalanced multi-task learning and causal inference. Theoretical properties, including bounds on bias estimation errors and improvements in prediction accuracy, are provided. Simulation results and data analysis on handwritten digit datasets demonstrate the effectiveness of our method.",http://arxiv.org/abs/2510.26046v1,2025-10-30T00:52:25Z,"Pengfei Lyu, Zhengchi Ma, Linjun Zhang, Anru R. Zhang","**Improving Machine Learning with More Accurate Synthetic Data**

Machine learning models can struggle when dealing with imbalanced data, where one type of example (e.g., a rare disease) is much less common than another (e.g., a healthy state). To address this issue, researchers often create synthetic examples of the less common type to train their models. However, these synthetic examples can introduce biases and inaccuracies, leading to reduced prediction accuracy.

A new method, called bias-corrected data synthesis, aims to mitigate these biases by using information from the more common type of example to correct the synthetic data. This approach has been shown to improve prediction accuracy and prevent overfitting, where the model becomes too specialized to the training data.

The method has been tested on simulated data and real-world handwritten digit datasets, with promising results. Additionally, the researchers have demonstrated that their approach can be extended to more complex scenarios, such as multi-task learning and causal inference.

Overall, this research has the potential to improve the performance of machine learning models on imbalanced data, which is a common problem in many fields, including healthcare, finance, and social sciences."
stat.ML,$L_1$-norm Regularized Indefinite Kernel Logistic Regression,"Kernel logistic regression (KLR) is a powerful classification method widely applied across diverse domains. In many real-world scenarios, indefinite kernels capture more domain-specific structural information than positive definite kernels. This paper proposes a novel $L_1$-norm regularized indefinite kernel logistic regression (RIKLR) model, which extends the existing IKLR framework by introducing sparsity via an $L_1$-norm penalty. The introduction of this regularization enhances interpretability and generalization while introducing nonsmoothness and nonconvexity into the optimization landscape. To address these challenges, a theoretically grounded and computationally efficient proximal linearized algorithm is developed. Experimental results on multiple benchmark datasets demonstrate the superior performance of the proposed method in terms of both accuracy and sparsity.",http://arxiv.org/abs/2510.26043v1,2025-10-30T00:44:56Z,"Shaoxin Wang, Hanjing Yao","**Improving Machine Learning with a New Method: $L_1$-norm Regularized Indefinite Kernel Logistic Regression**

Machine learning algorithms are widely used in various fields to make predictions and classify data. One such algorithm, Kernel Logistic Regression (KLR), is particularly effective in classification tasks. However, traditional KLR uses ""positive definite kernels"" which might not always capture the complex relationships in data.

Researchers have proposed a new method called $L_1$-norm Regularized Indefinite Kernel Logistic Regression (RIKLR). This method uses ""indefinite kernels"" which can better capture domain-specific information. To make the model more interpretable and efficient, the researchers introduced a technique called $L_1$-norm regularization, which helps to eliminate unnecessary features and improve the model's performance.

The new method was tested on several benchmark datasets and showed superior performance in terms of accuracy and sparsity compared to existing methods. The researchers also developed an efficient algorithm to optimize the model, which overcomes the challenges introduced by the regularization technique.

**In simple terms:** This new method improves machine learning models by using a more flexible and informative way to analyze data, and by eliminating unnecessary features to make the model more efficient and accurate. This can lead to better predictions and classifications in various fields."
stat.ML,Conformal Prediction Beyond the Horizon: Distribution-Free Inference for   Policy Evaluation,"Reliable uncertainty quantification is crucial for reinforcement learning (RL) in high-stakes settings. We propose a unified conformal prediction framework for infinite-horizon policy evaluation that constructs distribution-free prediction intervals {for returns} in both on-policy and off-policy settings. Our method integrates distributional RL with conformal calibration, addressing challenges such as unobserved returns, temporal dependencies, and distributional shifts. We propose a modular pseudo-return construction based on truncated rollouts and a time-aware calibration strategy using experience replay and weighted subsampling. These innovations mitigate model bias and restore approximate exchangeability, enabling uncertainty quantification even under policy shifts. Our theoretical analysis provides coverage guarantees that account for model misspecification and importance weight estimation. Empirical results, including experiments in synthetic and benchmark environments like Mountain Car, show that our method significantly improves coverage and reliability over standard distributional RL baselines.",http://arxiv.org/abs/2510.26026v1,2025-10-29T23:45:44Z,"Feichen Gan, Youcun Lu, Yingying Zhang, Yukun Liu","**Advancing Reliable Uncertainty Quantification in Reinforcement Learning**

Imagine you're teaching a robot to navigate a complex environment, like a self-driving car. You want to be confident that the robot's decisions are safe and reliable, especially in high-stakes situations. A new research paper proposes a method to achieve this by providing a more accurate way to estimate uncertainty in reinforcement learning (RL) algorithms.

The researchers developed a unified framework that combines two key techniques: distributional RL and conformal calibration. This framework allows for the creation of prediction intervals that estimate the range of possible outcomes for a given policy, without relying on specific assumptions about the data distribution.

The innovation lies in addressing challenges such as:

* Dealing with incomplete or unobserved data
* Handling temporal dependencies and changes in the environment
* Accounting for shifts in the data distribution

The proposed method uses a modular approach to construct pseudo-returns and a time-aware calibration strategy to mitigate model bias and ensure reliable uncertainty quantification.

**Key Benefits:**

* Provides distribution-free prediction intervals for returns in both on-policy and off-policy settings
* Offers coverage guarantees that account for model misspecification and importance weight estimation
* Demonstrates improved coverage and reliability over standard distributional RL baselines in synthetic and benchmark environments

**Impact:**

This research has significant implications for high-stakes applications of reinforcement learning, such as robotics, autonomous vehicles, and healthcare. By providing more accurate uncertainty estimates, the proposed method can help ensure safer and more reliable decision-making in complex environments."
stat.ML,Contrastive Predictive Coding Done Right for Mutual Information   Estimation,"The InfoNCE objective, originally introduced for contrastive representation learning, has become a popular choice for mutual information (MI) estimation, despite its indirect connection to MI. In this paper, we demonstrate why InfoNCE should not be regarded as a valid MI estimator, and we introduce a simple modification, which we refer to as InfoNCE-anchor, for accurate MI estimation. Our modification introduces an auxiliary anchor class, enabling consistent density ratio estimation and yielding a plug-in MI estimator with significantly reduced bias. Beyond this, we generalize our framework using proper scoring rules, which recover InfoNCE-anchor as a special case when the log score is employed. This formulation unifies a broad spectrum of contrastive objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single principled framework. Empirically, we find that InfoNCE-anchor with the log score achieves the most accurate MI estimates; however, in self-supervised representation learning experiments, we find that the anchor does not improve the downstream task performance. These findings corroborate that contrastive representation learning benefits not from accurate MI estimation per se, but from the learning of structured density ratios.",http://arxiv.org/abs/2510.25983v1,2025-10-29T21:33:59Z,"J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell","**Unlocking the Secrets of Contrastive Learning: A New Approach to Mutual Information Estimation**

Imagine you're trying to teach a computer to recognize objects in a picture. One way to do this is by using a technique called contrastive learning, which helps the computer learn to identify what makes an object unique. A key concept in contrastive learning is called mutual information (MI), which measures how much two things are related. For example, in a picture, the object and its surroundings might have a lot of mutual information.

Researchers have been using a method called InfoNCE to estimate MI, but it has some limitations. A new study shows that InfoNCE doesn't accurately measure MI and proposes a simple modification, called InfoNCE-anchor. This updated method provides more accurate estimates of MI by introducing an auxiliary ""anchor"" class.

The study also reveals that accurate MI estimation isn't the main goal of contrastive learning. Instead, it's the learning of ""structured density ratios"" that helps computers recognize objects. Think of density ratios like a map that shows how different parts of an image are related. By learning these ratios, the computer can better understand the relationships between objects and their surroundings.

The researchers tested their new method and found that it provides more accurate MI estimates. However, when they used it for self-supervised representation learning (a type of machine learning where the computer learns to recognize patterns on its own), they didn't see any improvement in performance. This suggests that contrastive learning is more about learning useful representations of data than accurately measuring MI.

The study's findings have implications for the development of more effective machine learning algorithms. By understanding how contrastive learning works, researchers can create better models that can learn from data more efficiently. This could lead to breakthroughs in areas like computer vision, natural language processing, and more."
stat.ML,Gradient Flow Sampler-based Distributionally Robust Optimization,"We propose a mathematically principled PDE gradient flow framework for distributionally robust optimization (DRO). Exploiting the recent advances in the intersection of Markov Chain Monte Carlo sampling and gradient flow theory, we show that our theoretical framework can be implemented as practical algorithms for sampling from worst-case distributions and, consequently, DRO. While numerous previous works have proposed various reformulation techniques and iterative algorithms, we contribute a sound gradient flow view of the distributional optimization that can be used to construct new algorithms. As an example of applications, we solve a class of Wasserstein and Sinkhorn DRO problems using the recently-discovered Wasserstein Fisher-Rao and Stein variational gradient flows. Notably, we also show some simple reductions of our framework recover exactly previously proposed popular DRO methods, and provide new insights into their theoretical limit and optimization dynamics. Numerical studies based on stochastic gradient descent provide empirical backing for our theoretical findings.",http://arxiv.org/abs/2510.25956v1,2025-10-29T20:53:44Z,"Zusen Xu, Jia-Jie Zhu","**New Approach to Optimizing Complex Systems**

Imagine you're trying to make a decision that depends on uncertain factors, like the weather or the stock market. You want to make the best choice, but you're not sure what the future holds. That's where ""distributionally robust optimization"" (DRO) comes in - a method that helps make decisions that are robust to uncertainty.

Researchers have now developed a new, mathematically sound framework for DRO, inspired by the concept of gradient flows. Think of gradient flows like a ball rolling down a hill, always following the steepest path. This framework uses a similar idea to sample from the ""worst-case"" scenarios, allowing for more informed decision-making.

The researchers showed that their framework can be used to create practical algorithms for solving DRO problems. They tested their approach on specific problems, like optimizing decisions under uncertainty in situations where the probability of different outcomes is uncertain. Their results provide new insights into popular DRO methods and demonstrate the potential of their framework for making better decisions in complex, uncertain situations.

**What does this mean?**

* This research provides a new, principled approach to DRO, which can be used to make more informed decisions under uncertainty.
* The framework has the potential to be applied to a wide range of fields, from finance to logistics, where uncertainty plays a key role.
* The findings provide new insights into popular DRO methods and demonstrate the potential for improved decision-making in complex situations."
