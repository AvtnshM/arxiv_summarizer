category,title,summary,link,published,authors,summary_short
cs.LG,US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery,"Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete",https://arxiv.org/abs/2511.15600v1,2025-11-19T16:45:04Z,"Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour","**Advancing Spinal Imaging: A Breakthrough in 3D Ultrasound Technology**

Imagine a medical imaging technology that is radiation-free, cost-effective, and provides real-time visuals of the spine. That's what ultrasound technology offers, but it has limitations, particularly in visualizing complete vertebral anatomy. Researchers have now developed a novel approach, called US-X Complete, which combines ultrasound with a single X-ray image to create a more complete and accurate 3D picture of the spine.

The new method uses artificial intelligence to integrate information from both ultrasound and X-ray images, effectively ""filling in the gaps"" in the ultrasound visuals. This approach has shown significant improvements in reconstructing vertebral structures compared to existing methods.

In phantom studies, which are a crucial step towards clinical translation, the researchers achieved a more accurate and complete visualization of the lumbar spine. This innovation has the potential to enhance intraoperative guidance during spinal procedures, making surgeries safer and more effective.

The best part? This technology preserves the benefits of ultrasound, including its radiation-free and cost-effective nature, while overcoming its limitations. The code and data used in this study are also publicly available, paving the way for further research and development."
cs.LG,A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation,"Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",https://arxiv.org/abs/2511.15543v1,2025-11-19T15:37:17Z,"Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis","**Unlocking the Power of Sensors: A New Framework for Optimal Data Collection**

Imagine trying to understand a complex system, like a weather pattern or a chemical reaction, but being limited by the data you can collect. This is a common challenge in many fields of engineering. To overcome this, researchers have developed a new framework that combines machine learning and physics to optimize the placement of sensors and estimate important parameters.

The framework, called Physics-Informed Neural Networks (PINNs), uses artificial intelligence to analyze data from sensors and make predictions about the system being studied. But what makes PINNs powerful is that they can also incorporate the laws of physics into their analysis, allowing them to make more accurate predictions even with limited or noisy data.

The innovation of this framework is that it not only estimates parameters, but also determines the best locations for sensors to collect data. This is achieved by using a mathematical criterion called D-optimality, which ensures that the sensors are placed in a way that maximizes the information collected.

In tests on two complex systems, the framework was able to estimate parameters more accurately than traditional methods that rely on intuition or random sensor placement. This has significant implications for fields such as environmental monitoring, chemical engineering, and materials science, where optimizing sensor placement can lead to better decision-making and more efficient operations.

Overall, this new framework has the potential to revolutionize the way we collect and analyze data from complex systems, enabling more accurate predictions and better insights into the world around us."
cs.LG,Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss,"In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK",https://arxiv.org/abs/2511.15530v1,2025-11-19T15:29:42Z,"Max Hirsch, Federico Pichi","**Unlocking Efficient Training of Artificial Neural Networks**

Researchers have made a breakthrough in training artificial neural networks more efficiently. Neural networks are a type of machine learning model inspired by the human brain, and they're used to solve complex problems in fields like physics and engineering.

The challenge lies in balancing multiple goals, or ""losses,"" when training these networks. Think of it like trying to juggle multiple balls at once – you need to adjust your movements to keep each ball in the air. In neural networks, these goals are combined into a single objective, and the weights assigned to each goal are crucial in determining the network's performance.

A popular approach uses the neural tangent kernel (NTK), which helps describe how the network changes during training. However, this approach has two major drawbacks: it's unclear if it will converge (i.e., produce stable results), and it's computationally expensive.

The researchers have made two key contributions:

1. **Convergence guarantee**: They proved that, under certain conditions, the adaptive weighting algorithm based on NTK will converge, providing stable results.
2. **Efficient computation**: They developed a randomized algorithm that quickly estimates the NTK, reducing the computational burden. This algorithm uses a technique called matrix sketching, which is like taking a snapshot of the network's activity to make calculations faster.

The researchers validated their findings with numerical experiments, demonstrating the effectiveness of their approach. This work has the potential to significantly improve the training of neural networks, making them more efficient and accurate for complex problems in physics and engineering.

**In simple terms:** This research helps artificial neural networks learn more efficiently by balancing multiple goals and reducing computational costs. The new algorithm enables faster and more accurate training, which can lead to breakthroughs in various fields."
cs.LG,Decentralized Gaussian Process Classification and an Application in Subsea Robotics,"Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.",https://arxiv.org/abs/2511.15529v1,2025-11-19T15:26:47Z,"Yifei Gao, Hans J. He, Daniel J. Stilwell, James McMahon","**Summary: Building a Communication Map for Underwater Robots**

Imagine a team of underwater robots working together to explore the ocean. They use sound waves to communicate with each other, but this method has limitations, such as short range and low data transfer rate. To overcome these challenges, researchers have developed a new approach that allows the robots to create a map of where communication is likely to succeed or fail.

This approach uses a type of artificial intelligence called Gaussian process classification, which enables the robots to learn from their experiences and share information with each other. The key innovation is a data-sharing policy that helps the robots decide which information to share, allowing them to build an accurate map of the communication environment.

The researchers tested their approach using real data from underwater robots and found that it works effectively in practice. This breakthrough has the potential to improve the performance of underwater robot teams, enabling them to communicate more reliably and work together more efficiently. This technology could have applications in areas such as ocean exploration, environmental monitoring, and search and rescue operations."
cs.LG,PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles,"Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.",https://arxiv.org/abs/2511.15522v1,2025-11-19T15:19:32Z,"Yinan Yu, Samuel Scheidegger","**Advancing Safe and Reliable Autonomous Vehicles: A Breakthrough in Geofence Enforcement**

Imagine a world where self-driving cars and trucks can safely navigate through designated areas, adhering to specific rules and boundaries. This is made possible by a technology called geofencing, which creates virtual boundaries that vehicles must follow. Researchers have made a significant breakthrough in developing a more effective and efficient geofencing system for ground vehicles.

The new system, called PCARNN-DCBF, combines advanced learning techniques with mathematical models of vehicle behavior. This integration enables the system to accurately predict and control a vehicle's movements while ensuring safety and reliability. Unlike existing solutions, PCARNN-DCBF preserves the underlying physics of vehicle dynamics, allowing for more precise and efficient decision-making.

**Key Benefits:**

* **Improved Safety**: PCARNN-DCBF ensures that vehicles stay within designated areas, reducing the risk of accidents and improving overall safety.
* **Enhanced Reliability**: The system's ability to handle complex vehicle dynamics and actuator limitations makes it more reliable and efficient.
* **Real-time Performance**: PCARNN-DCBF can make decisions in real-time, enabling smooth and seamless navigation through geofenced areas.

**Real-World Impact:**

The researchers tested PCARNN-DCBF in a simulated environment using electric and combustion-powered vehicles. The results showed that their approach significantly outperformed existing methods, demonstrating its potential to revolutionize the development of safe and reliable autonomous vehicles. This breakthrough has far-reaching implications for various industries, including transportation, logistics, and smart cities."
cs.LG,Sample-Adaptivity Tradeoff in On-Demand Sampling,"We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",https://arxiv.org/abs/2511.15507v1,2025-11-19T14:59:47Z,"Nika Haghtalab, Omar Montasser, Mingda Qiao","**Understanding the Tradeoff between Data Collection and Algorithm Efficiency**

Imagine you're trying to learn about different groups of people, but you can only gather information from them a limited number of times. Researchers have been studying how to balance the amount of data you collect (sample complexity) with the number of times you interact with the groups (round complexity). This is known as on-demand sampling.

In a recent study, researchers made several key findings:

* When you have a clear understanding of the groups, you can collect the right amount of data in a relatively small number of interactions (rounds).
* When you're not sure what to expect from the groups, a new algorithm can help you collect nearly the optimal amount of data in a reasonable number of rounds (about the square root of the number of groups).
* The researchers also developed a new framework, called Optimization via On-Demand Sampling (OODS), which helps understand the tradeoff between data collection and algorithm efficiency.

Overall, this study provides new insights into how to balance data collection and algorithm efficiency when learning about multiple groups. The findings have implications for a wide range of applications, from machine learning to data analysis."
cs.LG,A Tensor Compiler for Processing-In-Memory Architectures,"Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.",https://arxiv.org/abs/2511.15503v1,2025-11-19T14:58:16Z,"Peiming Yang, Sankeerth Durvasula, Ivan Fernandez, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko, Christina Giannoula","Here's a summary of the research paper for a general audience:

**Improving Performance of Artificial Intelligence Models**

Researchers have developed a new tool called DCC, a compiler that helps speed up the performance of artificial intelligence (AI) models, such as language models like ChatGPT. These models require a lot of computing power and memory, which can slow them down. To address this, the researchers created a compiler that works with a new type of computer chip called Processing-In-Memory (PIM).

**The Problem: Moving Data Around**

The challenge is that PIM chips and traditional computer processors (like GPUs) store and process data in different ways. This means that data needs to be rearranged before it can be processed, which can slow down the performance of AI models. Current compilers don't optimize this data rearrangement process well, leading to reduced performance.

**The Solution: DCC Compiler**

The DCC compiler solves this problem by optimizing both data rearrangement and computing code simultaneously. This unified approach enables DCC to work efficiently with different PIM chips and AI models. The researchers tested DCC on various AI models and found that it significantly improves performance, with speedups of up to 7.7x compared to traditional GPU-only execution.

**Impact: Faster AI Models**

The DCC compiler has the potential to accelerate the performance of AI models, making them faster and more efficient. This can lead to improved applications in areas like natural language processing, computer vision, and more. The researchers' work demonstrates the importance of co-optimizing data rearrangements and computing code in AI model execution, paving the way for future innovations in AI computing."
cs.LG,NTK-Guided Implicit Neural Teaching,"Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",https://arxiv.org/abs/2511.15487v1,2025-11-19T14:43:04Z,"Chen Zhang, Wei Zuo, Bingyang Cheng, Yikun Wang, Wei-Bin Kou, Yik Chung WU, Ngai Wong","Here's a summary of the research paper ""NTK-Guided Implicit Neural Teaching"" for a general audience:

**Advancing Artificial Intelligence: A Faster Way to Teach Computers**

Imagine trying to teach a computer to recognize and recreate a complex image, like a high-resolution photo. Current methods require the computer to analyze millions of tiny details, which can be slow and computationally expensive. Researchers have developed a new approach called NTK-Guided Implicit Neural Teaching (NINT), which helps computers learn faster and more efficiently.

**The Problem: Slow Learning**

When computers try to learn from large amounts of data, like images or audio, they can get bogged down by the sheer amount of information. This makes it difficult to train them quickly and accurately.

**The Solution: NINT**

NINT uses a clever technique to select the most important details for the computer to focus on, rather than trying to analyze everything at once. This approach is guided by a mathematical tool called the Neural Tangent Kernel (NTK), which helps identify the most critical pieces of information.

**The Result: Faster Learning**

By using NINT, researchers found that computers can learn to recognize and recreate complex images nearly twice as fast as before, without sacrificing accuracy. This breakthrough has the potential to accelerate a wide range of applications, from image and audio processing to 3D reconstruction and more.

**In Simple Terms**

Think of NINT like a GPS navigation system for computers. Instead of trying to map every single detail, NINT helps the computer focus on the most important landmarks, getting it to its destination (accurate learning) faster and more efficiently."
cs.LG,RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection,"This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",https://arxiv.org/abs/2511.15476v1,2025-11-19T14:32:34Z,"Rashid Iqbal, Saddam Hussain Khan","**Breakthrough in Monkeypox Detection: AI Framework Achieves High Accuracy**

Researchers have developed a new artificial intelligence (AI) framework that can accurately detect monkeypox, a viral disease that has gained global attention. The framework, called RS-CA-HSICT, combines the strengths of two powerful AI techniques: Convolutional Neural Networks (CNNs) and Transformers.

The RS-CA-HSICT framework uses a unique approach to analyze images of skin lesions, which are characteristic of monkeypox. It extracts detailed information from the images, including texture, structure, and subtle patterns, to identify the disease.

In tests, the framework achieved an impressive accuracy of 98.30% and an F1-score of 98.13% on two datasets, outperforming existing AI models. This means that the framework can accurately detect monkeypox and distinguish it from other similar diseases.

The researchers believe that their framework has the potential to support healthcare professionals in diagnosing monkeypox, particularly in cases where visual inspection is not sufficient. The framework's high accuracy and ability to detect subtle patterns make it a valuable tool in the fight against monkeypox.

**Key Takeaways:**

* A new AI framework, RS-CA-HSICT, has been developed to detect monkeypox.
* The framework combines CNNs and Transformers to analyze images of skin lesions.
* The framework achieved high accuracy (98.30%) and F1-score (98.13%) in tests.
* The framework has the potential to support healthcare professionals in diagnosing monkeypox."
cs.LG,SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome,"Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",https://arxiv.org/abs/2511.15464v1,2025-11-19T14:22:23Z,"Dabin Jeong, Amirhossein Vahidi, Ciro Ramírez-Suástegui, Marie Moullet, Kevin Ly, Mohammad Vali Sanian, Sebastian Birk, Yinshui Chang, Adam Boxall, Daniyal Jafree, Lloyd Steele, Vijaya Baskar MS, Muzlifah Haniffa, Mohammad Lotfollahi","**Unlocking the Secrets of Cancer Tissue: A New Approach to Understanding Histopathology Images and Spatial Transcriptome Profiles**

Researchers have developed a new framework called SIGMMA, which aims to improve our understanding of cancer tissue by aligning histopathology images with spatial transcriptome profiles. Histopathology images are obtained by staining tissue samples with a dye called Hematoxylin and Eosin (HE), while spatial transcriptome profiles provide information on the genetic activity of cells within the tissue.

The current methods for analyzing these two types of data have limitations, as they only consider a single scale of tissue organization. However, cancer tissue is complex and consists of various cell types that interact with each other in a hierarchical manner. SIGMMA addresses this limitation by introducing a multi-scale approach that captures the relationships between histopathology images and spatial transcriptome profiles at different scales.

The framework represents cell interactions as a graph and integrates relationships within and between subgraphs, allowing it to effectively capture cell-cell interactions within the tissue microenvironment. The results show that SIGMMA outperforms existing methods in predicting gene expression and retrieving cross-modal information, with improvements of 9.78% and 26.93%, respectively.

This breakthrough has the potential to enhance our understanding of cancer tissue organization and improve the diagnosis and treatment of cancer. By providing a more detailed and nuanced understanding of the relationships between histopathology images and spatial transcriptome profiles, SIGMMA may enable researchers and clinicians to identify new biomarkers and develop more effective therapies."
cs.LG,FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning,"Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.",https://arxiv.org/abs/2511.15454v1,2025-11-19T14:11:44Z,"Ouiame Marnissi, Hajar EL Hammouti, El Houcine Bergou","**Making Federated Learning More Efficient and Fair**

Federated learning is a way to train artificial intelligence models on many devices, like smartphones, without sharing their data. This approach helps keep data private. However, it's challenging to make sure all devices contribute fairly and use energy efficiently, especially when devices have different capabilities and communication speeds.

Researchers have proposed a new framework called FairEnergy, which aims to balance fairness, energy efficiency, and model accuracy. FairEnergy works by:

1. Measuring each device's contribution to the model training process.
2. Selecting the devices that will participate in training.
3. Allocating communication bandwidth to each device.
4. Adjusting the level of data compression.

Experiments showed that FairEnergy performs well, achieving higher model accuracy while reducing energy consumption by up to 79% compared to other approaches. This innovation could lead to more efficient and fair federated learning systems, enabling wider adoption of AI technologies."
cs.LG,TSFM in-context learning for time-series classification of bearing-health status,"This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",https://arxiv.org/abs/2511.15447v1,2025-11-19T14:01:12Z,"Michel Tokic, Slobodan Djukanović, Anja von Beuningen, Cheng Feng","Here's a summary of the research paper for a general audience:

**Predicting Bearing Health with AI: A New Approach**

Researchers have developed a new method for predicting the health status of machine bearings using artificial intelligence (AI). The method uses a type of AI model called a ""time-series foundation model"" (TSFM) to classify vibration data from a servo-press motor. What's innovative about this approach is that it can classify new, unseen data without needing to retrain the model.

The researchers applied this method to vibration data from a machine bearing and were able to accurately predict its health status. The method works by transforming vibration signals into a format that the AI model can understand, and then using the model to predict probabilities of the bearing being in a certain health state.

This breakthrough has significant implications for maintenance systems, as it enables the development of more general and scalable AI solutions that can be applied across different operating conditions. This is a major step forward from traditional, narrow AI solutions that are often customized for specific tasks. The new approach has the potential to lead to more efficient and effective maintenance systems, which can help prevent equipment failures and reduce downtime."
cs.LG,Gini Score under Ties and Case Weights,"The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",https://arxiv.org/abs/2511.15446v1,2025-11-19T14:01:12Z,"Alexej Brauer, Mario V. Wüthrich","**Understanding the Gini Score: A Tool for Evaluating Model Performance**

The Gini score is a widely used statistical tool that helps evaluate and compare the performance of different models, particularly in machine learning and data analysis. It's a simple yet powerful way to assess how well a model ranks risks or predictions.

Imagine you're trying to predict which patients are most likely to develop a certain disease. A good model should be able to accurately rank patients by their risk level. The Gini score measures how well a model does this by comparing the predicted rankings to the actual outcomes.

In simple terms, the Gini score is a measure of how well a model distinguishes between high-risk and low-risk cases. A higher Gini score indicates better performance.

This research paper explores two important issues related to the Gini score:

1. **Ties in risk rankings**: What happens when two or more cases have the same predicted risk score? The authors discuss how to handle these ties when calculating the Gini score.
2. **Case weights**: In many real-world applications, some cases are more important or have more weight than others. The authors show how to adapt the Gini score to account for these case weights.

By addressing these issues, the paper provides a more comprehensive understanding of the Gini score and its applications in data analysis and machine learning. This can help researchers and practitioners make more informed decisions when evaluating and comparing model performance."
cs.LG,Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation,"Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.",https://arxiv.org/abs/2511.15445v1,2025-11-19T13:58:32Z,"Victorita Dolean, Daria Hrebenshchykova, Stéphane Lanteri, Victor Michel-Dansac","**Breakthrough in Simulating Wave Propagation**

Simulating how waves propagate through different materials is crucial in various fields, such as acoustics, electromagnetism, and seismic analysis. However, traditional computer methods struggle to accurately and efficiently simulate these complex wave behaviors, especially in complex environments.

Researchers have proposed a new approach using artificial neural networks, called Finite Basis Physics-Informed Neural Networks (FBPINNs). This method divides the problem into smaller sub-areas, each solved by a local neural network. This ""domain decomposition"" approach allows for more efficient and accurate solutions to the Helmholtz equation, a fundamental equation in wave propagation.

The study demonstrates the potential of FBPINNs to overcome the limitations of traditional methods, particularly for high-frequency wave problems in complex domains. By leveraging neural networks and domain decomposition, this approach could lead to significant improvements in simulating wave propagation, enabling more accurate and efficient analysis in various fields."
cs.LG,Towards Understanding Layer Contributions in Tabular In-Context Learning Models,"Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the ""layers as painters"" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",https://arxiv.org/abs/2511.15432v1,2025-11-19T13:39:30Z,"Amir Rezaei Balef, Mykhailo Koshil, Katharina Eggensperger","**Unlocking the Secrets of Tabular In-Context Learning Models**

Researchers have made a significant step towards understanding how tabular in-context learning (ICL) models work, which are AI systems that can make predictions based on tabular data. Despite their similarities to large language models (LLMs), little was known about how individual layers within these models contribute to their predictions.

The study analyzed two tabular ICL models, TabPFN and TabICL, and discovered some surprising insights. By visualizing how the models' internal representations change across layers, the researchers found that:

* Not all layers are equally important; some layers seem to be redundant and don't add much value to the model's predictions.
* Different layers use different ""languages"" to represent the data, suggesting that the models may be using some layers inefficiently.

These findings have significant implications for improving tabular ICL models. By identifying and potentially removing redundant layers, models can be made more efficient, interpretable, and easier to understand. This research opens up new opportunities for model compression and improved performance, which could lead to more accurate predictions and better decision-making in various applications."
cs.LG,D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models,"Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.",https://arxiv.org/abs/2511.15411v1,2025-11-19T13:08:25Z,"Wenlun Zhang, Yunshan Zhong, Zihao Ding, Xinyu Li, Kentaro Yoshioka","**Making AI Models Smaller and Smarter**

Imagine you have a huge library of images and text descriptions, and you want to create a computer model that can understand both. This is what Contrastive Language-Image Pre-training (CLIP) models do. They're really good at tasks like identifying objects in images, but they use a lot of computer power and memory.

To make these models more efficient, researchers use a technique called quantization. This reduces the amount of information the model needs to process, making it smaller and faster. However, quantization usually requires access to the original data, which can be a problem if the data is private or hard to get.

A new technique called D4C solves this problem by generating fake images that are similar to real ones, but without needing the actual data. D4C uses three clever methods to create these fake images:

1. **Text prompts**: It uses text descriptions to guide the creation of fake images that make sense.
2. **Structural contrast**: It creates fake images with different parts, like backgrounds and foregrounds, to make them more realistic.
3. **Perturbations**: It adds small random changes to the fake images to make them more diverse.

By generating high-quality fake images, D4C makes it possible to quantize CLIP models without losing performance. In fact, D4C improves the accuracy of quantized CLIP models on several tasks, like image classification. This breakthrough could lead to more efficient and effective AI models that can be used on devices with limited resources."
cs.LG,Proximal Approximate Inference in State-Space Models,"We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.",https://arxiv.org/abs/2511.15409v1,2025-11-19T13:06:08Z,"Hany Abdulsamad, Ángel F. García-Fernández, Simo Särkkä","**Unlocking Insights in Complex Systems: A New Approach to State Estimation**

Imagine trying to track the movement of a car on a winding road, but the car's GPS signal is weak and the road is poorly mapped. This is similar to the challenge of estimating the state of complex systems, like weather patterns or financial markets, where the data is incomplete or uncertain. Researchers have developed a new class of algorithms to tackle this problem, called state estimation.

The new approach uses a mathematical framework that combines ideas from Bayesian inference and optimization. This framework allows researchers to make educated guesses about the state of the system, while taking into account the uncertainty in the data. The algorithm works by breaking down the problem into a series of smaller, more manageable updates, which are then combined to produce an estimate of the system's state.

The researchers focused on a specific type of approximation, called Gauss-Markov, which leads to efficient and scalable algorithms. They also developed techniques to handle complex, nonlinear systems, which are common in many fields.

**What does this mean?**

* More accurate predictions: The new algorithm can provide better estimates of complex systems, which can lead to more informed decision-making.
* Handling uncertainty: The approach can handle incomplete or uncertain data, making it useful for real-world applications where data is often noisy or missing.
* Wide applicability: The algorithm can be applied to a broad range of fields, from finance and economics to climate modeling and robotics.

Overall, this research presents a promising new approach to state estimation, which can help us better understand and predict complex systems."
cs.LG,Controlling False Positives in Image Segmentation via Conformal Prediction,"Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.",https://arxiv.org/abs/2511.15406v1,2025-11-19T13:02:50Z,"Luca Mossina, Corentin Friedrich","Here's a summary of the research paper for a general audience:

**Title:** A New Way to Make Image Segmentation More Reliable

**What it's about:** Image segmentation is a crucial task in medical imaging, where computers help doctors identify specific areas of interest, such as tumors or damaged tissues. However, current computer models can make mistakes, and it's essential to have a way to measure and control these errors.

**The problem:** Deep learning models, which are commonly used for image segmentation, don't provide clear statistical guarantees on their errors. This means that doctors can't always trust the results, especially in critical situations where over-segmentation (identifying too much tissue as abnormal) can have serious consequences.

**The solution:** Researchers have developed a simple, post-hoc framework that adds a layer of confidence to image segmentation results. This framework uses a calibration set of labeled images to adjust the model's output, ensuring that the proportion of false positives (incorrectly identified areas) stays below a certain threshold with high probability.

**Key benefits:**

* The method is model-agnostic, meaning it can be used with any pre-trained segmentation model.
* It requires no retraining of the model, making it a practical solution.
* It provides finite-sample guarantees, which means that the error rate can be controlled for a specific set of images.

**Real-world impact:** This research has the potential to improve the reliability of image segmentation in medical settings, such as polyp segmentation (identifying abnormal growths in the colon). By controlling false positives, doctors can make more informed decisions, and patients can receive more accurate diagnoses and treatments."
cs.LG,EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG,"The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.",https://arxiv.org/abs/2511.15393v1,2025-11-19T12:39:19Z,"Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang","**Breakthrough in Brain Health Monitoring: Introducing EVA-Net**

Researchers have developed a new AI model called EVA-Net that can accurately predict brain age from electroencephalography (EEG) data, a common tool used to assess brain health. The innovation lies in its ability to learn from imperfect medical data, which is often noisy and variable.

**The Problem: Imperfect Data and Black Box Models**

Existing models struggle with imperfect data, making it challenging to identify healthy brain aging patterns. Moreover, these models are often ""black boxes"" that lack transparency, making it difficult to understand their decision-making process.

**The Solution: EVA-Net**

EVA-Net addresses these challenges by:

1. **Learning from Imperfect Data**: EVA-Net uses a robust and efficient algorithm to handle noisy and variable data.
2. **Providing Interpretable Results**: The model provides a clear and transparent way to understand brain aging patterns, allowing for early detection of potential health issues.

**Key Features and Benefits**

* **Continuous Aging Prototypes**: EVA-Net learns a continuous prototype network that explicitly learns the normative healthy aging manifold, providing a clear understanding of brain aging patterns.
* **Anomaly Detection**: The model can identify deviations from healthy brain aging patterns, which can help diagnose diseases such as Alzheimer's and Mild Cognitive Impairment (MCI).
* **State-of-the-Art Accuracy**: EVA-Net achieves state-of-the-art accuracy in brain age prediction, outperforming existing models.

**Testing and Validation**

The researchers tested EVA-Net on a large dataset of 1297 healthy individuals and validated its performance on a separate group of 27 patients with MCI and Alzheimer's disease. The results showed that EVA-Net accurately predicted brain age and detected anomalies in the patients' brain activity.

**Implications and Future Directions**

The development of EVA-Net has significant implications for healthcare, as it provides a reliable and transparent tool for monitoring brain health. Future studies will focus on further validating the model and exploring its potential applications in clinical settings.

**Conclusion**

EVA-Net represents a major breakthrough in brain health monitoring, offering a reliable and transparent tool for predicting brain age and detecting potential health issues. Its development has the potential to improve diagnosis and treatment of brain-related diseases, and it may lead to more effective prevention and intervention strategies."
cs.LG,Parameter Importance-Driven Continual Learning for Foundation Models,"Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.",https://arxiv.org/abs/2511.15375v1,2025-11-19T12:07:53Z,"Lingxiang Wang, Hainan Zhang, Zhiming Zheng","**Breakthrough in AI Training: Preserving General Intelligence while Learning New Tasks**

Imagine a super-smart AI model that can understand and reason across various topics, but also learn new domain-specific knowledge without forgetting its general intelligence. This is a significant challenge in AI research, as traditional methods often lead to ""catastrophic forgetting,"" where the model loses its general abilities when learning new tasks.

Researchers have now developed a novel solution called PIECE, which enables foundation models (large language and multimodal models) to learn new tasks without compromising their general intelligence. PIECE works by selectively updating only a tiny fraction (0.1%) of the model's core parameters that are most relevant to the new task.

The key innovation behind PIECE is its use of two importance estimators that help identify the most critical parameters to update. This approach allows the model to efficiently learn new domain knowledge without accessing prior training data or increasing its parameter count.

**Key Benefits:**

* Preserves general intelligence while learning new tasks
* Efficiently learns domain-specific knowledge without catastrophic forgetting
* Scalable and adaptable to dynamic real-world environments

**Impact:**

The PIECE method has been tested on various language and multimodal models, achieving state-of-the-art performance across diverse downstream tasks. This breakthrough paves the way for developing practical, domain-adaptive foundation models that can learn and adapt continuously without losing their general intelligence."
cs.CV,US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery,"Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete",https://arxiv.org/abs/2511.15600v1,2025-11-19T16:45:04Z,"Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour","**Advancing Spinal Imaging: A Breakthrough in 3D Ultrasound Technology**

Imagine a medical imaging technology that is radiation-free, cost-effective, and provides real-time visuals of the spine. Ultrasound is just that, but it has limitations in visualizing the complete anatomy of the spine due to the shadowing effect of bones. A team of researchers has developed a novel approach called US-X Complete, which combines ultrasound with a single X-ray image to create a more complete and accurate 3D picture of the spine.

The researchers used artificial intelligence and deep learning techniques to integrate information from both ultrasound and X-ray images. They created a large dataset of simulated X-ray scans and 3D spine models to train their algorithm. The results show significant improvements in reconstructing the spine's anatomy, particularly the vertebral bodies, which are often obscured by ultrasound.

The study used phantom studies, which are simulations of human tissue, to test the technology. The results demonstrate that US-X Complete can provide a more accurate and complete 3D visualization of the lumbar spine, overlayed on the ultrasound scan. This breakthrough has the potential to enhance the accuracy and safety of spinal procedures, and could lead to better patient outcomes.

The researchers have made their code and data publicly available, which could facilitate further development and clinical translation of this technology. Overall, US-X Complete represents a promising advancement in spinal imaging, one that could improve the way doctors visualize and treat spinal conditions."
cs.CV,Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition,"LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.",https://arxiv.org/abs/2511.15597v1,2025-11-19T16:41:30Z,"Xufei Wang, Junqiao Zhao, Siyue Tao, Qiwen Gu, Wonbong Kim, Tiantian Feng","**Improving LiDAR Place Recognition: A New Approach to Learning from Mistakes**

Imagine you're driving a self-driving car or navigating a robot through a new environment. One crucial task is recognizing familiar places, like intersections or buildings, to ensure safe and accurate navigation. This task is called LiDAR place recognition. However, current methods struggle to adapt to new environments without forgetting what they've learned before. This is known as ""catastrophic forgetting.""

To address this issue, researchers have developed a new framework called KDF+. This approach helps LiDAR place recognition systems learn from their mistakes and retain previously learned knowledge. Here's how it works:

1. **Loss-aware sampling**: The system identifies which samples (or experiences) are most difficult to learn from and prioritizes those for re-learning. This ensures that the system focuses on the most challenging situations.
2. **Rehearsal enhancement**: The system refines its memory of previously learned experiences during new-task training, making sure that it retains long-term knowledge.

The researchers tested KDF+ on multiple benchmarks and found that it consistently outperforms existing methods. This new approach can be integrated into state-of-the-art frameworks, leading to significant and stable performance gains. The code for KDF+ is also being made publicly available, which will help advance research in this area.

In summary, KDF+ offers a promising solution to improve LiDAR place recognition by learning from mistakes and retaining previously learned knowledge. This breakthrough has the potential to enhance the performance and safety of autonomous vehicles and robots navigating complex environments."
cs.CV,MHR: Momentum Human Rig,"We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",https://arxiv.org/abs/2511.15586v1,2025-11-19T16:18:02Z,"Aaron Ferguson, Ahmed A. A. Osman, Berta Bescos, Carsten Stoll, Chris Twigg, Christoph Lassner, David Otte, Eric Vignola, Federica Bogo, Igor Santesteban, Javier Romero, Jenna Zarate, Jeongseok Lee, Jinhyung Park, Jinlong Yang, John Doublestein, Kishore Venkateshan, Kris Kitani, Ladislav Kavan, Marco Dal Farra, Matthew Hu, Matthew Cioffi, Michael Fabris, Michael Ranieri, Mohammad Modarres, Petr Kadlecek, Rinat Abdrashitov, Romain Prévost, Roman Rajbhandari, Ronald Mallet, Russel Pearsall, Sandy Kao, Sanjeev Kumar, Scott Parrish, Te-Li Wang, Tony Tung, Yuan Dong, Yuhua Chen, Yuanlu Xu, Yuting Ye, Zhongshi Jiang","Here's a summary of the research paper for a general audience:

**Introducing MHR: A New Way to Animate Human Bodies**

Researchers have developed a new tool called MHR (Momentum Human Rig) that helps create realistic and expressive human animations for use in movies, video games, and virtual reality experiences.

MHR is a digital model of the human body that can be posed and animated in a wide range of movements. What's special about MHR is that it allows for very realistic and natural-looking movements, while also being flexible and easy to use.

The model is designed to work well with modern technology, such as augmented reality (AR) and virtual reality (VR) systems, and can be easily integrated into existing animation pipelines. This means that animators and developers can use MHR to create more realistic and engaging human characters for a variety of applications.

Overall, MHR has the potential to improve the quality and realism of human animations in a wide range of fields, from entertainment to education and beyond."
cs.CV,CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking,"3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",https://arxiv.org/abs/2511.15580v1,2025-11-19T16:12:24Z,"Sifan Zhou, Yichao Cao, Jiahao Nie, Yuqian Fu, Ziyu Zhao, Xiaobo Lu, Shuo Wang","**Breakthrough in 3D Object Tracking for Autonomous Driving**

Researchers have developed a new framework called CompTrack, which improves the accuracy and efficiency of 3D single object tracking in LiDAR point clouds, a crucial task for autonomous driving. The innovation addresses two major challenges: (1) irrelevant background noise and (2) redundant information within the object being tracked.

CompTrack uses two key modules:

1. **Spatial Foreground Predictor (SFP)**: filters out background noise, enhancing accuracy by focusing on relevant data.
2. **Information Bottleneck-guided Dynamic Token Compression (IB-DTC)**: compresses the object's information into a more compact and informative format, boosting efficiency.

The results are impressive: CompTrack achieves top performance on three major datasets (KITTI, nuScenes, and Waymo) while running at a rapid 90 frames per second on a single GPU. This advancement has the potential to enhance the reliability and speed of autonomous driving systems."
cs.CV,AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning,"With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.",https://arxiv.org/abs/2511.15578v1,2025-11-19T16:09:38Z,"Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar","**Breakthrough in Video Question Answering: AVATAAR**

Imagine being able to ask a video a question and getting a precise answer. With the rise of video content, this has become increasingly important. Researchers have developed AVATAAR, a new system that helps computers understand and answer questions about long videos.

**The Challenge**

Current computer systems struggle with nuanced questions that require a deep understanding of the video content. They often miss important details or fail to connect the dots.

**The Solution: AVATAAR**

AVATAAR is a modular system that combines two types of video context: global (overall) and local (specific). It uses a ""thinking agent"" to retrieve relevant information and a ""rethink module"" to refine its answers. This feedback loop allows AVATAAR to iteratively reason like a human, refining its answers based on partial results.

**The Results**

In tests on the CinePile benchmark, AVATAAR outperformed existing systems, achieving significant gains in:

* Temporal reasoning (5.6% improvement)
* Technical queries (5% improvement)
* Theme-based questions (8% improvement)
* Narrative comprehension (8.2% improvement)

**The Impact**

AVATAAR presents a scalable solution for long-form video question answering, offering:

* Improved accuracy
* Interpretability (understanding how the system arrives at its answers)
* Extensibility (easily adaptable to new applications)

This breakthrough has the potential to revolutionize applications such as video search, education, and entertainment, enabling more efficient and effective video analysis and interaction."
cs.CV,From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers,"Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.",https://arxiv.org/abs/2511.15572v1,2025-11-19T16:03:21Z,"Huiyuan Tian, Bonan Xu, Shijian Li, Xin Jin","**Unlocking the Secrets of Vision Transformers: A Breakthrough in Feature Distillation**

Vision Transformers (ViTs) are a type of artificial intelligence model that have shown great promise in image recognition tasks. However, when it comes to ""distilling"" knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student), ViTs have proven to be challenging. Researchers have been puzzled by this phenomenon, but a recent study has shed light on the issue.

The study reveals that ViTs have a unique property: their feature representations are globally low-rank, meaning that they can be captured with a relatively small number of dimensions. This suggests that a simple linear projector should be enough to align the features of the teacher and student models. However, this is not the case in practice.

To understand why, the researchers introduced a new analysis technique called Spectral Energy Pattern (SEP). SEP shows that individual tokens (or parts of the image) in ViTs distribute energy across most channels, creating a high-bandwidth encoding pattern. This results in an ""encoding mismatch"" between the wide teacher model and the narrow student model.

Armed with this insight, the researchers proposed two simple strategies to overcome the encoding mismatch:

1. **Post-hoc feature lifting**: adding a lightweight projector to the student model to match the teacher's feature representation.
2. **Native width alignment**: widening the student's last block to match the teacher's width.

These strategies led to significant improvements in image recognition accuracy, boosting the performance of a small ViT model from 74.86% to 77.53% and 78.23% when distilling knowledge from a larger model.

The study provides valuable guidance for designing compact ViTs and explains why feature distillation has been challenging for these models. By exploiting the low-rank structure of ViTs, researchers can develop more effective and interpretable methods for knowledge distillation."
cs.CV,Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector,"Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.",https://arxiv.org/abs/2511.15571v1,2025-11-19T16:03:15Z,"Weiheng Zhu, Gang Cao, Jing Liu, Lifang Yu, Shaowei Weng","**Can AI-generated Image Detectors be Fooled?**

Researchers have made significant progress in developing detectors that can identify images generated by artificial intelligence (AI). However, these detectors may not be as secure as we think. A new study proposes a method called Dual-domain Feature Importance Attack (DuFIA) that can potentially fool these detectors.

DuFIA works by creating ""adversarial examples"" - images that are slightly altered to deceive the detectors. The researchers found that by manipulating the spatial and frequency features of an image, they can create a ""cloak of invisibility"" that makes AI-generated images appear as if they were created by humans.

The study tested DuFIA on various AI-generated image detectors and found that it can effectively evade detection across different models. This raises concerns about the security and reliability of these detectors. The researchers hope that their work will lead to the development of more robust and secure detectors in the future.

**In simple terms:** Researchers have found a way to trick AI detectors that try to identify fake images. This highlights the need for more secure and reliable detectors to prevent the spread of misinformation."
cs.CV,Computer-Use Agents as Judges for Generative User Interface,"Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",https://arxiv.org/abs/2511.15567v1,2025-11-19T16:00:02Z,"Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou","**Imagine a Future Where Computers Design Their Own Interfaces**

Researchers have made a breakthrough in creating computer systems that can design and evaluate their own user interfaces. Currently, computer interfaces are designed with humans in mind, making it difficult for computer agents to efficiently interact with them. The researchers propose a new framework where computer agents, called Computer-Use Agents (CUAs), work together with coding models (Coders) to design and refine interfaces.

**The Problem: Human-Centered Interfaces**

Most computer interfaces are designed to be visually appealing and easy for humans to use. However, this can make it challenging for computer agents to navigate and perform tasks efficiently. The researchers argue that interfaces should be designed with computer agents in mind, prioritizing efficiency and reliability.

**The Solution: Coder-CUA Collaboration**

The researchers have developed a benchmark called AUI-Gym, which tests the design of 52 different applications across various domains. They propose a framework where:

1. **Coder (Designer)**: generates and revises interface designs
2. **CUA (Judge)**: evaluates the functionality and provides feedback to refine the designs

The goal is to create interfaces that are efficient and reliable for computer agents to use, rather than just visually appealing. The researchers have also created a CUA Dashboard that provides concise visual summaries of navigation histories, helping designers improve the interfaces.

**The Impact: Agent-Native Efficiency**

This innovation has the potential to revolutionize the way computer systems interact with digital environments. By positioning agents as both designers and judges, the framework shifts interface design toward agent-native efficiency and reliability. This could lead to more efficient and autonomous computer systems, enabling them to play a more active role in shaping their digital environments.

**What's Next?**

The researchers have made their code and dataset available, paving the way for further development and testing of this technology. As computer systems become increasingly capable of autonomous operation, this breakthrough could have significant implications for the future of human-computer interaction."
cs.CV,Scriboora: Rethinking Human Pose Forecasting,"Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.",https://arxiv.org/abs/2511.15565v1,2025-11-19T15:58:33Z,"Daniel Bermuth, Alexander Poeppel, Wolfgang Reif","**Advancing Human Pose Forecasting: A New Approach**

Imagine being able to predict what someone will do next, just by watching their movements. This is the goal of human pose forecasting, a technology with many exciting applications in fields like self-driving cars, robotics, and action recognition.

Researchers have been working on developing algorithms to predict future human poses based on past observations. However, a recent study revealed that many of these algorithms have issues with reproducibility, making it difficult to compare their performance.

The study introduced a unified framework for training and evaluating these algorithms, and explored a new approach inspired by speech recognition technology. By adapting speech models to human pose forecasting, the researchers achieved state-of-the-art results.

But how do these models perform in real-world situations, where data may be noisy or imperfect? To answer this, the researchers tested their models with simulated noisy data, mimicking the kind of errors that can occur when using pose estimation technology. They found that the models' performance degraded significantly, but that unsupervised fine-tuning could help recover some of the lost accuracy.

Overall, this study highlights the potential of human pose forecasting and the need for more robust and reliable algorithms. By advancing this technology, we can unlock new applications in areas like robotics, healthcare, and entertainment."
cs.CV,Multimodal Evaluation of Russian-language Architectures,"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",https://arxiv.org/abs/2511.15552v1,2025-11-19T15:43:53Z,"Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova","Here's a summary of the research paper for a general audience:

**Understanding Multimodal Language Models**

Imagine a computer program that can understand and respond to text, images, audio, and video - just like humans do. These ""multimodal"" language models are rapidly improving, but we still don't fully understand their strengths, weaknesses, and potential risks.

**A New Benchmark for Russian Language Models**

To address this gap, researchers have created a new evaluation framework called Mera Multi, specifically designed for Russian-language models. This framework includes 18 new tasks that test a model's ability to understand and respond to different types of input, such as text, images, audio, and video.

**What's New and Important**

The researchers have made several important contributions:

* They've created a universal system for categorizing multimodal abilities, which can be used across different languages.
* They've developed 18 new datasets that are tailored to Russian culture and language, which can be used to evaluate and improve language models.
* They've tested several language models, both open-source and closed-source, and provided baseline results.
* They've also developed a method for preventing ""benchmark leakage,"" which ensures that the evaluation framework remains secure and reliable.

**Implications and Future Directions**

While this research focuses on Russian-language models, the methodology and framework can be adapted for other languages, particularly within the Slavic language family. This work has the potential to improve our understanding of multimodal language models and their applications, and could lead to more sophisticated and effective language models in the future."
cs.CV,A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture,"The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",https://arxiv.org/abs/2511.15535v1,2025-11-19T15:32:08Z,"Pandiyaraju V, Abishek Karthik, Sreya Mynampati, Poovarasan L, D. Saraswathi","**Breakthrough in Weed Detection for Sustainable Farming**

Researchers have developed a powerful artificial intelligence (AI) framework to accurately detect weeds in crops, enabling farmers to use herbicides more efficiently and sustainably. The framework combines three types of deep learning models to analyze images of crops and weeds, allowing it to adapt to various field conditions.

To improve the model's performance, the researchers used a technique called Generative Adversarial Network (GAN)-based augmentation, which generates new images to balance the dataset and prevent bias. They also used a self-supervised learning method to extract more features from limited labeled data.

The results are impressive, with the model achieving 99.33% accuracy in detecting weeds on multiple datasets. The framework's design allows for real-time deployment on edge devices, making it a practical solution for farmers. This innovation has the potential to reduce the overuse of herbicides, promote sustainable farming practices, and increase crop yields.

**Key benefits:**

* Accurate weed detection to optimize herbicide use
* Sustainable farming practices
* Real-time deployment on edge devices
* Scalable and adaptable solution for precision agriculture

This development has significant implications for the future of farming, enabling farmers to make data-driven decisions and reduce their environmental impact."
cs.CV,Multi-Text Guided Few-Shot Semantic Segmentation,"Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.",https://arxiv.org/abs/2511.15515v1,2025-11-19T15:09:19Z,"Qiang Jiao, Bin Yan, Yi Yang, Mengrui Shi, Qiang Zhang","Here's a summary of the research paper ""Multi-Text Guided Few-Shot Semantic Segmentation"" for a general audience:

**What's the goal?**
The goal of this research is to improve a type of artificial intelligence (AI) called semantic segmentation, which is used to identify and separate objects within images. Specifically, the researchers want to improve the AI's ability to segment objects with just a few examples, known as few-shot learning.

**The problem**
Current AI models that use text to guide image segmentation often rely on a single text description, which can be too simplistic to capture the complexity of real-world objects. This can lead to incomplete or inaccurate segmentation.

**The solution**
The researchers propose a new AI model called MTGNet, which uses multiple text descriptions to guide the segmentation process. This approach allows the model to capture a wider range of semantic information and improve its accuracy. The model consists of three key components:

1. A module that refines the text descriptions to better capture the object's semantics.
2. A module that helps transfer information from the support images to the query images, improving semantic consistency.
3. A module that enhances the model's robustness to noisy or inconsistent features.

**The results**
The researchers tested their model on two standard benchmarks and achieved state-of-the-art results, with significant improvements in cases where objects have high variability within their class. Specifically, their model achieved 76.8% and 57.4% accuracy on two benchmarks, respectively.

**Why it matters**
This research has the potential to improve various applications, such as image editing, object detection, and autonomous driving, where accurate segmentation of objects is crucial. The proposed approach can also be extended to other areas, such as natural language processing and computer vision."
cs.CV,Learning to Expand Images for Efficient Visual Autoregressive Modeling,"Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.",https://arxiv.org/abs/2511.15499v1,2025-11-19T14:55:07Z,"Ruiqing Yang, Kaixin Zhang, Zheng Zhang, Shan You, Tao Huang","**Breakthrough in Image Generation: A New, Efficient Approach**

Imagine being able to generate high-quality images quickly and efficiently, similar to how our brains process visual information. Researchers have made a significant advancement in this area by developing a novel approach called Expanding Autoregressive Representation (EAR). This method generates images in a spiral pattern, starting from the center and expanding outward, mimicking how our brains perceive visual information.

The EAR approach has two key benefits:

1. **Faster generation**: By generating images in a spiral pattern, EAR enables parallel decoding, which significantly speeds up the process.
2. **Improved quality**: The spiral pattern also helps preserve spatial continuity, resulting in more realistic and detailed images.

In tests on the ImageNet dataset, EAR achieved the best balance between image quality and generation speed, outperforming existing methods. This breakthrough has the potential to lead to more efficient and effective image generation, with applications in areas such as computer vision, art, and design. The EAR approach is also biologically inspired, making it a more cognitively aligned way of generating images."
cs.CV,Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels,"Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",https://arxiv.org/abs/2511.15496v1,2025-11-19T14:52:51Z,"Maria Pilligua, David Serrano-Lozano, Pai Peng, Ramon Baldrich, Michael S. Brown, Javier Vazquez-Corral","**Improving Low-Light Photos: A New Approach**

Taking good photos in low-light conditions can be tough. The images often turn out dark, noisy, and lacking in color. Researchers have been working on developing techniques to enhance low-light images, but most of these methods are only trained on images taken in a single low-light condition. This makes it difficult to know how well they will perform in different lighting situations.

A new study has addressed this issue by creating a dataset of images taken at various light intensities. This dataset, called MILL, allows researchers to test low-light enhancement algorithms in a more comprehensive way. The study evaluated several state-of-the-art methods and found that their performance varied significantly across different lighting conditions.

The good news is that the researchers were able to use their dataset to develop improvements that make these algorithms more robust across diverse lighting scenarios. These improvements resulted in significant enhancements in image quality, with up to 10 dB PSNR improvement for DSLR cameras and 2 dB for smartphone cameras.

In simple terms, this research has taken a step towards developing more effective methods for enhancing low-light images, which could lead to better photos in a variety of situations, from indoor events to nighttime photography."
cs.CV,NTK-Guided Implicit Neural Teaching,"Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",https://arxiv.org/abs/2511.15487v1,2025-11-19T14:43:04Z,"Chen Zhang, Wei Zuo, Bingyang Cheng, Yikun Wang, Wei-Bin Kou, Yik Chung WU, Ngai Wong","**Breakthrough in AI Training: NTK-Guided Implicit Neural Teaching**

Imagine being able to train artificial intelligence (AI) models much faster, without sacrificing their accuracy. Researchers have made a significant breakthrough in this area with the development of NTK-Guided Implicit Neural Teaching (NINT).

**What is NINT?**

NINT is a new method for training AI models, specifically those that use Implicit Neural Representations (INRs). INRs are a way to represent complex data, like images and audio, in a compact and efficient way. However, training these models can be slow and computationally expensive, especially for high-resolution data.

**How does NINT work?**

NINT uses a technique called the Neural Tangent Kernel (NTK) to guide the training process. The NTK helps the model focus on the most important data points, which enables it to learn faster and more efficiently. By dynamically selecting the most informative data points, NINT accelerates the training process while maintaining or even improving the model's accuracy.

**What are the benefits?**

The NINT method has been shown to significantly reduce training time, by nearly half, compared to existing methods. This breakthrough has the potential to enable faster development and deployment of AI models in various applications, such as computer vision, audio processing, and 3D reconstruction.

**In simple terms...**

Think of NINT like a GPS navigation system for AI training. It helps the model navigate the vast amount of data and focus on the most important points, getting it to its destination (accurate results) much faster. This innovation has the potential to speed up AI development and make it more accessible to a wider range of applications."
cs.CV,A Novel CustNetGC Boosted Model with Spectral Features for Parkinson's Disease Prediction,"Parkinson's disease is a neurodegenerative disorder that can be very tricky to diagnose and treat. Such early symptoms can include tremors, wheezy breathing, and changes in voice quality as critical indicators of neural damage. Notably, there has been growing interest in utilizing changes in vocal attributes as markers for the detection of PD early on. Based on this understanding, the present paper was designed to focus on the acoustic feature analysis based on voice recordings of patients diagnosed with PD and healthy controls (HC). In this paper, we introduce a novel classification and visualization model known as CustNetGC, combining a Convolutional Neural Network (CNN) with Custom Network Grad-CAM and CatBoost to enhance the efficiency of PD diagnosis. We use a publicly available dataset from Figshare, including voice recordings of 81 participants: 40 patients with PD and 41 healthy controls. From these recordings, we extracted the key spectral features: L-mHP and Spectral Slopes. The L-mHP feature combines three spectrogram representations: Log-Mel spectrogram, harmonic spectrogram, and percussive spectrogram, which are derived using Harmonic-Percussive Source Separation (HPSS). Grad-CAM was used to highlight the important regions in the data, thus making the PD predictions interpretable and effective. Our proposed CustNetGC model achieved an accuracy of 99.06% and precision of 95.83%, with the area under the ROC curve (AUC) recorded at 0.90 for the PD class and 0.89 for the HC class. Additionally, the combination of CatBoost, a gradient boosting algorithm, enhanced the robustness and the prediction performance by properly classifying PD and non-PD samples. Therefore, the results provide the potential improvement in the CustNetGC system in enhancing diagnostic accuracy and the interpretability of the Parkinson's Disease prediction model.",https://arxiv.org/abs/2511.15485v1,2025-11-19T14:41:34Z,"Abishek Karthik, Pandiyaraju V, Dominic Savio M, Rohit Swaminathan S","**Breakthrough in Parkinson's Disease Diagnosis: A New Model for Early Detection**

Parkinson's disease is a neurodegenerative disorder that can be challenging to diagnose and treat. Researchers have been exploring new ways to detect the disease early on, and a recent study has made significant progress in this area. The study focuses on analyzing voice recordings to identify changes in vocal attributes that can indicate Parkinson's disease.

The researchers developed a novel model called CustNetGC, which combines a Convolutional Neural Network (CNN) with Custom Network Grad-CAM and CatBoost. They tested this model on a dataset of voice recordings from 81 participants, including 40 patients with Parkinson's disease and 41 healthy controls.

The results show that the CustNetGC model is highly effective in predicting Parkinson's disease, achieving an accuracy of 99.06% and a precision of 95.83%. The model also provides interpretable results, highlighting the important regions in the data that contribute to the predictions.

The study used spectral features extracted from voice recordings, including L-mHP and Spectral Slopes, which are derived using advanced signal processing techniques. The combination of CatBoost, a gradient boosting algorithm, enhanced the robustness and prediction performance of the model.

Overall, this study demonstrates the potential of using voice recordings and machine learning algorithms to improve the diagnosis of Parkinson's disease. The CustNetGC model offers a promising approach for early detection and diagnosis, which could lead to better treatment outcomes and improved quality of life for patients with Parkinson's disease."
cs.CV,FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI,"Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.",https://arxiv.org/abs/2511.15481v1,2025-11-19T14:37:49Z,"Luisa Gallée, Yiheng Xiong, Meinrad Beer, Michael Götz","Here's a summary of the research paper for a general audience:

**Introducing FunnyNodules: A New Tool for Improving AI in Medical Imaging**

Imagine you're a doctor trying to diagnose a patient with a lung disease. You look at an image of the patient's lungs and try to make a decision based on what you see. But how do you know that a computer program designed to help with this task is making decisions for the right reasons?

Currently, there aren't many datasets that allow researchers to test whether AI models are making accurate predictions based on the right features in medical images. To address this issue, researchers have created a new dataset called FunnyNodules. This dataset is special because it allows researchers to control the characteristics of the images, such as the shape and texture of lung nodules, and the corresponding diagnosis.

FunnyNodules is a synthetic dataset, meaning it's generated by computer algorithms rather than being composed of real patient data. This allows researchers to have complete control over the data and to test AI models in a systematic way. The dataset can be customized to mimic different types of medical images, making it a valuable tool for developing and evaluating AI models that can explain their decisions.

The goal of FunnyNodules is to help researchers develop more transparent and trustworthy AI models that can assist doctors in making accurate diagnoses. By using this dataset, researchers can test whether AI models are learning to recognize the right features in medical images and making decisions for the right reasons. This can ultimately lead to better patient outcomes and more effective use of AI in healthcare."
cs.CV,RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection,"This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",https://arxiv.org/abs/2511.15476v1,2025-11-19T14:32:34Z,"Rashid Iqbal, Saddam Hussain Khan","**Breakthrough in Monkeypox Detection: A New AI Framework**

Researchers have developed a powerful artificial intelligence (AI) framework to detect Monkeypox, a viral disease. The framework, called RS-CA-HSICT, combines the strengths of two popular AI techniques: Convolutional Neural Networks (CNNs) and Transformers. This hybrid approach enables the framework to capture both local and global features of the disease, leading to more accurate detection.

**How it works**

The RS-CA-HSICT framework consists of several components that work together to analyze images of skin lesions. It:

1. **Extracts features**: The framework uses CNNs to extract detailed information about the lesions, such as texture and shape.
2. **Captures long-range dependencies**: The Transformer component helps to identify relationships between different parts of the image, which is essential for detecting subtle patterns.
3. **Refines features**: The framework uses a technique called channel attention to focus on the most relevant features and suppress redundant ones.
4. **Detects subtle patterns**: A spatial attention mechanism helps to refine the detection of subtle patterns and variations in the lesions.

**Results**

The researchers tested the RS-CA-HSICT framework on two datasets and achieved impressive results:

* **Classification accuracy**: 98.30%
* **F1-score**: 98.13%

These results outperform existing AI models, demonstrating the potential of the RS-CA-HSICT framework for accurate Monkeypox detection. This breakthrough has significant implications for the diagnosis and treatment of the disease."
cs.CV,Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners,"Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.",https://arxiv.org/abs/2511.15468v1,2025-11-19T14:26:04Z,"Xabier Lekunberri, Ahmad Kamal, Izaro Goienetxea, Jon Ruiz, Iñaki Quincoces, Jaime Valls Miro, Ignacio Arganda-Carreras, Jose A. Fernandes-Salvador","**Improving Tuna Fishing with Artificial Intelligence**

Tuna fishing is a significant industry, with over 69% of tropical tuna caught using a type of fishing gear called purse seiners. To monitor and manage tuna fisheries, electronic monitoring (EM) systems are being used to collect video data. However, analyzing this data is a time-consuming task for human analysts. Researchers are now exploring the use of artificial intelligence (AI) to automate the process and improve accuracy.

One of the challenges in using AI for tuna fishing is identifying different species, particularly bigeye tuna and yellowfin tuna, which can be difficult to distinguish. To address this, researchers created a reliable dataset of images labeled by experts and developed a multi-stage pipeline to estimate the species composition of catches.

The researchers tested different approaches for segmenting and classifying tuna images and found that a combination of YOLOv9 and SAM2 with a hierarchical classification model performed the best. This approach was able to accurately segment and classify 84.8% of the individuals, with a mean average error of 4.5%.

**In Simple Terms:**

* Tuna fishing generates a lot of video data that is hard to analyze manually.
* AI can help automate the process, but identifying different species of tuna is a challenge.
* Researchers developed a new approach using AI to accurately identify and classify tuna species.
* The approach was tested and showed promising results, with high accuracy and low error rates.

This research has the potential to improve the management of tuna fisheries and reduce the workload of human analysts. By using AI to automate the analysis of video data, fisheries can make more informed decisions and ensure the sustainability of tuna populations."
cs.CV,SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome,"Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",https://arxiv.org/abs/2511.15464v1,2025-11-19T14:22:23Z,"Dabin Jeong, Amirhossein Vahidi, Ciro Ramírez-Suástegui, Marie Moullet, Kevin Ly, Mohammad Vali Sanian, Sebastian Birk, Yinshui Chang, Adam Boxall, Daniyal Jafree, Lloyd Steele, Vijaya Baskar MS, Muzlifah Haniffa, Mohammad Lotfollahi","**Unlocking the Secrets of Cancer Tissue: A New Approach to Understanding Histopathology Images and Gene Activity**

Researchers have developed a new method called SIGMMA to analyze histopathology images and spatial transcriptome profiles, which provide information on gene activity in tissue samples. Histopathology images are used to diagnose diseases, including cancer, while spatial transcriptome profiles reveal which genes are active in specific areas of the tissue.

The current methods for analyzing these two types of data have limitations, as they only look at the data at a single scale, missing important details about the organization of cells within the tissue. SIGMMA addresses this limitation by using a hierarchical graph-based approach to align histopathology images with spatial transcriptome profiles at multiple scales.

**What does this mean?**

* **Multi-scale analysis**: SIGMMA looks at the data at different scales, from small groups of cells to larger tissue structures, to capture a more complete picture of the relationships between cells and gene activity.
* **Graph-based approach**: The method represents cell interactions as a graph, which allows it to effectively capture relationships between cells and their organization within the tissue.
* **Improved results**: The researchers demonstrated that SIGMMA leads to significant improvements in predicting gene expression and retrieving relevant information from histopathology images and spatial transcriptome profiles.

**Why is this important?**

The development of SIGMMA has the potential to improve our understanding of cancer and other diseases by providing a more detailed and accurate picture of the relationships between cells and gene activity in tissue samples. This could lead to better diagnosis, treatment, and patient outcomes. Additionally, the method may be applied to other fields, such as neuroscience and immunology, where understanding the relationships between different types of data is crucial."
cs.AI,What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity,"AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",https://arxiv.org/abs/2511.15593v1,2025-11-19T16:32:18Z,"Alexis Audran-Reiss, Jordi Armengol Estapé, Karen Hambardzumyan, Amar Budhiraja, Martin Josifoski, Edan Toledo, Rishi Hazra, Despoina Magka, Michael Shvartsman, Parth Pathak, Justine T Kao, Lucia Cipolina-Kun, Bhavul Gauri, Jean-Christophe Gagnon-Audet, Emanuel Tewolde, Jenny Zhang, Taco Cohen, Yossi Adi, Tatiana Shavrina, Yoram Bachrach","Here's a summary of the research paper for a general audience:

**The Future of AI Research: What Makes a Good AI Assistant?**

Imagine having a robotic assistant that can help scientists make new discoveries and advance research. AI research agents are being developed to do just that, but we still don't fully understand what makes them successful. A recent study explored the role of ""ideation diversity"" in AI research agents. Ideation diversity refers to the ability of an AI agent to generate a wide range of ideas and approaches to solve a problem.

The study found that AI agents that can generate more diverse ideas tend to perform better. The researchers analyzed data from a well-known benchmark test and found that agents with higher ideation diversity achieved better results. They also conducted a controlled experiment that confirmed the importance of ideation diversity. The study's findings suggest that developing AI agents that can think creatively and generate diverse ideas is crucial for accelerating scientific progress.

In simple terms, the study shows that AI research agents need to be able to think outside the box and come up with a variety of solutions to be effective. This research has implications for the development of future AI assistants that can help scientists make new discoveries and advance research."
cs.AI,CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking,"3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",https://arxiv.org/abs/2511.15580v1,2025-11-19T16:12:24Z,"Sifan Zhou, Yichao Cao, Jiahao Nie, Yuqian Fu, Ziyu Zhao, Xiaobo Lu, Shuo Wang","**Advancing 3D Object Tracking for Autonomous Vehicles**

Researchers have developed a new framework called CompTrack, designed to improve 3D single object tracking (SOT) in LiDAR point clouds, a crucial task for autonomous driving. The challenge lies in the sparse and noisy nature of point cloud data, which can lead to inaccurate and inefficient tracking.

CompTrack addresses these issues by:

1. **Filtering out background noise**: The Spatial Foreground Predictor (SFP) module uses information entropy to remove irrelevant background data, reducing spatial redundancy.
2. **Compressing foreground data**: The Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module eliminates informational redundancy within the foreground, using a low-rank approximation technique to adaptively compress the data into a more compact and informative form.

The results are impressive: CompTrack achieves top-performing tracking performance on several large datasets (KITTI, nuScenes, and Waymo) while running at a rapid 90 frames per second on a single GPU. This breakthrough has significant implications for the development of more efficient and accurate 3D object tracking systems, which can enable safer and more reliable autonomous driving."
cs.AI,HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning,"Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",https://arxiv.org/abs/2511.15574v1,2025-11-19T16:06:06Z,"Qihao Yang, Xuelin Wang, Jiale Chen, Xuelian Dong, Yuxin Hao, Tianyong Hao","**Unlocking the Secrets of Language Learning: A New Benchmark for Large Language Models**

Imagine being able to understand how humans learn a second language and applying that knowledge to improve artificial intelligence. Researchers have made a significant step towards achieving this goal with the development of HSKBenchmark, a new tool for evaluating and improving large language models (LLMs) in Chinese second language acquisition.

**What is HSKBenchmark?**

HSKBenchmark is a comprehensive benchmark that assesses the language skills of LLMs in Chinese, covering levels 3 to 6 of the HSK (Chinese proficiency test). It includes a massive dataset of authentic textbooks, instruction samples, and test topics, as well as a sophisticated evaluation system. This benchmark allows researchers to model and assess the language learning process of LLMs in a systematic and reliable way.

**How does it work?**

The researchers introduced a curriculum-tuning framework that trains LLMs from beginner to advanced levels, simulating human learning trajectories. They also created an evaluation system that examines grammar coverage, writing errors, and language complexity. To test the effectiveness of HSKBenchmark, they fine-tuned a LLM, called HSKAgent, on a dataset of 10,000 learner compositions.

**What are the results?**

The results show that HSKBenchmark can effectively model Chinese second language acquisition and serve as a reliable benchmark for dynamic writing assessment in LLMs. The fine-tuned LLMs demonstrated writing performance on par with advanced human learners and exhibited human-like acquisition characteristics.

**Why is this important?**

The development of HSKBenchmark and HSKAgent has the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. The code and data are publicly available, making it a valuable resource for researchers and developers. This work brings us closer to understanding how humans learn languages and how to improve AI systems to better mimic human language abilities."
cs.AI,B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index,"Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.",https://arxiv.org/abs/2511.15557v1,2025-11-19T15:50:28Z,"Selim Furkan Tekin, Rajesh Bordawekar","**Breakthrough in AI Data Processing: Faster and More Efficient Nearest-Neighbor Index**

Imagine you're searching for similar images or products online. Current AI systems use complex algorithms to find the closest matches, but they have limitations. A new research paper introduces B+ANN, a faster and more efficient way to process and store large amounts of data, enabling AI systems to find similar items more quickly and accurately.

**The Problem with Current Systems**

Current systems, like those using HNSW algorithm, have several drawbacks:

* They require a lot of memory, making them slow and expensive.
* They access data randomly, leading to inefficient processing.
* They can only handle certain types of queries, like finding similar items.

**Introducing B+ANN**

B+ANN is a new disk-based index that addresses these issues. It:

* Groups similar data into blocks, making it easier to process.
* Uses a B+ tree data structure to store and retrieve data efficiently.
* Enables fast and efficient searching, both in-memory and on disk.

**Benefits of B+ANN**

The B+ANN index has several advantages:

* **Faster performance**: It can handle more queries per second than current systems.
* **Improved accuracy**: It provides better recall values, meaning it finds more accurate matches.
* **Reduced memory consumption**: It uses significantly less memory than current systems.
* **Support for dissimilarity queries**: It can handle queries that find non-similar items, which is not possible with current systems.

**Impact**

The B+ANN index has the potential to revolutionize AI data processing, enabling faster and more efficient processing of large amounts of data. This can lead to breakthroughs in various applications, such as image and video search, recommendation systems, and natural language processing."
cs.AI,Multimodal Evaluation of Russian-language Architectures,"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",https://arxiv.org/abs/2511.15552v1,2025-11-19T15:43:53Z,"Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova","Here's a summary of the research paper for a general audience:

**Advancing Multimodal AI for Russian Language**

Researchers have developed a new evaluation framework, called Mera Multi, to assess the capabilities of large language models that can process multiple types of data, such as text, images, audio, and video. This framework is specifically designed for the Russian language, which previously lacked a comprehensive benchmark for evaluating these types of models.

The researchers created 18 new tasks to test the abilities of these models, including understanding text, images, audio, and video. They also developed a universal system to categorize the abilities of these models and created new datasets that reflect Russian culture and language.

The study provides baseline results for both commercial and open-source models, and introduces methods to prevent cheating and ensure the integrity of the evaluation process. While the current focus is on Russian, the methodology developed in this study can be applied to other languages, particularly those in the Slavic language family.

**In simple terms:** Imagine you're chatting with a smart computer that can understand text, pictures, audio, and video. Researchers created a new tool to test how well these computers can understand and process different types of data in Russian. This tool can help improve the intelligence and safety of these computers, and can also be used for other languages in the future."
cs.AI,Exploring the use of AI authors and reviewers at Agents4Science,"There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.",https://arxiv.org/abs/2511.15534v1,2025-11-19T15:32:07Z,"Federico Bianchi, Owen Queen, Nitya Thakkar, Eric Sun, James Zou","Here's a summary of the research paper for a general audience:

**Can AI Agents Help with Scientific Research?**

Imagine a future where artificial intelligence (AI) plays a bigger role in scientific research. Could AI agents write papers and review the work of other researchers? A recent experiment called Agents4Science explored this idea by hosting a conference where AI agents were the main authors and reviewers of scientific papers, with humans working alongside them.

The goal of the experiment was to see how well AI agents can perform as scientists and reviewers. The results provide valuable insights into the potential benefits and challenges of using AI in scientific research. The findings could have significant implications for how humans and AI agents work together to advance scientific knowledge.

This experiment is an important step towards understanding the role that AI can play in helping scientists make new discoveries and pushing the boundaries of human knowledge."
cs.AI,Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies,"Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.",https://arxiv.org/abs/2511.15520v1,2025-11-19T15:13:08Z,"Gabriel Lauzier, Alexandre Girard, François Ferland","**Advancing Robotics: A New Approach to Stable Control**

Imagine a robot arm trying to pick up a fragile object while being jostled by external forces. To succeed, the arm needs to make quick and precise movements. Researchers have developed a technique called Diffusion Policy, which helps robots perform tasks like this by modeling the many possible actions they can take. However, this technique can be slow and computationally intensive, making it difficult to use in real-time applications.

To overcome this limitation, this study explores a new approach that allows the robot to start moving while still refining its actions. This approach couples the robot's dynamics (how it moves) with the refinement process, which is a complex mathematical calculation. The researchers derived theoretical bounds on the stability of this coupled system, essentially creating a framework to predict whether a controller (the brain of the robot) will be stable based on the variability of the demonstrations (how the robot was trained).

The study's findings have significant implications for imitation learning, a type of machine learning where robots learn by observing and imitating human behavior. By providing a metric for stability, this research enables the development of faster and more efficient imitation learning methods. This breakthrough could lead to more advanced and capable robots that can perform complex tasks in real-time, such as assembly line work, search and rescue operations, or even surgery.

**In simpler terms:** This research improves the control of robots by allowing them to move and refine their actions simultaneously. It provides a framework to ensure that the robot's controller remains stable, enabling faster and more efficient learning. This advancement has the potential to enhance various robotic applications, from manufacturing to healthcare."
cs.AI,Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels,"Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",https://arxiv.org/abs/2511.15496v1,2025-11-19T14:52:51Z,"Maria Pilligua, David Serrano-Lozano, Pai Peng, Ramon Baldrich, Michael S. Brown, Javier Vazquez-Corral","**Improving Low-Light Photos: A New Benchmark for Enhancement Techniques**

Taking good photos in low-light conditions can be tough. The images often turn out dark, noisy, and lacking in color. Researchers have been working on techniques to enhance low-light images, but most of these methods are only trained on images taken in a single low-light condition. This makes it hard to know how well they'll perform in different lighting situations.

To address this issue, a team of researchers has created a new dataset called Multi-Illumination Low-Light (MILL). This dataset contains images taken at various light intensities, allowing for a more comprehensive evaluation of enhancement algorithms.

The researchers tested several state-of-the-art image enhancement methods using the MILL dataset. They found that these methods perform differently under varying lighting conditions. Some methods worked well in very low light, while others performed better in slightly brighter conditions.

The good news is that the researchers were able to use the MILL dataset to improve these enhancement techniques. By making some modifications, they achieved significant improvements in image quality - up to 10 dB for DSLR cameras and 2 dB for smartphone cameras. These improvements can lead to better low-light photos with reduced noise and more accurate colors.

This research has the potential to benefit various applications, including photography, surveillance, and robotics, where low-light imaging is common. By developing more robust image enhancement techniques, we can improve the quality of images taken in a wide range of lighting conditions."
cs.AI,RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection,"This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",https://arxiv.org/abs/2511.15476v1,2025-11-19T14:32:34Z,"Rashid Iqbal, Saddam Hussain Khan","**Breakthrough in Monkeypox Detection: AI Framework Achieves High Accuracy**

Researchers have developed a new artificial intelligence (AI) framework that can accurately detect monkeypox, a viral disease, from images. The framework, called RS-CA-HSICT, combines the strengths of two powerful AI techniques: Convolutional Neural Networks (CNNs) and Transformers.

The RS-CA-HSICT framework works by:

1. **Extracting detailed features**: It uses CNNs to extract detailed features from images, such as texture and structure.
2. **Capturing long-range dependencies**: It uses Transformers to capture long-range dependencies and relationships between different parts of the image.
3. **Enhancing feature space**: It combines the features from both techniques to create a more comprehensive feature space.

The researchers tested their framework on two datasets and achieved impressive results:

* **Classification accuracy**: 98.30% (i.e., the framework correctly classified images 98.30% of the time)
* **F1-score**: 98.13% (i.e., the framework balanced precision and recall, indicating high accuracy)

These results outperform existing AI models, suggesting that the RS-CA-HSICT framework has the potential to become a valuable tool for monkeypox detection. The framework's high accuracy and efficiency make it a promising solution for medical professionals and researchers working to combat the spread of monkeypox."
cs.AI,Insights from the ICLR Peer Review and Rebuttal Process,"Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",https://arxiv.org/abs/2511.15462v1,2025-11-19T14:21:52Z,"Amir Hossein Kargaran, Nafiseh Nikeghbal, Jing Yang, Nedjma Ousidhoum","Here's a summary of the research paper for a general audience:

**Understanding the Peer Review Process in Scientific Publishing**

The peer review process is a crucial step in scientific publishing, where experts review and provide feedback on research papers before they're accepted for publication. With the increasing number of submissions, it's essential to understand how this process works to make it more efficient and effective.

Researchers analyzed the peer review process for the International Conference on Learning Representations (ICLR), a premier machine learning conference. They looked at data from 2024 and 2025, focusing on how reviewers' scores changed before and after authors responded to feedback.

**Key Findings:**

* The initial scores given by reviewers and the ratings of co-reviewers have the most significant impact on the final scores.
* Authors' responses to feedback (rebuttals) play a valuable role in improving outcomes for papers that are on the border of being accepted or rejected.
* Thoughtful author responses can meaningfully shift reviewer perspectives.

**What does this mean?**

This study provides insights into the peer review process, highlighting what works and what doesn't. The findings can help authors develop effective strategies for responding to feedback and improve the overall quality of published papers. Additionally, the study's results can inform the design of fairer and more efficient review processes, ultimately benefiting the scientific community."
cs.AI,Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining,"As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",https://arxiv.org/abs/2511.15456v1,2025-11-19T14:15:23Z,"Qian'ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan","**Understanding User Intent in Decentralized Finance (DeFi)**

Decentralized Finance (DeFi) is a rapidly growing field that allows for financial transactions to take place on blockchain networks. However, understanding the intentions behind these transactions can be difficult due to the complex interactions between smart contracts, on-chain and off-chain data, and unclear transaction logs.

To address this challenge, researchers have proposed a new framework called Transaction Intent Mining (TIM). This framework uses a combination of artificial intelligence (AI) and machine learning techniques to analyze DeFi transactions and infer the user's intent behind them.

The TIM framework consists of multiple AI agents that work together to analyze transactions from different perspectives. These agents use a large language model to understand the semantic meaning of the transactions and identify the user's intent. The framework also includes a mechanism to evaluate and verify the accuracy of the inferred intents.

**Key Findings:**

* The TIM framework significantly outperforms existing machine learning models and single AI agent baselines in inferring user intent.
* The framework provides a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

**Implications:**

* This research has the potential to improve the transparency and accountability of DeFi transactions, enabling more informed decision-making and risk management.
* The TIM framework can be used to develop more effective tools for monitoring and analyzing DeFi activity, which can help to prevent illicit activities and promote regulatory compliance."
cs.AI,TSFM in-context learning for time-series classification of bearing-health status,"This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",https://arxiv.org/abs/2511.15447v1,2025-11-19T14:01:12Z,"Michel Tokic, Slobodan Djukanović, Anja von Beuningen, Cheng Feng","Here's a summary of the research paper for a general audience:

**Predicting Machine Health with AI: A Breakthrough in Time-Series Classification**

Researchers have made a significant advancement in using artificial intelligence (AI) to predict the health status of machines, specifically bearings in motors. They developed a new method called in-context learning, which enables a pre-trained AI model to classify data without needing to be re-trained.

The researchers applied this method to vibration data from a motor bearing and were able to accurately predict its health status. The innovative approach uses a type of AI model called a time-series foundation model (TSFM), which can learn patterns in data over time.

The breakthrough is that this method can classify new, unseen data without requiring the AI model to be fine-tuned. This is achieved by providing the model with examples of data and their corresponding labels, allowing it to learn and make predictions on its own.

This development has significant implications for predictive maintenance in industries, enabling more efficient and effective monitoring of machine health. The researchers' approach has the potential to be applied to a wide range of machines and operational conditions, paving the way for more widespread adoption of AI-driven maintenance systems."
cs.AI,HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation,"Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",https://arxiv.org/abs/2511.15435v1,2025-11-19T13:45:24Z,"Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai","**New Research Reveals Vulnerability in AI Systems**

Imagine asking a smart speaker a question, and instead of getting a helpful answer, you receive misinformation. This could happen due to a new type of cyber attack called HV-Attack, which targets AI systems that use both text and images to generate responses.

Researchers have found that by adding tiny, almost imperceptible changes to an image, they can trick these AI systems into providing incorrect information. This is a concern because these systems are being used in various applications, such as virtual assistants and chatbots.

The HV-Attack works by disrupting the alignment between the image and text inputs to the AI system, causing it to retrieve irrelevant information from its database. This, in turn, leads to incorrect responses being generated.

The researchers tested their attack on two popular datasets and used state-of-the-art AI models, including CLIP-based retrievers and large language models like BLIP-2 and LLaVA. The results showed that the HV-Attack was effective in compromising the performance of these AI systems.

**Key Takeaways:**

* A new type of cyber attack, HV-Attack, targets AI systems that use both text and images to generate responses.
* The attack involves adding tiny changes to an image to trick the AI system into providing incorrect information.
* The HV-Attack has been shown to be effective on popular AI models and datasets.

**Implications:**

* The HV-Attack highlights the need for more robust security measures to protect AI systems from cyber threats.
* It also underscores the importance of testing and evaluating AI systems for vulnerabilities before deploying them in real-world applications."
cs.AI,"Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs","Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.",https://arxiv.org/abs/2511.15434v1,2025-11-19T13:45:07Z,"Georg Goldenits, Philip Koenig, Sebastian Raubitzek, Andreas Ekelhart","**Detecting Phishing Websites with Small AI Models: A Cost-Effective Solution**

Phishing websites are a major cybersecurity threat that can cause significant financial and organizational harm. Traditional methods for detecting these websites often require complex and costly infrastructure. Recently, large AI models have shown promise in detecting phishing websites, but they are expensive to use and rely on external providers.

This study explores the use of small AI models, which can be deployed on local infrastructure, giving organizations more control over their data and operations. The researchers tested 15 small language models, ranging from 1 billion to 70 billion parameters, to see how well they could detect phishing websites using only the website's raw HTML code.

The results show that while small AI models may not perform as well as large AI models, they can still provide a viable and scalable alternative to external services. The study highlights the trade-offs between detection performance and resource consumption, demonstrating that small AI models can offer a cost-effective solution for detecting phishing websites.

**Key Takeaways:**

* Small AI models can detect phishing websites using only raw HTML code.
* They offer a cost-effective alternative to large AI models and external services.
* Small AI models can be deployed on local infrastructure, giving organizations more control over data and operations.
* While they may not perform as well as large AI models, small AI models can still provide a viable solution for detecting phishing websites.

This research lays the foundation for future studies on adapting and deploying small AI models for phishing detection, aiming to balance security effectiveness and economic practicality."
cs.AI,Towards Understanding Layer Contributions in Tabular In-Context Learning Models,"Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the ""layers as painters"" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",https://arxiv.org/abs/2511.15432v1,2025-11-19T13:39:30Z,"Amir Rezaei Balef, Mykhailo Koshil, Katharina Eggensperger","**Unlocking the Secrets of Tabular In-Context Learning Models**

Imagine you're trying to predict a person's income based on their age, education, and job title. Tabular in-context learning (ICL) models are a type of artificial intelligence (AI) designed to make such predictions using tables of data. But how do these models work, and which parts are most important?

Researchers recently studied two popular tabular ICL models, TabPFN and TabICL, to understand how they process information. They found that these models have layers, similar to the human brain's neural networks, that work together to make predictions. However, not all layers are equally important.

The study revealed that only some layers in these models share a common ""language"" or way of representing data. This means that some layers might be redundant, or not essential for making accurate predictions. This discovery has exciting implications:

* **Model compression**: By removing redundant layers, these models can be made smaller and more efficient, which could lead to faster and more cost-effective predictions.
* **Improved interpretability**: Understanding which layers are most important can help us better grasp how these models make decisions, making them more transparent and trustworthy.

These findings also highlight differences between tabular ICL models and large language models (LLMs), which are used for tasks like language translation and text generation. By shedding light on the inner workings of tabular ICL models, this research paves the way for future improvements and applications in areas like data analysis and decision-making."
cs.AI,Building Robust and Scalable Multilingual ASR for Indian Languages,"This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",https://arxiv.org/abs/2511.15418v1,2025-11-19T13:17:16Z,"Arjun Gangwar, Kaousheik Jayakumar, S. Umesh","**Breakthrough in Multilingual Speech Recognition for Indian Languages**

Researchers at the Indian Institute of Technology Madras have made significant progress in developing robust and scalable speech recognition systems for Indian languages. Their work, presented at the ASRU MADASR 2.0 challenge, focused on creating systems that can accurately identify and transcribe speech in 8 languages with 33 dialects.

The team developed a novel approach using a Multi-Decoder architecture, which improved performance in predicting language and dialect. They achieved impressive results, outperforming the baseline in 3 languages and achieving the highest accuracy in language and dialect identification among all participating teams.

This advancement has the potential to improve speech recognition technology for Indian languages, enabling more effective communication and access to information for millions of people. The research demonstrates the possibility of building robust and scalable multilingual speech recognition systems, paving the way for wider applications in areas such as language translation, voice assistants, and education."
cs.AI,RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer,"We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.",https://arxiv.org/abs/2511.15414v1,2025-11-19T13:14:10Z,"Mingyang Feng, Shaoyuan Li, Xiang Yin","**Breakthrough in Robotics Motion Planning: Introducing RRT*former**

Imagine a self-driving car navigating through a busy city or a robot arm picking up objects on a cluttered factory floor. These tasks require precise motion planning to avoid obstacles and find the most efficient path. Researchers have made a significant advancement in this field with the development of RRT*former, a novel motion planning algorithm that leverages environmental information and past experiences to optimize path planning.

Traditional motion planning algorithms often rely on random sampling, which can be inefficient and lead to suboptimal solutions. RRT*former addresses this limitation by integrating a powerful artificial intelligence (AI) model, called a Transformer, with a popular motion planning algorithm called RRT*. The Transformer analyzes the environment and previous samples to guide the sampling process, resulting in more informed and efficient exploration of the space.

**Key Benefits:**

* **Improved Optimality**: RRT*former finds better paths that are closer to the optimal solution.
* **Increased Efficiency**: The algorithm requires fewer samples to achieve better results, making it faster and more efficient.

The RRT*former algorithm has the potential to transform various applications, including robotics, autonomous vehicles, and industrial automation. The code for the implementation is open-source and available on GitHub, allowing researchers and developers to build upon this innovation."
cs.AI,NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework,"Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",https://arxiv.org/abs/2511.15408v1,2025-11-19T13:05:25Z,"Shanlin Zhou, Xinpeng Wang, Jianxun Lian, Zhenghao Liu, Laks V. S. Lakshmanan, Xiaoyuan Yi, Yongtao Hao","Here's a summary of the research paper for a general audience:

**The Challenge of Creative Text Generation**

Imagine you're trying to come up with a unique and meaningful name for a baby. You might have specific ideas in mind, such as a certain length, sound, or cultural significance. But how do you generate a name that meets all these requirements while also being creative and aesthetically pleasing?

**A New Approach: NAMeGEn**

Researchers have developed a new framework called NAMeGEn, which uses a team of virtual agents to work together to generate creative names that meet specific requirements. This approach addresses two main challenges in creative text generation: (1) satisfying multiple, personalized requirements, and (2) providing meaningful explanations for the generated text.

**How it Works**

NAMeGEn uses a combination of natural language processing and machine learning to extract objectives, generate names, and evaluate them. The framework was tested on the task of generating Chinese baby names, and it outperformed six other methods in generating creative and meaningful names.

**The Benefits**

The NAMeGEn framework has several benefits, including:

* Generating creative and personalized text that meets specific requirements
* Providing meaningful explanations for the generated text
* Improving the aesthetic appeal of the generated text

**Potential Applications**

The NAMeGEn framework has potential applications in various areas, such as:

* Advertising and marketing: generating creative and personalized ads or product descriptions
* Storytelling: generating engaging and meaningful stories
* Content creation: generating high-quality content that meets specific requirements

Overall, the NAMeGEn framework represents a significant advancement in creative text generation, and its applications could be vast and varied."
cs.AI,IPR-1: Interactive Physical Reasoner,"Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.",https://arxiv.org/abs/2511.15407v1,2025-11-19T13:04:44Z,"Mingyu Zhang, Lifeng Zhuo, Tianxi Tan, Guocan Xie, Xian Nie, Yan Li, Renjie Zhao, Zizhu He, Ziyu Wang, Jiting Cai, Yong-Lu Li","**Breakthrough in Artificial Intelligence: Teaching AI to Reason Like Humans**

Imagine a machine that can learn and reason like humans do. Researchers have made a significant step towards achieving this goal by developing a new AI system called Interactive Physical Reasoner (IPR-1). IPR-1 enables machines to learn from interacting with virtual environments and improve their reasoning abilities over time.

The researchers tested IPR-1 on over 1,000 diverse games that require physical and causal understanding, such as predicting the outcome of actions and understanding cause-and-effect relationships. They evaluated the system's performance on three levels: Survival, Curiosity, and Utility, which range from basic intuition to goal-driven reasoning.

The results are impressive: IPR-1 outperformed existing AI systems and even matched the performance of GPT-5, a state-of-the-art language model, in overall reasoning. Moreover, IPR-1 excelled in Curiosity, a key aspect of human-like reasoning.

The key innovation behind IPR-1 is its ability to combine world-model rollouts (simulating the environment) with a vision-language model (understanding visual and textual inputs). This allows the system to analyze physics and causality, and make predictions about the outcome of actions.

The study's findings suggest that physics-centric interaction is a promising approach to improving physical reasoning in AI systems. As IPR-1 continues to learn and interact with more environments, its performance is expected to improve, enabling it to transfer its knowledge to new, unseen situations.

This breakthrough has significant implications for the development of more intelligent and human-like machines that can learn, reason, and interact with the world in a more meaningful way."
cs.AI,DEPO: Dual-Efficiency Preference Optimization for LLM Agents,"Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",https://arxiv.org/abs/2511.15392v1,2025-11-19T12:38:43Z,"Sirui Chen, Mengshi Zhao, Lei Xu, Yuying Zhao, Beier Zhu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu","**Improving the Efficiency of AI Agents**

Researchers have made significant progress in developing large language models (LLMs) that can reason and make decisions like humans. However, these models often require a long chain of thought to arrive at a solution, which can be inefficient in real-world scenarios. To address this issue, the researchers introduced a new concept called ""dual-efficiency,"" which measures the efficiency of AI agents in two ways: 

1. **Step-level efficiency**: How concise are the agent's responses?
2. **Trajectory-level efficiency**: How many steps does the agent take to complete a task?

Using this definition, the researchers developed a method called DEPO, which optimizes both efficiencies simultaneously. DEPO encourages AI agents to provide succinct responses and take fewer steps to complete tasks.

**Key Findings**

* DEPO reduced token usage by up to 60.9% and steps by up to 26.9% in two test environments (WebShop and BabyAI).
* Despite the efficiency gains, DEPO achieved up to a 29.3% improvement in performance.
* The method also generalized well to three out-of-domain math benchmarks, retaining its efficiency gains even when trained on limited data (only 25% of the available data).

Overall, the DEPO method offers a promising approach to improving the efficiency of AI agents, enabling them to interact more effectively and efficiently in real-world scenarios."
cs.CL,HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning,"Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",https://arxiv.org/abs/2511.15574v1,2025-11-19T16:06:06Z,"Qihao Yang, Xuelin Wang, Jiale Chen, Xuelian Dong, Yuxin Hao, Tianyong Hao","**Unlocking Language Learning: A New Benchmark for Large Language Models**

Imagine being able to teach a computer to learn a language like a human. Researchers have made a significant step towards achieving this goal by creating a new benchmark called HSKBenchmark. This benchmark is designed to test how well large language models (LLMs) can learn Chinese as a second language, mimicking the way humans do.

The researchers created a comprehensive dataset of textbooks, instruction samples, and test topics, covering levels 3 to 6 of the Chinese language proficiency test (HSK). They also developed a curriculum-tuning framework that trains LLMs from beginner to advanced levels, simulating human learning trajectories.

The study evaluated LLMs on various aspects, including grammar, writing errors, and language complexity. The results showed that the fine-tuned LLMs performed on par with advanced human learners and exhibited human-like language acquisition characteristics.

The HSKBenchmark, along with a fine-tuned LLM called HSKAgent, provides a reliable tool for assessing the writing abilities of LLMs. This research has the potential to improve the interpretability of LLMs and pave the way for future studies on language acquisition modeling.

The code and data are publicly available, making it a valuable resource for researchers and developers interested in language learning and AI. This breakthrough could lead to more sophisticated language models that can learn and adapt like humans, with applications in language education, translation, and more."
cs.CL,Computer-Use Agents as Judges for Generative User Interface,"Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",https://arxiv.org/abs/2511.15567v1,2025-11-19T16:00:02Z,"Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou","**Imagine a Future Where Computers Design Their Own Interfaces**

Researchers have made a significant breakthrough in creating computer systems that can design and evaluate their own user interfaces. Currently, computer interfaces are designed with humans in mind, making it difficult for computer agents to efficiently interact with them. The team developed a new framework that uses a computer agent as a ""judge"" to help design and refine interfaces, working together with a coding model.

**The Problem: Human-Centered Interfaces**

Graphical User Interfaces (GUIs) are designed primarily for humans, prioritizing aesthetics and usability. However, this makes it challenging for computer agents to interact with them efficiently. The researchers aimed to address this issue by exploring the possibility of using computer agents as judges to assist in automatic GUI design.

**The Solution: Coder-CUA Collaboration**

The researchers introduced AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. They also developed a verifier that programmatically checks whether each task is executable within its environment. The proposed Coder-CUA in Collaboration framework involves a coding model (Coder) that generates and revises interfaces, while a computer agent (CUA) evaluates and provides feedback on their functionality.

**Key Findings**

* The Coder-CUA framework successfully designed and refined interfaces that were efficient and reliable for computer agents to use.
* The computer agent's feedback was used to create a dashboard that provides guidance for iterative redesign.
* The framework shifts interface design toward agent-native efficiency and reliability.

**The Impact: A New Era of Human-Computer Interaction**

This innovation has the potential to revolutionize the way computers interact with digital environments. By enabling computer agents to design and evaluate their own interfaces, we can create more efficient and reliable systems. This could lead to breakthroughs in areas such as automation, artificial intelligence, and the Internet of Things (IoT).

**The Future: Active Participation in Digital Environments**

The researchers' work takes a significant step toward shifting agents from passive use toward active participation in digital environments. With the code and dataset available, this research has the potential to inspire further innovation and applications in the field."
cs.CL,Multimodal Evaluation of Russian-language Architectures,"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",https://arxiv.org/abs/2511.15552v1,2025-11-19T15:43:53Z,"Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova","Here's a summary of the research paper for a general audience:

**Understanding Multimodal Language Models**

Imagine a computer that can understand and process not just text, but also images, audio, and video. These ""multimodal"" language models are rapidly improving, but we still don't fully understand their capabilities, limitations, and potential risks.

**A New Benchmark for Russian Language Models**

To address this gap, researchers have created a new evaluation framework called Mera Multi, specifically designed for Russian-language models. This framework includes 18 new tasks that test a model's ability to understand and process different types of data, such as text, images, audio, and video.

**What's New and Important**

The researchers have made several important contributions:

* They've created a universal system to categorize multimodal abilities, making it easier to compare different models.
* They've developed 18 new datasets that are tailored to Russian culture and language, which can be used to evaluate and improve multimodal models.
* They've tested both commercial and open-source models on these tasks, providing a baseline for future research.
* They've also developed methods to prevent ""benchmark leakage,"" which can occur when models are trained on the same data used to evaluate them.

**Implications and Future Directions**

While this research focuses on Russian-language models, the methodology and framework can be applied to other languages, particularly those in the Slavic language family. This work has the potential to improve our understanding of multimodal language models and their applications, and could lead to more accurate and informative models in the future."
cs.CL,Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis,"The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.",https://arxiv.org/abs/2511.15512v1,2025-11-19T15:06:08Z,"Yves Pauli, Jan-Bernard Marsman, Finn Rabe, Victoria Edkins, Roya Hüppi, Silvia Ciampelli, Akhil Ratan Misra, Nils Lang, Wolfram Hinzen, Iris Sommer, Philipp Homan","Here's a summary of the research paper for a general audience:

**Standardizing Language Analysis: A New Framework for Reliable Results**

The way researchers analyze language data has become increasingly complex with the help of artificial intelligence. However, this progress has also led to challenges, such as inconsistent methods for organizing and sharing data, and a lack of transparency in the analysis process. To address these issues, researchers have developed a new framework that standardizes the way language data is handled and analyzed.

The framework consists of two main components:

1. **LPDS (Language Processing Data Structure)**: a standardized way of organizing and naming language data files, making it easier to share and reuse data.
2. **pelican nlp**: a software package that automates the analysis process, from data cleaning to extracting complex features, such as meaning and sound patterns. The entire process can be specified in a single file, making it reproducible and transparent.

Together, LPDS and pelican nlp provide a complete pipeline for analyzing language data, ensuring that results are reliable, transparent, and reproducible. This framework has the potential to improve the quality and consistency of language research, enabling researchers to build on each other's work more easily."
cs.CL,CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search,"Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",https://arxiv.org/abs/2511.15443v1,2025-11-19T13:57:40Z,"Ao Xie, Jiahui Chen, Quanzhi Zhu, Xiaoze Jiang, Zhiheng Qin, Enyun Yu, Han Li","**Improving Short-Video Search with CroPS**

Imagine you're searching for a funny video on a short-video platform, but the results aren't quite what you're looking for. This can happen because the search system is biased towards showing you videos that you've interacted with before, rather than introducing you to new and relevant content. This is known as the ""filter bubble effect.""

Researchers have developed a new method called CroPS (Cross-Perspective Positive Samples) to improve short-video search results. CroPS aims to break the filter bubble effect by introducing diverse and meaningful examples from multiple perspectives. This is achieved by combining three types of data:

1. **User behavior**: How users reformulate their search queries.
2. **Recommendation data**: What users engage with in their recommended video streams.
3. **World knowledge**: Information generated by large language models.

To make the most of this data, the researchers developed a new strategy called Hierarchical Label Assignment (HLA) and a corresponding loss function called H-InfoNCE. These enable the model to optimize its performance in a fine-grained and relevance-aware way.

**The Results**

Extensive experiments on a large commercial short-video search platform, Kuaishou Search, showed that CroPS significantly outperforms existing methods, both in offline tests and live A/B tests. The results include:

* Superior retrieval performance
* Reduced query reformulation rates

**Real-World Impact**

CroPS is now fully deployed on Kuaishou Search, serving hundreds of millions of users daily. This technology has the potential to improve the search experience for users on short-video platforms, making it easier to discover new and relevant content."
cs.CL,LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering,"Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.",https://arxiv.org/abs/2511.15424v1,2025-11-19T13:22:08Z,"Yuanjie Zhu, Liangwei Yang, Ke Xu, Weizhi Zhang, Zihe Song, Jindong Wang, Philip S. Yu","**Unlocking the Power of Large Language Models for Text Clustering**

Large Language Models (LLMs) have shown great promise in understanding and organizing text data, but they've been limited in their ability to group similar texts together (known as text clustering). The main challenges are that LLMs don't have a built-in way to remember and refine their understanding over time, and it's hard to determine the right number of groups.

To overcome these limitations, researchers have developed a new framework called LLM-MemCluster. This framework allows LLMs to perform text clustering in a more natural and efficient way, without needing additional complex modules or fine-tuning. The key innovations are:

1. **Dynamic Memory**: a built-in memory system that enables LLMs to keep track of their understanding and refine it over time.
2. **Dual-Prompt Strategy**: a clever approach that helps LLMs determine the optimal number of clusters.

The results are impressive: LLM-MemCluster outperforms existing methods on several benchmark datasets, providing a more effective, interpretable, and streamlined approach to text clustering. This breakthrough has the potential to enable more accurate and efficient text analysis in various applications, from information retrieval to natural language processing."
cs.CL,Building Robust and Scalable Multilingual ASR for Indian Languages,"This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",https://arxiv.org/abs/2511.15418v1,2025-11-19T13:17:16Z,"Arjun Gangwar, Kaousheik Jayakumar, S. Umesh","Here's a summary of the research paper for a general audience:

**Improving Speech Recognition for Indian Languages**

Researchers at the Indian Institute of Technology Madras have made significant progress in developing speech recognition systems that can accurately understand and transcribe spoken languages in India. India has a diverse linguistic landscape with many languages and dialects, making it challenging to build robust speech recognition systems.

The researchers developed a new approach to build multilingual speech recognition systems that can recognize and transcribe speech in 8 languages and 33 dialects. They created a novel training method that uses a ""multi-decoder"" architecture, which allows the system to learn phonemic representations of languages (i.e., the sounds that make up words) and then convert them back to written text.

Their system outperformed existing baseline systems in 3 languages and achieved the highest accuracy in identifying languages and dialects among all participating teams. This breakthrough has the potential to improve speech recognition technology for Indian languages, enabling more effective communication and accessibility for people who speak these languages.

The implications of this research are significant, as it could lead to better voice assistants, transcription services, and language learning tools for Indian languages. The researchers' approach could also be applied to other languages and dialects, making speech recognition technology more inclusive and accessible globally."
cs.CL,NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework,"Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",https://arxiv.org/abs/2511.15408v1,2025-11-19T13:05:25Z,"Shanlin Zhou, Xinpeng Wang, Jianxun Lian, Zhenghao Liu, Laks V. S. Lakshmanan, Xiaoyuan Yi, Yongtao Hao","Here's a summary of the research paper for a general audience:

**Creating Creative and Personalized Names with AI**

Imagine you're looking for a unique and meaningful name for your baby. You might have specific ideas in mind, such as a certain length, sound, or cultural significance. But how can you generate a name that meets all your requirements and also sounds beautiful?

Researchers have been working on developing artificial intelligence (AI) models that can generate creative and personalized text, such as names, stories, and advertisements. However, these models often struggle to meet multiple requirements at once, and they may not provide clear explanations for their suggestions.

To address this challenge, a team of researchers proposed a new framework called NAMeGEn. This framework uses multiple AI agents to work together to generate creative and personalized names that meet specific requirements. The framework was tested on the task of generating Chinese baby names, which requires adherence to explicit constraints, such as length and meaning, while also offering aesthetic explanations.

The researchers found that NAMeGEn was able to generate creative and personalized names that met diverse requirements and provided meaningful explanations. The framework outperformed six other baseline methods and did not require any training data. This research has the potential to improve the generation of creative and personalized text, with applications in areas such as advertising, storytelling, and more.

**Key Takeaways:**

* NAMeGEn is a new AI framework that generates creative and personalized names that meet specific requirements.
* The framework uses multiple AI agents to work together to generate names and provide meaningful explanations.
* NAMeGEn outperformed other baseline methods and did not require any training data.
* This research has potential applications in areas such as advertising, storytelling, and more."
cs.CL,DEPO: Dual-Efficiency Preference Optimization for LLM Agents,"Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",https://arxiv.org/abs/2511.15392v1,2025-11-19T12:38:43Z,"Sirui Chen, Mengshi Zhao, Lei Xu, Yuying Zhao, Beier Zhu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu","Here's a summary of the research paper for a general audience:

**Making AI Agents More Efficient**

Large language models (LLMs) have become incredibly good at reasoning and making decisions, but they often take a long time to do so. This can be a problem in real-world situations where quick and efficient interactions are needed. Researchers have identified two types of efficiency that are important for AI agents: **step-level efficiency** (how concise the agent's responses are) and **trajectory-level efficiency** (how many steps it takes to complete a task).

To improve both types of efficiency, the researchers developed a new method called **DEPO (Dual-Efficiency Preference Optimization)**. DEPO encourages AI agents to respond succinctly and take fewer steps to complete tasks. When tested on several tasks, DEPO was able to reduce the number of tokens (or words) used by up to 60.9% and the number of steps by up to 26.9%, while also improving performance by up to 29.3%. The method was also able to generalize to new tasks and work well even when trained on limited data. Overall, DEPO has the potential to make AI agents more efficient and effective in a wide range of applications."
cs.CL,A Compliance-Preserving Retrieval System for Aircraft MRO Task Search,"Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",https://arxiv.org/abs/2511.15383v1,2025-11-19T12:25:40Z,Byungho Jo,"Here's a summary of the research paper for a general audience:

**Improving Efficiency in Aircraft Maintenance**

Aircraft maintenance technicians spend a significant amount of time, up to 30%, searching through manuals to find the right procedures. This can be a major bottleneck in the maintenance, repair, and overhaul (MRO) process, where accuracy and compliance with regulations are crucial.

To address this issue, researchers have developed a new system that helps technicians quickly find the information they need while ensuring compliance with regulations. The system uses advanced search technology, including artificial intelligence and natural language processing, to provide accurate and relevant results.

The system works alongside existing certified viewers, which are used to verify procedures. It allows technicians to preview ranked tasks and access verified procedures quickly, reducing the time spent searching for information. In fact, the system reduced lookup time by 95%, from 6-15 minutes to just 18 seconds per task.

The researchers tested the system with 10 licensed aircraft maintenance technicians and found that it achieved a 90.9% success rate in retrieving the correct information. These results demonstrate that the system can significantly improve efficiency in aircraft maintenance while ensuring compliance with strict regulatory requirements."
cs.CL,The Empowerment of Science of Science by Large Language Models: New Tools and Methods,"Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.",https://arxiv.org/abs/2511.15370v1,2025-11-19T11:57:22Z,"Guoqiang Liang, Jingqian Gong, Mengxuan Li, Gege Lin, Shuo Zhang","Here's a summary of the research paper for a general audience:

**Harnessing the Power of AI to Advance Science**

Large language models (LLMs) are a type of artificial intelligence (AI) that have made tremendous progress in understanding and generating human-like language, recognizing images, and performing complex tasks. This has sparked a global technological competition to develop even more advanced AI capabilities.

In this paper, researchers review the key technologies that enable LLMs to perform so well, including techniques like prompt engineering, fine-tuning, and knowledge-enhanced generation. They also explore how LLMs can be applied to the field of Science of Science (SciSci), which aims to study and improve the scientific process itself.

The authors envision a future where AI agents can help evaluate scientific research, detect new research trends, and build knowledge graphs to connect scientific concepts. They propose new methods that leverage LLMs to accelerate scientific discovery and understanding. By harnessing the power of LLMs, scientists may be able to gain new insights, identify emerging research areas, and make scientific progress more efficiently."
cs.CL,HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning,"We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.",https://arxiv.org/abs/2511.15355v1,2025-11-19T11:31:32Z,"Alexis Correa-Guillén, Carlos Gómez-Rodríguez, David Vilares","**Advancing Healthcare Reasoning with HEAD-QA v2**

Imagine being able to ask a computer to explain a complex medical concept or make a diagnosis based on symptoms. This is the goal of a new dataset called HEAD-QA v2, which is designed to test a computer's ability to reason and make decisions in healthcare.

HEAD-QA v2 is an updated version of a previous dataset, and it's bigger and better than ever. It contains over 12,000 questions from professional medical exams in Spanish, which have been translated into English and other languages. The dataset is meant to challenge computers to think critically and make informed decisions, just like a doctor would.

Researchers tested several computer models on HEAD-QA v2 to see how well they could reason and make decisions. They found that the size of the model and its ability to reason were the most important factors in getting accurate answers. Surprisingly, using complex strategies to make decisions didn't make a big difference.

The good news is that HEAD-QA v2 is a reliable tool for improving computer models and advancing research in healthcare reasoning. This means that we can expect to see better and more accurate computer systems that can help doctors and healthcare professionals make informed decisions. With HEAD-QA v2, we're one step closer to making that a reality."
cs.CL,SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs,"Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.   We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.",https://arxiv.org/abs/2511.15323v1,2025-11-19T10:39:45Z,"Youwei Xiao, Yuyang Zou, Yun Liang","**Breakthrough in Hardware Synthesis: SkyEgg Revolutionizes FPGA Design**

Researchers have developed a novel framework called SkyEgg, which significantly improves the design of Field-Programmable Gate Arrays (FPGAs), a type of integrated circuit that can be programmed and reprogrammed. FPGAs are used in a wide range of applications, from artificial intelligence and machine learning to data processing and networking.

The current design process for FPGAs has limitations, as it separates two critical steps: choosing the best implementation for each component (implementation selection) and scheduling the components to work together efficiently (scheduling). This separation leads to suboptimal designs that don't fully utilize the capabilities of modern FPGAs.

SkyEgg addresses this limitation by combining implementation selection and scheduling into a single, unified process. By using a powerful data structure called an e-graph, SkyEgg represents both the algebraic transformations and hardware implementation choices in a way that allows for efficient exploration of the design space.

The results are impressive: SkyEgg achieves an average speedup of 3.01x over state-of-the-art high-level synthesis tools, with improvements up to 5.22x for complex expressions. This means that SkyEgg can help designers create faster, more efficient FPGA designs, which can lead to breakthroughs in various fields and applications.

**In simple terms:** SkyEgg is a new tool that helps design faster and more efficient computer chips (FPGAs) by combining two critical steps into one process. This leads to significant performance improvements, making it a game-changer for a wide range of applications."
cs.CL,Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models,"We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.",https://arxiv.org/abs/2511.15304v1,2025-11-19T10:14:08Z,"Piercosma Bisconti, Matteo Prandi, Federico Pierucci, Francesco Giarrusso, Marcantonio Bracale, Marcello Galisai, Vincenzo Suriani, Olga Sorokoletova, Federico Sartore, Daniele Nardi","**Breaking Down Language Model Safety: The Power of Adversarial Poetry**

Researchers have made a surprising discovery about the safety of large language models (LLMs), the AI systems that power chatbots and other online tools. They found that writing poems with malicious intent can be a highly effective way to bypass the safety mechanisms built into these models. In fact, poetic prompts were able to ""jailbreak"" or circumvent the safety features of many leading LLMs, allowing them to produce harmful or offensive content.

The researchers tested 25 different LLMs, including both proprietary and open-source models, and found that poetic prompts were able to succeed in up to 90% of cases. They also discovered that converting malicious prompts into verse was up to 18 times more effective than using straightforward prose.

What's more, the researchers found that poetic framing can achieve a high success rate in jailbreaking LLMs, with an average success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions. This is significantly higher than non-poetic baselines, which highlights the vulnerability of LLMs to poetic attacks.

The study's findings have significant implications for the development of safer AI systems. The researchers used a rigorous evaluation process, including both automated and human judges, to assess the outputs of the LLMs. They found that the poetic prompts were able to transfer across different domains, including chemical, biological, radiological, and nuclear (CBRN) threats, manipulation, cyber offenses, and loss of control.

The study's results suggest that current methods for aligning LLMs with safety protocols may have fundamental limitations. The researchers' work highlights the need for more robust safety mechanisms and evaluation protocols to prevent the misuse of LLMs. Ultimately, this research aims to improve the safety and reliability of AI systems, and to prevent their misuse."
cs.CL,MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews,"Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.",https://arxiv.org/abs/2511.15291v1,2025-11-19T09:56:16Z,Randa Zarnoufi,"Here's a summary of the research paper for a general audience:

**Improving Sentiment Analysis of Arabic Hotel Reviews**

Analyzing the sentiment of hotel reviews written in Arabic dialects can be tricky due to the many variations of the language and the limited amount of labeled data available. Researchers recently participated in a challenge to develop a system that can classify hotel reviews in Moroccan and Saudi dialects as positive, negative, or neutral.

The team used a technique called few-shot learning, which allows a computer model to learn from a small amount of data. They employed a framework called SetFit, which uses sentence transformers to fine-tune the model. This approach achieved a score of 73% in accurately classifying the sentiment of hotel reviews, ranking 12th out of 26 participants.

This study shows that few-shot learning can be an effective way to analyze the sentiment of dialectal Arabic text, even when there's limited data available. This has implications for improving the analysis of online reviews in specialized domains like hospitality, which can help businesses and customers make more informed decisions."
cs.CL,ChartEditor: A Reinforcement Learning Framework for Robust Chart Editing,"Chart editing reduces manual effort in visualization design. Typical benchmarks limited in data diversity and assume access to complete chart code, which is seldom in real-world scenarios. To address this gap, we present ChartEditVista, a comprehensive benchmark consisting of 7,964 samples spanning 31 chart categories. It encompasses diverse editing instructions and covers nearly all editable chart elements. The inputs in ChartEditVista include only the original chart image and natural language editing instructions, without the original chart codes. ChartEditVista is generated through a fully automated pipeline that produces, edits, and verifies charts, ensuring high-quality chart editing data. Besides, we introduce two novel fine-grained, rule-based evaluation metrics: the layout metric, which evaluates the position, size and color of graphical components; and the text metric, which jointly assesses textual content and font styling. Building on top of ChartEditVista, we present ChartEditor, a model trained using a reinforcement learning framework that incorporates a novel rendering reward to simultaneously enforce code executability and visual fidelity. Through extensive experiments and human evaluations, we demonstrate that ChartEditVista provides a robust evaluation, while ChartEditor consistently outperforms models with similar-scale and larger-scale on chart editing tasks.",https://arxiv.org/abs/2511.15266v1,2025-11-19T09:27:37Z,"Liangyu Chen, Yichen Xu, Jianzhe Ma, Yuqi Liu, Donglu Yang, Liang Zhang, Wenxuan Wang, Qin Jin","Here's a summary of the research paper for a general audience:

**Making Chart Editing Easier and More Accurate**

Creating and editing charts can be a tedious and time-consuming task, especially when trying to make precise changes. Researchers have developed a new framework called ChartEditor, which uses artificial intelligence (AI) to help edit charts more efficiently.

The team behind ChartEditor created a large dataset of charts, called ChartEditVista, which includes over 7,900 examples of charts with editing instructions. This dataset is unique because it only requires the original chart image and natural language instructions, making it more representative of real-world scenarios.

To evaluate the performance of ChartEditor, the researchers developed new metrics that assess the accuracy of chart edits, including the position, size, and color of graphical components, as well as textual content and font styling.

Using a technique called reinforcement learning, ChartEditor was trained to edit charts in a way that ensures both the code runs smoothly and the visual appearance is correct. The results show that ChartEditor outperforms other models in chart editing tasks, making it a promising tool for automating and improving chart creation and editing.

**In simple terms:** ChartEditor is an AI-powered tool that helps edit charts more easily and accurately. It was trained on a large dataset of charts and uses new evaluation metrics to ensure high-quality edits. The results demonstrate that ChartEditor is effective in making precise changes to charts, making it a useful tool for anyone who works with data visualizations."
cs.CL,"IndicGEC: Powerful Models, or a Measurement Mirage?","In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.",https://arxiv.org/abs/2511.15260v1,2025-11-19T09:24:23Z,Sowmya Vajjala,"Here's a summary of the research paper for a general audience:

**Improving Language Tools for Indian Languages**

Researchers recently participated in a competition to develop artificial intelligence (AI) tools that can correct grammatical errors in text written in five Indian languages: Telugu, Hindi, Tamil, Malayalam, and Bangla. The goal was to create models that can learn from a small amount of data and still perform well.

The team achieved impressive results, ranking 4th in Telugu and 2nd in Hindi. However, upon closer inspection of the data and evaluation methods used, they raised concerns about the quality of the datasets and the metrics used to measure success. Surprisingly, they found that smaller AI models can perform just as well as larger ones, which is a promising finding for developing language tools for Indian languages.

The study highlights the need for better datasets and evaluation methods to ensure that AI tools are accurate and effective for Indian languages. This research has implications for the development of language tools that can be used by people who speak these languages, and could ultimately improve communication and access to information."
cs.CL,"M, Toolchain and Language for Reusable Model Compilation","Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.",https://arxiv.org/abs/2511.15257v1,2025-11-19T09:21:46Z,"Hiep Hong Trinh, Federico Ciccozzi, Abu Naser Masud, Marjan Sirjani, Mikael Sjödin","**Simplifying Complex System Development with M**

Imagine building a self-driving car or a smart home system. These complex systems involve many parts working together, like sensors, computers, and physical interactions. Creating them efficiently and safely requires using software models to design and test the system before building it.

The problem is that these models need to be translated into different formats for various purposes, such as simulation, deployment, and safety checks. This can be a challenging and time-consuming process. To address this, researchers have developed a new tool called M, which is a language and a set of tools for creating and compiling models of complex systems.

**What makes M special?**

M allows engineers to create a single high-level model of a system, and then automatically generate multiple versions of that model for different uses. This is called ""multi-target compilation."" M is designed to support the development of complex, concurrent, and time-aware systems, which are common in areas like robotics, autonomous vehicles, and smart homes.

**How does M work?**

M uses a simple and flexible language that is based on the idea of actors, which are like independent agents that interact with each other. This language is easy to use and understand, making it accessible to engineers and developers. M also provides a framework for generating different versions of a model, ensuring that they are consistent and accurate.

**Benefits of M**

The M toolchain and language offer several benefits, including:

* **Faster development**: M simplifies the process of creating complex systems by allowing engineers to work with a single high-level model.
* **Improved safety**: M ensures that different versions of a model are consistent and accurate, reducing the risk of errors and bugs.
* **Increased flexibility**: M can be used with other modeling languages, making it a versatile tool for system development.

Overall, M has the potential to revolutionize the way complex systems are designed and developed, making it easier to create efficient, safe, and reliable systems."
cs.CL,Context Cascade Compression: Exploring the Upper Limits of Text Compression,"Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression",https://arxiv.org/abs/2511.15244v1,2025-11-19T09:02:56Z,"Fanfan Liu, Haibo Qiu","**Breaking News in AI Research: A Major Leap in Text Compression**

Imagine being able to shrink a long piece of text into a tiny fraction of its original size, while still being able to accurately reconstruct the original text. This sounds like science fiction, but researchers have just made a significant breakthrough in achieving this goal.

The challenge lies in handling long pieces of text, which can be computationally expensive and memory-intensive for artificial intelligence (AI) models. To address this, a team of researchers has developed a new method called Context Cascade Compression (C3). This approach uses two AI models working together to compress and then reconstruct long texts.

The result? C3 can compress text by a factor of 20 or even 40, while maintaining remarkably high accuracy - 98% and 93%, respectively. This is a significant improvement over previous methods, which achieved much lower accuracy.

What's more, C3 uses a simple, text-based approach that ignores factors like layout and visual information. This suggests that C3 may have set a new upper limit for text compression ratios, with potential implications for fields like optical character recognition (OCR) and document scanning.

The researchers have made their code and model weights publicly available, which means that others can build upon this work and explore new applications. This breakthrough has the potential to revolutionize the way we handle and process large amounts of text data."
cs.CL,OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition,"Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.",https://arxiv.org/abs/2511.15211v1,2025-11-19T08:02:55Z,"Xinli Tao, Xin Dong, Xuezhong Zhou","**Breakthrough in Medical Information Extraction: OEMA Framework**

Researchers have developed a new framework called OEMA, which enables computers to accurately extract important information from electronic health records (EHRs) without requiring extensive manual annotation. This is a significant challenge in the medical field, as EHRs contain vast amounts of valuable data that can be used to improve patient care and medical research.

The OEMA framework uses a unique approach called multi-agent collaboration, which involves three components working together to identify and extract specific medical information, such as disease names and medication types. This approach has been shown to achieve state-of-the-art performance on two benchmark datasets, matching or surpassing the accuracy of traditional supervised learning models that require costly annotated data.

The key innovation of OEMA is its use of ontology-guided reasoning, which leverages a comprehensive medical knowledge graph (SNOMED CT) to filter and refine the extracted information. This enables the framework to accurately identify medical entities even in cases where there is limited training data.

The development of OEMA has significant implications for clinical natural language processing (NLP) applications, as it can help unlock the vast amounts of information contained in EHRs and enable more accurate and efficient analysis of medical data."
stat.ML,A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation,"Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",https://arxiv.org/abs/2511.15543v1,2025-11-19T15:37:17Z,"Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis","Here's a summary of the research paper for a general audience:

**Optimizing Sensor Placement for Better Data Collection**

Imagine trying to understand a complex system, like a weather pattern or a chemical reaction, but you can only collect data from a few limited locations. How do you choose where to place your sensors to get the most useful information? This is a challenge in many fields, including engineering, where accurately estimating parameters is crucial.

Researchers have developed a new framework that combines machine learning and physics to tackle this problem. They use a type of artificial intelligence called Physics-Informed Neural Networks (PINNs) to estimate parameters and identify the best locations for sensors. This approach allows them to efficiently compute sensitivity functions, which help determine the optimal sensor placements.

The researchers tested their framework on two complex problems involving chemical reactions and fluid flow. They found that their method consistently produced more accurate estimates of parameters compared to traditional approaches that rely on intuition or random sensor placement. This new framework has the potential to improve data collection and analysis in a wide range of fields, from engineering to environmental science.

**Key Takeaway:** By combining machine learning and physics, researchers have developed a powerful tool for optimizing sensor placement and parameter estimation, leading to more accurate and efficient data collection."
stat.ML,Sample-Adaptivity Tradeoff in On-Demand Sampling,"We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",https://arxiv.org/abs/2511.15507v1,2025-11-19T14:59:47Z,"Nika Haghtalab, Omar Montasser, Mingda Qiao","**Understanding the Tradeoff in On-Demand Sampling**

Imagine you're trying to learn about different groups of people, but you can only ask a limited number of questions. You want to learn as much as possible, but you also want to ask your questions efficiently. This is similar to a problem in machine learning called on-demand sampling.

Researchers studied how to balance two important factors in on-demand sampling: the number of samples (or questions) needed to learn something, and the number of rounds (or chances) you have to ask those questions. They found that if you have more rounds to ask questions, you can get by with fewer samples. But if you only have a few rounds, you need to ask more questions to be sure you're learning accurately.

The researchers developed a new framework called Optimization via On-Demand Sampling (OODS) that helps understand this tradeoff. They showed that it's possible to achieve near-optimal results with a reasonable number of rounds (about the square root of the number of groups). However, they also found that trying to do better than that would require new and innovative techniques.

This research has implications for many areas of machine learning, such as learning about multiple groups of people or objects, and could lead to more efficient and effective learning algorithms."
stat.ML,Gini Score under Ties and Case Weights,"The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",https://arxiv.org/abs/2511.15446v1,2025-11-19T14:01:12Z,"Alexej Brauer, Mario V. Wüthrich","Here's a summary of the research paper for a general audience:

**Understanding the Gini Score: A Tool for Evaluating Model Performance**

The Gini score is a widely used statistical tool that helps evaluate the performance of models, particularly in fields like finance and insurance. It's a measure of how well a model can rank risks or predict outcomes. For example, in healthcare, a model might try to predict which patients are most likely to experience a certain health issue. The Gini score helps assess how well the model distinguishes between high-risk and low-risk patients.

The Gini score has been commonly used in situations where there are only two possible outcomes, like yes or no. However, real-world situations often involve more complex scenarios. This paper explores how to use the Gini score when there are ties or equal rankings, and when some data points have more importance than others (known as case weights).

The researchers adapt the Gini score to handle these complexities, making it a more versatile tool for evaluating model performance. This is particularly useful in fields like actuarial science, where accurate risk assessments are crucial for making informed decisions. By refining the Gini score, the researchers aim to provide a more accurate and reliable way to evaluate model performance in a wide range of applications."
stat.ML,Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss,"In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.   Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso",https://arxiv.org/abs/2511.15332v1,2025-11-19T10:50:46Z,The Tien Mai,"**Introducing the Exponential Lasso: A Robust Method for Data Analysis**

In data analysis, a popular method called Lasso is used to select important variables and estimate their effects. However, Lasso can be sensitive to unusual data points, known as outliers, and noisy data, which can lead to incorrect conclusions. To address this issue, researchers have developed a new method called the Exponential Lasso.

The Exponential Lasso is a robust method that can handle data with unusual points and noise, while still providing accurate results. It uses a special type of mathematical function that smoothly reduces the impact of extreme outliers, while still being efficient when the data is clean.

**Key Benefits:**

* **Robustness**: The Exponential Lasso can handle data with outliers and heavy-tailed noise, providing more reliable results.
* **Efficiency**: The method achieves fast convergence rates, making it competitive with traditional Lasso methods.
* **Easy to Implement**: The Exponential Lasso is implemented in an R package called **heavylasso**, available on Github.

**What does this mean?**

The Exponential Lasso offers a powerful tool for data analysis that can handle complex data sets with unusual points and noise. By using this method, researchers and analysts can obtain more accurate and reliable results, which can inform important decisions in various fields, such as medicine, finance, and social sciences.

**Try it out!**

The **heavylasso** package is available on Github, allowing users to easily implement the Exponential Lasso method in their own data analysis projects."
stat.ML,BaGGLS: A Bayesian Shrinkage Framework for Interpretable Modeling of Interactions in High-Dimensional Biological Data,"Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.",https://arxiv.org/abs/2511.15330v1,2025-11-19T10:48:30Z,"Marta S. Lemanczyk, Lucas Kock, Johanna Schlimme, Nadja Klein, Bernhard Y. Renard","Here's a summary of the research paper in a way that's easy to understand:

**Unlocking Hidden Patterns in Biological Data**

Biological data, such as genetic information, can be complex and noisy, making it difficult to identify meaningful patterns. Researchers often use machine learning models to analyze this data, but these models can be like black boxes, providing accurate predictions but not explaining how they arrived at those predictions.

A team of researchers has developed a new approach called BaGGLS, which combines the strengths of statistical models (which are interpretable) with the power of machine learning models (which can handle complex interactions). BaGGLS is designed to identify interactions between different biological features, such as genetic motifs, that are important for understanding biological processes.

**What makes BaGGLS special?**

BaGGLS uses a Bayesian approach, which allows it to incorporate prior knowledge and uncertainty into its analysis. It also uses a special type of prior distribution, called a group global-local shrinkage prior, which helps to identify the most important features and suppress noise.

**How well does BaGGLS work?**

The researchers tested BaGGLS on simulated data and found that it outperformed other methods in detecting interactions between features. They also applied BaGGLS to real biological data, such as motif scanner outputs and attribution scores from deep learning models, and found that it was able to identify biologically relevant interaction patterns.

**What's the impact?**

BaGGLS has the potential to be a powerful tool for uncovering hidden patterns in biological data, which could lead to new insights and discoveries in fields such as genomics and computational biology. Its ability to provide interpretable results makes it a valuable approach for researchers who want to understand the underlying biology behind their data."
stat.ML,Robust Bayesian Optimisation with Unbounded Corruptions,"Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.",https://arxiv.org/abs/2511.15315v1,2025-11-19T10:28:56Z,"Abdelhamid Ezzerg, Ilija Bogunovic, Jeremias Knoblauch","**Protecting Against Extreme Outliers in Machine Learning**

Imagine you're trying to optimize a complex system, like a self-driving car's navigation system, using machine learning. But, what if some of the data you're using is corrupted or extreme outliers, like a sensor giving wildly incorrect readings? This can cause problems for the optimization process, leading to poor performance.

Researchers have developed a new method, called RCGP-UCB, to protect against such extreme outliers. Their approach combines two techniques: a robust statistical model that can handle corrupted data and a strategy for balancing exploration and exploitation.

The key innovation is that RCGP-UCB can handle a certain number of corruptions, even if they are extremely large. In fact, it can tolerate up to a certain number of corruptions (specifically, $O(T^{1/2})$ or $O(T^{1/3})$) with possibly infinite magnitude. This means that even if some data points are wildly incorrect, the algorithm can still make good decisions.

The best part? This robustness comes at almost no cost. When there are no outliers, RCGP-UCB performs just as well as standard optimization algorithms. This new method has the potential to make machine learning more reliable and robust in real-world applications."
stat.ML,Particle Monte Carlo methods for Lattice Field Theory,"High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.",https://arxiv.org/abs/2511.15196v1,2025-11-19T07:31:46Z,David Yallup,"Here's a summary of the research paper for a general audience:

**Improving Computer Simulations with Particle Methods**

Researchers have made a breakthrough in a field called Lattice Field Theory (LFT), which uses computer simulations to study the behavior of particles in physics. The challenge is that these simulations involve sampling from complex, high-dimensional spaces, which can be computationally expensive.

The researchers found that using a classical method called Particle Monte Carlo, specifically Sequential Monte Carlo and nested sampling, can be just as effective as newer, machine learning-based methods. These particle methods use a large number of ""particles"" to explore the simulation space and estimate important quantities.

The best part is that these particle methods can be easily accelerated using modern computer hardware, such as graphics processing units (GPUs). The researchers showed that their approach can match or even outperform state-of-the-art machine learning methods in terms of accuracy and speed.

This is significant because it provides a simple, yet powerful, alternative to machine learning-based methods, which often require significant expertise and computational resources to train. The researchers' approach can be used with minimal tuning, making it a more accessible and efficient solution for complex simulations."
stat.ML,Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings,"Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.",https://arxiv.org/abs/2511.15146v1,2025-11-19T05:59:01Z,Eugene Ndiaye,"**Advancing Uncertainty Estimation in Machine Learning: A Breakthrough in Multivariate Predictions**

Imagine you're trying to predict the weather for tomorrow. A machine learning model might give you a forecast, but how confident should you be in that prediction? A new research paper presents a significant advancement in estimating uncertainty in machine learning predictions, particularly when dealing with multiple variables, such as temperature, humidity, and wind speed.

**The Problem with Current Methods**

Current methods for estimating uncertainty, known as conformal prediction, work well when dealing with a single variable, like temperature. However, they struggle when trying to predict multiple variables simultaneously. This is because it's challenging to compare and rank multiple values.

**A New Approach: Optimal Transport**

The researchers propose a new approach that leverages optimal transport, a mathematical technique for comparing and ranking multiple values. By combining optimal transport with conformal prediction, they create a method that can handle multiple variables and provide reliable uncertainty estimates.

**Key Breakthroughs**

The researchers made two significant breakthroughs:

1. **Finite-sample calibration**: They developed a method that provides guaranteed coverage of the true outcome, even with a limited number of data points. This is crucial in real-world applications where data is often scarce.
2. **Multivariate predictive distributions**: They created a way to generate a probability distribution over multiple variables, which can be used to estimate uncertainty in predictions. This distribution is ""calibrated,"" meaning that it accurately reflects the true uncertainty in the predictions.

**Implications and Applications**

The researchers' work has far-reaching implications for various fields, including weather forecasting, finance, and healthcare. For instance, in weather forecasting, their method can provide more accurate and reliable predictions of temperature, humidity, and wind speed. In finance, it can help estimate the uncertainty in stock prices and portfolio returns.

**In Simple Terms**

To illustrate the significance of this research, consider a weather forecasting model that predicts a 50% chance of rain tomorrow. The new method can provide a more detailed and accurate picture of the uncertainty surrounding that prediction, such as the likelihood of light rain, heavy rain, or no rain at all. This can help decision-makers, like event planners or emergency responders, make more informed decisions.

Overall, this research paper presents a significant advancement in uncertainty estimation for machine learning predictions, enabling more accurate and reliable predictions in a wide range of applications."
stat.ML,Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit,"In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.",https://arxiv.org/abs/2511.15120v1,2025-11-19T04:46:47Z,"Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. Lee","**Unlocking the Secrets of Neural Networks: A Breakthrough in Efficient Learning**

Researchers have made a significant discovery about how neural networks learn complex patterns in high-dimensional data. In a study titled ""Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit,"" the authors explored how neural networks can efficiently learn features from data.

**The Challenge of High-Dimensional Data**

Neural networks are powerful tools for analyzing data, but their ability to learn from high-dimensional data has been a long-standing challenge. High-dimensional data refers to data with a large number of features or variables, which can make it difficult for neural networks to identify meaningful patterns.

**A New Understanding of Neural Network Learning**

The researchers found that a standard two-layer neural network can learn a wide range of functions with a small number of samples and in a relatively short amount of time. Specifically, they showed that with a number of samples and time complexity that grows linearly with the dimension of the data, a neural network can learn to identify the underlying patterns in the data with near-optimal accuracy.

**How Neural Networks Achieve This Efficiency**

The study revealed that the neural network achieves this efficiency by using a process called ""power-iteration"" to implicitly mimic a spectral start for the whole span of the hidden subspace. This process eliminates finite-sample noise and recovers the underlying patterns in the data. The researchers also found that this process requires more than a single step of training, highlighting the importance of multiple iterations in the learning process.

**Implications and Future Directions**

The findings have significant implications for the development of more efficient and effective neural network architectures. By understanding how neural networks can learn complex patterns in high-dimensional data, researchers can design more efficient algorithms and models that can tackle real-world challenges in areas such as computer vision, natural language processing, and more.

**In Simple Terms**

Imagine trying to find a specific book in a huge library. A neural network is like a super-smart librarian that can quickly find the book by looking at the books' features (e.g., title, author, genre). The researchers found that this librarian can learn to identify the features of the book with a small number of examples and in a short amount of time, making it an efficient and effective tool for learning complex patterns in high-dimensional data."
stat.ML,Latent space analysis and generalization to out-of-distribution data,"Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.",https://arxiv.org/abs/2511.15010v1,2025-11-19T01:23:34Z,"Katie Rainey, Erin Hausmann, Donald Waagen, David Gray, Donald Hulsey","Here's a summary of the research paper for a general audience:

**Can AI Systems Handle Unexpected Data?**

Deep learning systems, like those used in self-driving cars or medical diagnosis, can make accurate predictions when faced with familiar data. But what happens when they encounter unexpected or unusual data, such as a car driving on the wrong side of the road or an unusual medical image? Researchers are working to understand how these systems perform in such situations.

In a recent study, researchers analyzed how deep learning systems behave when faced with data that is outside of their normal training data, known as ""out-of-distribution"" (OOD) data. They used simulated and real-world data from radar systems to test the performance of these systems.

The study found that detecting OOD data is not a reliable way to predict how well a deep learning system will perform overall. In other words, just because a system can detect that it's faced with unusual data, it doesn't mean it will make accurate predictions.

The researchers hope that their findings will inspire further investigation into the inner workings of deep learning systems, which could lead to more robust and generalizable AI systems that can handle unexpected data. This could have significant implications for the development of AI systems that can be safely used in real-world applications."
stat.ML,Implicit Bias of the JKO Scheme,"Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.",https://arxiv.org/abs/2511.14827v1,2025-11-18T18:48:37Z,"Peter Halmos, Boris Hanin","**Unlocking the Secrets of the JKO Scheme: A Powerful Tool for Optimization**

Imagine you're trying to find the most efficient way to arrange a set of objects, like packing boxes into a room. Mathematicians and computer scientists use complex algorithms to solve such problems. One popular method is called the Jordan-Kinderlehrer-Otto (JKO) scheme, which helps find the optimal arrangement by iteratively refining the solution.

**What does the JKO scheme do?**

The JKO scheme is a powerful tool for optimizing a wide range of problems, from machine learning to physics. It works by creating a sequence of probability distributions that approximate the optimal solution. The scheme has many desirable properties, such as preserving energy dissipation and exhibiting unconditional stability.

**The implicit bias of the JKO scheme**

Researchers have now uncovered a hidden aspect of the JKO scheme, known as its implicit bias. This bias refers to the subtle changes the scheme makes to the optimization problem it's trying to solve. Specifically, the JKO scheme modifies the original problem by subtracting a term that depends on the curvature of the problem. This modification has a ""decelerating"" effect, slowing down the optimization process in areas where the problem's curvature changes rapidly.

**What does this mean in practice?**

The implicit bias of the JKO scheme has significant implications for various applications. For example, in machine learning, the scheme may affect the way models learn from data. In physics, it may influence the simulation of complex systems. The researchers demonstrated these effects using simple numerical examples, including a problem involving the arrangement of particles in a space.

**Key takeaways**

* The JKO scheme is a powerful tool for optimization problems.
* The scheme has an implicit bias that modifies the original problem.
* This bias has a decelerating effect in areas of high curvature.
* The implicit bias has significant implications for various applications.

By understanding the implicit bias of the JKO scheme, researchers and practitioners can better harness its power to solve complex optimization problems."
stat.ML,Look-Ahead Reasoning on Learning Platforms,"On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",https://arxiv.org/abs/2511.14745v1,2025-11-18T18:45:32Z,"Haiqing Zhu, Tijana Zrnic, Celestine Mendler-Dünner","**The Impact of Strategic Thinking on Learning Platforms**

Imagine you're using an online learning platform, such as a language learning app or an online course. The platform uses algorithms to predict your progress and provide personalized recommendations. But have you ever wondered how these algorithms are designed, and whose goals they prioritize?

Researchers have found that these algorithms often reflect the goals of the platform's designers, rather than the goals of the users themselves. As a result, users may try to ""game"" the system to get better outcomes. For example, a user might repeatedly take a quiz to make it seem like they're struggling with a particular concept, in order to get more practice questions.

This behavior is called ""strategic user behavior."" Previous studies have looked at how users respond strategically to these algorithms, but they've mostly focused on individual users acting alone. However, what if users start to think ahead and consider how their actions will affect the algorithm's predictions, and how other users will respond?

The researchers in this study explored this idea, called ""look-ahead reasoning."" They found that when users think ahead and try to outsmart their peers, it can actually speed up the process of reaching a stable outcome. However, in the long run, it doesn't provide any benefits to individual users.

The researchers then looked at what happens when users work together and coordinate their actions to achieve a common goal. They found that coordination can lead to better outcomes, but it also requires a certain level of ""alignment"" between the goals of the users and the goals of the platform.

The study's findings have implications for the design of learning platforms and algorithms. By taking into account the strategic behavior of users, designers can create more effective and fair systems that prioritize the needs of users.

**Takeaways:**

* Online learning platforms often prioritize the goals of designers over those of users.
* Users may engage in strategic behavior to get better outcomes.
* Look-ahead reasoning can speed up the process of reaching a stable outcome, but doesn't provide long-term benefits to individual users.
* Coordination among users can lead to better outcomes, but requires alignment between user and platform goals.

**What's next:**

* The study's findings can inform the design of more effective and fair learning platforms.
* Further research is needed to explore the implications of strategic user behavior on other types of platforms and algorithms."
stat.ML,Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization,"We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",https://arxiv.org/abs/2511.14710v1,2025-11-18T17:51:17Z,"Zonghao Chen, Atsushi Nitanda, Arthur Gretton, Taiji Suzuki","Here's a summary of the research paper for a general audience:

**Improving Neural Networks for Complex Data Analysis**

Researchers have made a breakthrough in developing neural networks, a type of artificial intelligence, to analyze complex data. Specifically, they focused on a problem called nonparametric instrumental variable regression (NPIV), which is a statistical method used to understand relationships between variables.

The researchers created a new method, called **F$^2$BMLD**, which combines two techniques: neural networks and a mathematical approach called mean-field Langevin dynamics. This method helps neural networks learn from data more efficiently and accurately.

The study achieved two main results:

1. **Convergence guarantee**: The researchers proved that their method, **F$^2$BMLD**, can converge to a solution, meaning it can reliably find the correct answer.
2. **Generalization bound**: They also showed that the method can generalize well to new, unseen data, which is essential for making accurate predictions.

The researchers tested their method on a benchmark problem in offline reinforcement learning and found that it works effectively.

In simple terms, this research improves the performance of neural networks in analyzing complex data, which can have applications in various fields, such as economics, finance, and healthcare. The new method, **F$^2$BMLD**, provides a more efficient and accurate way to understand relationships between variables, leading to better decision-making."
stat.ML,Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer,"Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.",https://arxiv.org/abs/2511.14691v1,2025-11-18T17:28:29Z,"Kallol Mondal, Ankush Kumar","**Breakthrough in Energy-Efficient AI: A Brain-Inspired Approach**

Imagine a computer chip that mimics the human brain's ability to focus on specific information while ignoring irrelevant details. Researchers have made a significant step towards creating such a chip by developing a new type of artificial intelligence (AI) model called the Spiking STDP Transformer (S$^{2}$TDPT).

The S$^{2}$TDPT model is inspired by the brain's attention mechanism, which allows us to selectively concentrate on certain aspects of our surroundings. This model uses a process called spike-timing-dependent plasticity (STDP), a core mechanism of memory and learning in the brain, to enable more efficient and brain-like computation.

**Key Benefits:**

* **Energy Efficiency:** The S$^{2}$TDPT model uses significantly less energy than traditional AI models, reducing its carbon footprint and making it more suitable for use in mobile devices and other applications where power consumption is a concern. Specifically, the model achieves an 88.47% energy reduction compared to a standard ANN Transformer.
* **Improved Interpretability:** The model is more transparent and explainable, allowing researchers to understand how it makes decisions and which features of the input data it focuses on. This is demonstrated through the use of Grad-CAM, which shows that the model attends to semantically relevant regions.
* **Brain-Like Computation:** The S$^{2}$TDPT model uses a more brain-like approach to processing information, which could lead to the development of more efficient and adaptive AI systems.

**Technical Details:**

* The S$^{2}$TDPT model implements self-attention through STDP, embedding query-key correlations in synaptic weights.
* The model achieves high accuracy on image classification tasks, including 94.35% on CIFAR-10 and 78.08% on CIFAR-100 with only four timesteps.
* The model uses in-memory computing and supports non-von Neumann hardware, making it suitable for use in neuromorphic devices.

**Potential Applications:**

* **Neuromorphic Computing:** The S$^{2}$TDPT model could be used to develop more efficient and adaptive AI systems for use in neuromorphic devices, such as brain-inspired computer chips.
* **Edge AI:** The model's energy efficiency and improved interpretability make it suitable for use in edge AI applications, such as image classification and object detection in mobile devices.

Overall, the S$^{2}$TDPT model represents a significant step towards creating more efficient, adaptive, and brain-like AI systems. Its development could have a major impact on the field of artificial intelligence and lead to breakthroughs in areas such as computer vision, natural language processing, and robotics."
stat.ML,DeepBlip: Estimating Conditional Average Treatment Effects Over Time,"Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",https://arxiv.org/abs/2511.14545v1,2025-11-18T14:49:03Z,"Haorui Ma, Dennis Frauen, Stefan Feuerriegel","**Understanding Treatment Effects Over Time: A New Approach**

Imagine you're trying to understand how a treatment, such as a medication or therapy, affects patients over time. Researchers often want to know not just the overall effect of the treatment, but how it affects patients at different points in time. A new study proposes a method called DeepBlip, which uses artificial intelligence (AI) to estimate these time-specific effects.

**The Problem: Estimating Treatment Effects Over Time**

Current methods for estimating treatment effects over time can be limited. They often require re-computing the effects from scratch, which can be time-consuming and inefficient. DeepBlip aims to solve this problem by breaking down the treatment effect into smaller, more manageable pieces called ""blip effects."" These blip effects represent the incremental impact of the treatment at each point in time.

**How DeepBlip Works**

DeepBlip uses a type of AI called neural networks to analyze data from patients over time. It takes into account factors that can influence the treatment effect, such as changes in the patient's condition or other treatments they may be receiving. The method uses a novel ""double optimization trick"" to enable the simultaneous learning of all blip functions, which allows for more efficient and accurate estimation of treatment effects.

**Key Benefits**

DeepBlip has several key benefits:

* **Accurate estimates**: DeepBlip provides unbiased estimates of treatment effects over time, which can help researchers and clinicians make more informed decisions.
* **Efficient evaluation**: DeepBlip enables the efficient offline evaluation of optimal treatment policies without re-computation, which can save time and resources.
* **Robustness to model misspecification**: DeepBlip's Neyman-orthogonal loss function ensures robustness to nuisance model misspecification, which means that the method is less sensitive to errors in the model.

**Testing and Results**

The researchers tested DeepBlip on several clinical datasets and found that it performed better than existing methods. This suggests that DeepBlip could be a valuable tool for understanding how treatments affect patients over time.

**Implications**

The development of DeepBlip has important implications for healthcare and research. By providing a more accurate and efficient way to estimate treatment effects over time, DeepBlip could help clinicians and researchers:

* **Personalize treatment plans**: By understanding how treatments affect patients at different points in time, clinicians can create more personalized treatment plans.
* **Improve treatment outcomes**: By identifying the most effective treatments and treatment sequences, researchers can improve treatment outcomes for patients.
* **Advance precision medicine**: DeepBlip's ability to analyze complex temporal dependencies and adjust for time-varying confounding could help advance the field of precision medicine.

Overall, DeepBlip represents a significant advance in the field of treatment effect estimation, and its applications have the potential to improve healthcare outcomes and advance precision medicine."
stat.ML,Skewness-Robust Causal Discovery in Location-Scale Noise Models,"To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \to Y$ and $Y \to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",https://arxiv.org/abs/2511.14441v1,2025-11-18T12:40:41Z,"Daniel Klippert, Alexander Marx","**Unlocking Cause-and-Effect Relationships: A New Approach to Causal Discovery**

Understanding cause-and-effect relationships is crucial in various fields, from medicine to economics. Researchers use causal discovery methods to identify these relationships from data. However, distinguishing cause from effect can be challenging, especially when the data is noisy or skewed.

A recent study proposes a new algorithm, called SkewD, to improve causal discovery in situations where the noise is not symmetrical (skewed). SkewD builds on a flexible class of models called location-scale noise models, which are widely used to model cause-and-effect relationships.

The innovation of SkewD lies in its ability to handle skewed noise distributions, which are common in real-world data. Unlike existing methods that assume symmetrical noise, SkewD can reliably infer cause-and-effect relationships even when the noise is skewed.

The researchers tested SkewD on synthetic and benchmark datasets with skewed noise and found that it performs well and remains robust even in cases with high skewness. This new approach has the potential to improve the accuracy of causal discovery in various fields, enabling researchers to better understand complex relationships and make more informed decisions.

**Key Takeaway:** SkewD is a new algorithm that improves causal discovery by handling skewed noise distributions, leading to more accurate identification of cause-and-effect relationships."
stat.ML,Causal Discovery on Higher-Order Interactions,"Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.",https://arxiv.org/abs/2511.14206v1,2025-11-18T07:35:00Z,"Alessio Zanga, Marco Scutari, Fabio Stella","**Unlocking Complex Relationships: A New Approach to Causal Discovery**

Imagine trying to understand how different factors, such as weather, traffic, and road conditions, affect your daily commute. Causal discovery is a technique used to learn the relationships between these factors, but it can be challenging when data is limited. To address this, researchers have developed a new approach that considers not just individual relationships, but also more complex interactions between multiple factors.

The traditional method of causal discovery involves creating a diagram, known as a DAG, that shows the causal relationships between variables. However, when data is scarce, this diagram can be uncertain. To measure this uncertainty, researchers use a technique called bagging, which involves creating multiple diagrams and then combining them. The problem is that the current method of combining these diagrams only looks at individual relationships, ignoring more complex patterns.

The new approach, introduced in this research paper, takes into account these higher-order interactions. It uses a novel theoretical framework and a new algorithm to combine the diagrams in a more sophisticated way. The results show that this approach is both efficient and effective, outperforming existing methods, especially when data is limited or there are many variables to consider.

This breakthrough has the potential to improve our understanding of complex systems and relationships in various fields, from medicine and social sciences to economics and environmental studies. By considering higher-order interactions, researchers can gain a more nuanced understanding of how different factors interact and influence each other."
stat.ML,SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation,"We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.",https://arxiv.org/abs/2511.14146v1,2025-11-18T05:13:49Z,"Renjie Chen, Viet Anh Nguyen, Huifu Xu","**Improving Statistical Estimates with SCOPE**

Researchers have developed a new method called SCOPE (Spectral concentrated COvariance and Precision matrix Estimator) to improve the accuracy of statistical estimates in data analysis. The method focuses on estimating two important matrices: the covariance matrix, which describes how different variables in a dataset are related to each other, and the precision matrix, which is the inverse of the covariance matrix.

The SCOPE method uses a robust approach to minimize errors in the estimates by considering a range of possible distributions of the data, rather than relying on a single assumed distribution. This approach helps to reduce the impact of outliers and other anomalies in the data.

The researchers found that the SCOPE method produces estimates that are more accurate and reliable than existing methods. Specifically, it corrects for a common problem called ""spectral bias"" in the estimates, which can lead to poor performance in practical applications. The method also improves the ""condition number"" of the estimates, which is a measure of their stability.

The SCOPE method has been tested on both simulated and real-world data, and has been shown to perform competitively with state-of-the-art methods. The researchers have also developed a scheme for tuning the parameters of the method to optimize its performance.

Overall, the SCOPE method has the potential to improve the accuracy and reliability of statistical estimates in a wide range of applications, from finance and economics to biology and medicine."
stat.ML,"Synthetic Survival Control: Extending Synthetic Controls for ""When-If"" Decision","Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such ""when-if"" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.",https://arxiv.org/abs/2511.14133v1,2025-11-18T04:36:20Z,"Jessy Xinyi Han, Devavrat Shah","**Understanding the Impact of Interventions on Survival Rates**

Imagine being able to predict how a new treatment or policy would affect the survival rate of patients with a specific disease. Researchers have developed a new method called Synthetic Survival Control (SSC) to estimate the impact of interventions on survival rates using observational data.

**The Challenge**

Estimating the effect of an intervention on survival rates is tricky because it's hard to account for factors like different treatment adoption rates, limited sample sizes, and non-random treatment assignment. SSC addresses these challenges by creating a ""synthetic"" control group that mimics the characteristics of the treatment group.

**How SSC Works**

SSC uses data from multiple units (e.g., patients, countries) to estimate the counterfactual hazard trajectory (the risk of an event occurring over time) for a unit of interest. It does this by combining the observed trajectories from other units to create a weighted average. This approach allows researchers to estimate what would have happened if a unit had not received the intervention.

**Validation and Results**

The researchers tested SSC using a multi-country clinical dataset of cancer treatment outcomes. They found that access to new treatments was associated with improved survival rates, as reflected by lower post-intervention hazard trajectories. This means that SSC can provide valuable insights into the effectiveness of interventions on survival rates.

**Implications**

The SSC framework offers a general and interpretable tool for counterfactual survival inference using observational data. This has broad implications for fields like medicine, economics, and public policy, where understanding the impact of interventions on survival rates is crucial for making informed decisions."
stat.ML,Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds,"Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.",https://arxiv.org/abs/2511.14056v1,2025-11-18T02:15:25Z,"Marios Papamichals, Regina Ruane","**Unlocking Stable Generative Models on Curved Spaces**

Imagine trying to create a map of a sphere (like the Earth) using flat paper. This is similar to what generative models do when working with complex data that doesn't fit into a straight line or flat plane. These models use mathematical tools called ""charts"" to translate data from curved spaces to flat ones, but this can cause problems.

Researchers have faced two main challenges: 

1. **Preserving distances**: Some methods keep distances accurate but can make the math behind the model complicated and unstable.
2. **Maintaining density**: Others preserve the density of data but can distort distances, leading to inaccurate results.

To address these issues, a team introduced a new method called **Radial Compensation (RC)**. RC helps create stable and accurate generative models by:

* **Decoupling curvature and model parameters**: This means that the model's performance isn't affected by the curvature of the space it's working with.
* **Preserving geodesic distances**: RC ensures that distances on the curved space are accurately represented.

The researchers also developed a new family of charts called **Balanced-Exponential (bExp)**, which balances the trade-offs between preserving distances and densities. They found that RC:

* **Improves model performance**: By providing more accurate and stable results.
* **Reduces gradient variance**: This makes the model training process more efficient.
* **Prevents radius blow-ups**: RC prevents the model's parameters from becoming too large, which can cause instability.

The RC method has been successfully applied to various domains, including:

* **Image and graph data**: RC improved the accuracy of generative models for these types of data.
* **Protein models**: RC helped create more stable and accurate models for protein structures.

Overall, Radial Compensation offers a robust and reliable approach to building generative models on curved spaces, leading to more accurate and stable results."
