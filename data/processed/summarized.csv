category,title,summary,link,published,authors,summary_short
cs.LG,CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation,"Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move",https://arxiv.org/abs/2511.11522v1,2025-11-14T17:50:35Z,"Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra","**Converting Physical Chess to Digital: A Breakthrough AI Framework**

Imagine being able to take a photo of a physical chessboard and instantly get suggestions for your next move. Researchers have developed a deep learning framework called CVChess, which can convert images of chessboards into digital notation that can be fed into online chess engines.

The system uses a smartphone camera to capture an image of the board, which is then processed through several steps to identify the pieces on the board. A type of artificial neural network called a convolutional neural network (CNN) with residual layers is used to recognize the pieces, even in varying lighting conditions and angles.

The researchers tested their model on a dataset of 10,800 annotated images and achieved promising results. The system can encode the chessboard state into a digital format called Forsyth-Edwards Notation (FEN), which can be input into online chess engines to provide recommendations for the best next move.

This innovation has the potential to bridge the gap between physical and digital chess experiences, making it easier for players to analyze and improve their game using digital tools. With CVChess, chess enthusiasts can now access the benefits of digital chess analysis, regardless of whether they're playing on a physical or digital board."
cs.LG,Experience-Guided Adaptation of Inference-Time Reasoning Strategies,"Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",https://arxiv.org/abs/2511.11519v1,2025-11-14T17:45:28Z,"Adam Stein, Matthew Trager, Benjamin Bowman, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto","**Breakthrough in AI Problem-Solving: Experience-Guided Reasoner**

Imagine an artificial intelligence (AI) system that can learn from its experiences and adapt its problem-solving approach on the fly. Researchers have made a significant step towards achieving this goal with the development of the Experience-Guided Reasoner (EGuR).

EGuR is a new AI system that enables machines to generate customized strategies for solving complex problems. Unlike existing systems, EGuR can modify its approach dynamically, adjusting parameters, tools, and even switching between different problem-solving paradigms. This flexibility allows EGuR to learn from its experiences and improve its performance over time.

The researchers tested EGuR on five challenging benchmarks and found that it outperformed existing systems, achieving up to 14% accuracy improvements while reducing computational costs by up to 111 times. The more experience EGuR gained, the better it performed.

This breakthrough has the potential to enable more efficient and effective AI systems that can tackle complex problems in a wide range of applications. With EGuR, AI systems can adapt and learn from their experiences, leading to improved performance and reduced computational costs."
cs.LG,FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models,"Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.",https://arxiv.org/abs/2511.11505v1,2025-11-14T17:25:14Z,"Yonatan Dukler, Guihong Li, Deval Shah, Vikram Appia, Emad Barsoum","**Unlocking Efficient AI Model Training and Performance**

Researchers have made a significant breakthrough in improving the efficiency of large AI models, known as Mixture of Experts (MoE) models, when run on multiple computers. These models are crucial for various applications, but their performance is often hindered by communication delays between computers.

The team developed a new approach called FarSkip-Collective, which modifies the model architecture to enable simultaneous computation and communication. This innovation allows for faster training and inference (the process of making predictions) of large AI models.

In a remarkable achievement, the researchers successfully converted several state-of-the-art models, ranging from 16 billion to 109 billion parameters, to work with FarSkip-Collective. Notably, they converted a massive 109 billion parameter model, Llama 4 Scout, without sacrificing its accuracy. The modified model's performance was within 1% of the original model's accuracy across various evaluations.

The FarSkip-Collective approach not only preserves the accuracy of large AI models but also accelerates their training and inference times. This breakthrough has the potential to significantly enhance the efficiency and scalability of AI model development, enabling researchers and developers to build more powerful and accurate models."
cs.LG,Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation,"Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$λ$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $λ$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.",https://arxiv.org/abs/2511.11500v1,2025-11-14T17:20:45Z,"Mohamad Amin Mohamadi, Tianhao Wang, Zhiyuan Li","**The Problem with Language Models: When to Say ""I Don't Know""**

Language models, like those used in chatbots and virtual assistants, have become incredibly good at answering questions. However, they often struggle with a fundamental aspect of trustworthy intelligence: knowing when not to answer. These models tend to provide confident but incorrect answers, even when the stakes are high.

**The Research: A New Approach to Language Models**

Researchers have proposed a new approach called Reinforced Hesitation (RH) to address this issue. They modified the way language models are trained to include a ""don't know"" option, which allows the model to abstain from answering if it's unsure. The researchers found that by adjusting the penalty for incorrect answers, they could train models with varying levels of caution.

**The Findings: A New Way to Train Language Models**

The study showed that models trained with this approach can learn to balance accuracy with caution. The researchers also developed two strategies for using these models in practice: cascading, which routes queries through models with decreasing risk tolerance, and self-cascading, which re-queries the same model on abstention. Both strategies outperformed traditional methods, such as majority voting, while requiring less computational power.

**The Implications: A Step Towards Trustworthy Language Models**

The findings suggest that teaching language models to say ""I don't know"" when they're unsure can be a key aspect of building trustworthy intelligence. By prioritizing honesty over accuracy, these models can earn trust by being transparent about their limitations. This research has the potential to improve the reliability and safety of language models in a wide range of applications."
cs.LG,Learning and Testing Convex Functions,"We consider the problems of \emph{learning} and \emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.   In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:   - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\varepsilon$ using $n^{O(1/\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\mathrm{poly}(1/\varepsilon)}$ samples in the \emph{correlational statistical query (CSQ)} model.   - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\sqrt{n}/\varepsilon)^n$ samples.",https://arxiv.org/abs/2511.11498v1,2025-11-14T17:19:44Z,"Renato Ferreira Pinto, Cassandra Marcussen, Elchanan Mossel, Shivam Nadimpalli","**Unlocking the Secrets of Convex Functions**

Imagine you're trying to understand a complex shape, like a bowl or a hill. A convex function is a mathematical representation of such shapes, where the curve always bends upwards. But how can we learn and verify if a given function is indeed convex?

Researchers have made significant progress in solving this problem, specifically for high-dimensional convex functions that are smooth and continuous, like those found in Gaussian space. They developed two key algorithms:

1. **Learning Convex Functions**: A new algorithm can learn a convex function with a high degree of accuracy, using a reasonable number of samples (n^O(1/ε^2)). This algorithm is ""agnostic,"" meaning it doesn't rely on prior knowledge about the function. The researchers also established a lower bound, showing that any algorithm would require at least n^poly(1/ε) samples to achieve similar accuracy.
2. **Testing Convex Functions**: The researchers created two types of testers to verify if a function is convex. The first tester can detect both convex and non-convex functions with high accuracy, using the same number of samples as the learning algorithm. The second tester is more conservative, ensuring that it never incorrectly rejects a convex function, but requires more samples (O(√n/ε)^n).

These findings have significant implications for machine learning, statistics, and computer science, as they provide a foundation for working with convex functions in high-dimensional spaces. The researchers' work enables more efficient and accurate learning and testing of convex functions, which can lead to breakthroughs in various fields, such as optimization, signal processing, and data analysis."
cs.LG,Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models,"In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.",https://arxiv.org/abs/2511.11490v1,2025-11-14T17:09:01Z,"Joan Font-Quer Roset, Devina Mohan, Anna Scaife","Here's a summary of the research paper for a general audience:

**Understanding Radio Galaxy Images with AI**

Researchers have developed a new method to analyze images of radio galaxies, which are galaxies that emit strong radio waves. They used a type of artificial intelligence (AI) called a diffusion model to estimate the ""intrinsic dimension"" (iD) of a large dataset of radio galaxy images, known as Radio Galaxy Zoo (RGZ).

**What is intrinsic dimension?**

The intrinsic dimension is a measure of how complex or detailed an image is. Think of it like the number of features or patterns that make up an image. For example, a simple image of a circle might have a low intrinsic dimension, while a complex image of a galaxy with many stars and features might have a higher intrinsic dimension.

**What did the researchers find?**

The researchers found that the images in the RGZ dataset have a higher intrinsic dimension than typical natural images, such as photos of animals or landscapes. This means that radio galaxy images are more complex and have more features than everyday images.

They also discovered that images that are very different from the norm (called ""out-of-distribution"" sources) have a higher intrinsic dimension. Additionally, they found a weak relationship between the intrinsic dimension and the signal-to-noise ratio (SNR) of the images, which measures how clear or noisy the image is.

**What does this mean for future research?**

The researchers suggest that their findings could help improve the way AI algorithms learn to represent and understand radio galaxy images. This could lead to better classification and analysis of these images, which could in turn help scientists learn more about the galaxies themselves.

Overall, this study demonstrates the power of AI in analyzing complex astronomical data and paves the way for future research in this field."
cs.LG,Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys,"Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.",https://arxiv.org/abs/2511.11485v1,2025-11-14T17:01:02Z,"Alinda Ezgi Gerçek, Till Korten, Paul Chekhonin, Maleeha Hassan, Peter Steinbach","**Breakthrough in Analyzing Steel Alloys: AI Model Accurately Identifies Carbide Microstructures**

Researchers have developed a powerful AI model that can accurately identify and segment carbide microstructures in steel alloys from scanning electron microscopy (SEM) images. This is crucial for predicting the mechanical properties of steel, as carbides can both strengthen the alloy and cause cracks.

The innovative model, called a U-Net, was trained on just 10 annotated SEM images, a remarkably small dataset. Despite this limited data, the model achieved an impressive accuracy of 0.98, outperforming traditional image analysis techniques (0.85) and reducing the need for manual annotation by a significant amount.

This breakthrough has significant implications for alloy design and materials science. The AI model enables rapid and automated quantification of carbide microstructures, which can help researchers and engineers design stronger and more durable steel alloys. Moreover, this approach can be applied to other types of steel, demonstrating the potential of data-efficient deep learning in materials analysis.

**In simple terms:** This AI model helps analyze steel alloys more efficiently and accurately, which can lead to the development of stronger and more durable materials."
cs.LG,Inferring response times of perceptual decisions with Poisson variational autoencoders,"Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",https://arxiv.org/abs/2511.11480v1,2025-11-14T16:58:04Z,"Hayden R. Johnson, Anastasia N. Krouglova, Hadi Vafaii, Jacob L. Yates, Pedro J. Gonçalves","Here's a summary of the research paper for a general audience:

**Understanding How We Make Quick Decisions**

When we look at something, like a picture or a word, our brain quickly processes the information and helps us make a decision, such as identifying what we're looking at. But have you ever wondered how our brain decides when to stop processing information and make a choice? Researchers have developed a new model that tries to answer this question.

The model uses a type of artificial intelligence called a ""Poisson variational autoencoder"" to simulate how our brain's neurons work together to process visual information. The model then uses this information to make decisions, like classifying a picture of a digit.

What's exciting about this model is that it can generate realistic patterns of decision-making, including:

* How long it takes us to make a decision (response times)
* The variability in our choices and response times
* How the number of options affects our response times (e.g., it's harder to choose from many options)
* The trade-off between speed and accuracy

The researchers tested their model on a simple task, classifying pictures of digits (like 0-9). The model performed well and produced results that matched what we know about human decision-making. This work has implications for understanding how our brains make quick decisions and could lead to new insights into decision-making processes."
cs.LG,Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations,"Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.",https://arxiv.org/abs/2511.11472v1,2025-11-14T16:42:42Z,"Sooyong Jang, Insup Lee","**Improving Machine Learning Predictions: A New Approach to Adaptivity**

Machine learning models can make predictions, but they often lack a crucial aspect: reliability. Conformal prediction is a technique that addresses this by providing a set of possible labels instead of a single prediction, along with a guarantee of how likely it is to be correct. However, it's also important for these predictions to be adaptive, meaning that the model should be more confident (and provide a smaller set of labels) for easy examples and less confident (and provide a larger set of labels) for harder examples.

The problem is that current methods for evaluating adaptivity have limitations. They often group examples by difficulty and then calculate the model's performance, but this can lead to inaccurate results if the groups are imbalanced. To address this, researchers have developed a new method that uses input transformations to sort examples by difficulty and then groups them into balanced bins.

Using this approach, the researchers have introduced two new metrics that better evaluate a model's adaptivity. They have also developed a new algorithm that takes into account the estimated difficulty of each example and adjusts the prediction sets accordingly. Experiments on image classification and medical tasks show that this new approach outperforms existing methods, providing more accurate and reliable predictions.

**In Simple Terms:** Imagine you're trying to recognize objects in a picture. A good machine learning model should be confident when it sees a clear picture of a cat, but less confident when it sees a blurry picture. This new approach helps machine learning models provide more accurate and reliable predictions by adjusting their confidence level based on the difficulty of the example."
cs.LG,Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates,"Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.",https://arxiv.org/abs/2511.11466v1,2025-11-14T16:38:15Z,"Dmitry Kovalev, Ekaterina Borodich","**Unlocking the Power of Non-Euclidean SGD: A Breakthrough in Optimization**

Imagine you're trying to find the best route to get to your destination. You could use a map to guide you, but what if the map isn't a perfect representation of the real world? That's similar to the challenge faced by optimization algorithms, like those used to train deep neural networks. These algorithms need to navigate complex mathematical landscapes to find the best solution.

Recently, a new class of optimization algorithms called non-Euclidean SGD (Stochastic Gradient Descent) has gained popularity. These algorithms, including SignSGD, Lion, and Muon, have shown impressive results in training deep neural networks. However, their theoretical foundations have been unclear - until now.

Researchers have developed a new unified analysis that explains why non-Euclidean SGD algorithms work so well. The key insight is that these algorithms can take advantage of the structure of the problem, such as sparsity or low-rank properties. This allows them to perform better than traditional Euclidean SGD algorithms.

The study shows that non-Euclidean SGD algorithms can:

* Leverage problem structure to improve performance
* Benefit from popular techniques like extrapolation and momentum variance reduction
* Achieve state-of-the-art convergence rates, matching those of more complex algorithms

In simple terms, non-Euclidean SGD algorithms are like GPS navigation systems that can adapt to the specific terrain of the problem, leading to faster and more efficient convergence. This breakthrough provides a solid theoretical foundation for the use of non-Euclidean SGD algorithms in optimization problems, paving the way for further improvements in fields like deep learning."
cs.LG,Adaptive Intrusion Detection for Evolving RPL IoT Attacks Using Incremental Learning,"The routing protocol for low-power and lossy networks (RPL) has become the de facto routing standard for resource-constrained IoT systems, but its lightweight design exposes critical vulnerabilities to a wide range of routing-layer attacks such as hello flood, decreased rank, and version number manipulation. Traditional countermeasures, including protocol-level modifications and machine learning classifiers, can achieve high accuracy against known threats, yet they fail when confronted with novel or zero-day attacks unless fully retrained, an approach that is impractical for dynamic IoT environments. In this paper, we investigate incremental learning as a practical and adaptive strategy for intrusion detection in RPL-based networks. We systematically evaluate five model families, including ensemble models and deep learning models. Our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats, all while reducing training time compared to full retraining. By combining five diverse models with attack-specific analysis, forgetting behavior, and time efficiency, this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving RPL-based IoT networks.",https://arxiv.org/abs/2511.11464v1,2025-11-14T16:35:48Z,"Sumeyye Bas, Kiymet Kaya, Elif Ak, Sule Gunduz Oguducu","**Protecting Internet of Things (IoT) Networks from Evolving Cyber Threats**

The Internet of Things (IoT) is a network of physical devices, vehicles, and other items that are embedded with sensors, software, and connectivity, allowing them to collect and exchange data. However, the growing number of IoT devices has also increased the risk of cyber attacks. A key challenge in securing IoT networks is detecting and preventing malicious activity, particularly in resource-constrained systems.

Researchers have investigated a new approach to detecting cyber attacks on IoT networks, specifically those using the Routing Protocol for Low-Power and Lossy Networks (RPL). RPL is a widely used protocol for IoT networks, but its lightweight design makes it vulnerable to various types of attacks.

The researchers tested five different machine learning models to see how well they could detect new types of attacks on RPL networks. They found that using a technique called incremental learning, which allows the models to learn from new data without having to be completely retrained, can help to improve detection performance. This approach enables the models to adapt to new threats and reduces the need for frequent retraining.

The study's findings suggest that incremental learning is a promising strategy for maintaining robust security in IoT networks. By using this approach, IoT systems can stay protected against evolving cyber threats, even as new types of attacks emerge. This research has important implications for the development of more secure and resilient IoT networks."
cs.LG,MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture,"We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.   The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.",https://arxiv.org/abs/2511.11462v1,2025-11-14T16:35:14Z,"Kevin Chen, Kenneth W. Parker, Anish Arora","Here's a summary of the research paper for a general audience:

**Converting Body Movements to Radar Signatures**

Imagine a system that can translate human movements into radar signatures, which are like patterns that radar systems use to detect and track objects. Researchers have developed a machine learning model called MoCap2Radar that can do just that. The model uses data from motion capture systems, which track the movements of a person's body, to generate radar signatures.

**How it Works**

The MoCap2Radar model uses a type of artificial intelligence called a transformer to analyze the movements of a person's body over time. It looks at how different parts of the body move in relation to each other and how these movements change over time. This allows the model to generate radar signatures that are similar to those produced by real radar systems.

**Why it Matters**

This technology has several potential applications, including:

* **Edge Computing and IoT Radars**: The model can be used in edge computing and Internet of Things (IoT) radars, which are small, low-power radar systems that can be used in a variety of applications, such as smart homes and cities.
* **Augmenting Radar Datasets**: The model can also be used to augment scarce radar datasets using more abundant motion capture data. This can help train higher-level applications, such as object detection and tracking systems.
* **Efficient Computation**: The model requires much less computation than traditional physics-based methods for generating radar data, making it a more efficient and practical solution.

**What's Next**

The researchers behind this project are excited about the potential applications of their technology, including the possibility of using it to improve radar systems for tracking and detecting objects. They also see potential for using this technology in fields such as robotics, autonomous vehicles, and healthcare."
cs.LG,Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies,"Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.",https://arxiv.org/abs/2511.11461v1,2025-11-14T16:32:42Z,"Riku Green, Huw Day, Zahraa S. Abdallah, Telmo M. Silva Filho","**Breaking Down Errors in Time Series Forecasting**

Time series forecasting is a crucial task in many fields, such as finance, weather prediction, and energy management. It involves predicting future values in a sequence of data based on past patterns. There are two main approaches to multi-step forecasting: recursive and direct strategies. A common understanding is that recursive strategies tend to have high bias (consistently under or overestimating) and low variance (consistent results), while direct strategies have low bias and high variance.

However, a recent study challenges this conventional wisdom by breaking down the errors in multi-step forecasting into three components:

1. **Irreducible noise**: the inherent uncertainty in the data that cannot be reduced.
2. **Structural approximation gap**: the error introduced by the model's inability to perfectly capture the underlying patterns.
3. **Estimation variance**: the error due to the uncertainty in estimating the model's parameters.

The study found that:

* For simple linear models, the structural gap is zero, and the errors are mainly due to irreducible noise and estimation variance.
* For more complex nonlinear models, the recursive strategy can lead to a larger structural gap, but also a higher estimation variance.
* The study proposes a new way to understand the trade-offs between recursive and direct strategies, based on the model's nonlinearity and noise characteristics.

The findings suggest that the choice between recursive and direct strategies should not be based solely on traditional bias-variance intuition. Instead, practitioners should consider the specific characteristics of their model and data to make an informed decision. This research provides practical guidance for improving the accuracy of time series forecasting."
cs.LG,FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression,"There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.",https://arxiv.org/abs/2511.11459v1,2025-11-14T16:31:21Z,"Xiaoyin Xi, Zhe Yu","**Ensuring Fairness in AI: A New Approach to Regression Analysis**

As AI software becomes increasingly prevalent in high-stakes decision-making, concerns about fairness and bias have grown. While most research has focused on fairness in classification tasks, such as predicting yes/no outcomes, fairness in regression tasks - which predict continuous outcomes, like scores or probabilities - has been largely overlooked.

A team of researchers has developed a new framework, called FairReweighing, to address fairness in regression analysis. Their approach uses density estimation to reweight the data, ensuring that the learned model treats all groups fairly, regardless of racial, gender, or age characteristics.

The researchers tested FairReweighing on both synthetic and real-world data and found that it outperformed existing state-of-the-art solutions in improving separation - a key aspect of fairness - while maintaining high accuracy. The proposed algorithm provides a theoretical guarantee of fairness in the training data, assuming data independence.

This breakthrough has significant implications for AI development, as it provides a new tool for ensuring fairness in regression analysis, a crucial aspect of many high-stakes decision-making applications. By using FairReweighing, developers can create more equitable AI systems that promote fairness and reduce bias."
cs.LG,Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer,"Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.",https://arxiv.org/abs/2511.11452v1,2025-11-14T16:20:44Z,"Seth Alain Chang, Muhammad Mueez Amjad, Noorul Wahab, Ethar Alzaid, Nasir Rajpoot, Adam Shephard","**Unlocking the Power of Multimodal Deep Learning in Prostate Cancer Research**

Imagine a world where doctors can accurately predict the likelihood of prostate cancer returning after treatment. A recent study in the field of computational pathology takes a significant step towards making this a reality. Researchers explored the concept of multimodal deep learning, which combines information from multiple sources, such as medical images and patient data, to make predictions about patient outcomes.

The study focused on predicting the time it takes for prostate cancer to recur after treatment, a crucial aspect of patient care. The researchers asked a fundamental question: does combining multiple sources of information always lead to better predictions? Their findings revealed a surprising answer: it depends on the quality of the individual sources of information.

When combining high-quality sources, such as detailed medical images and patient data, the predictions became more accurate. However, when a weak or low-quality source was added to the mix, the predictions actually became less accurate. This is because the weak source introduced ""noise"" or irrelevant information that interfered with the predictions.

The study's results have significant implications for the design of multimodal deep learning models in medical research. They suggest that simply combining multiple sources of information is not enough; instead, researchers must carefully select and integrate high-quality sources to achieve the best results. This approach has the potential to improve predictions and ultimately lead to better patient outcomes in prostate cancer and other diseases. By harnessing the power of multimodal deep learning, researchers can develop more accurate and effective tools for predicting patient outcomes and improving healthcare."
cs.LG,VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation,"We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell",https://arxiv.org/abs/2511.11450v1,2025-11-14T16:20:07Z,"Maximilian Rokuss, Moritz Langenberg, Yannick Kirchhoff, Fabian Isensee, Benjamin Hamm, Constantin Ulrich, Sebastian Regnery, Lukas Bauer, Efthimios Katsigiannopulos, Tobias Norajitra, Klaus Maier-Hein","**Breakthrough in Medical Imaging: AI Model Enables Precise 3D Segmentation with Simple Text Prompts**

Researchers have developed a cutting-edge AI model called VoxTell, which can accurately segment 3D medical images using simple text prompts. This innovative technology has the potential to revolutionize medical imaging analysis.

**What does it do?**
VoxTell can take a short text description, such as a single word or a clinical sentence, and use it to identify specific areas of interest in 3D medical images, like CT, MRI, or PET scans. This allows doctors to quickly and precisely segment images, which is essential for diagnosis and treatment.

**How does it work?**
The VoxTell model was trained on a massive dataset of over 62,000 medical images, covering more than 1,000 different types of anatomical and pathological classes. It uses a sophisticated technique called vision-language fusion to align textual and visual features at multiple scales.

**What's remarkable about VoxTell?**
This AI model achieves state-of-the-art performance, even when faced with images and text prompts it has never seen before. It can generalize to related but unseen classes, making it a robust and reliable tool for medical professionals. Additionally, VoxTell demonstrates strong cross-modality transfer, meaning it can adapt to different types of medical images, and is robust to linguistic variations and clinical language.

**Why is this important?**
VoxTell has the potential to streamline medical imaging analysis, making it faster and more accurate. This can lead to better diagnosis, treatment, and patient outcomes. The code for VoxTell is now available, making it accessible to the medical and research communities for further development and application."
cs.LG,DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference,"Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.",https://arxiv.org/abs/2511.11446v1,2025-11-14T16:14:58Z,"Farhana Amin, Sabiha Afroz, Kanchon Gharami, Mona Moghadampanah, Dimitrios S. Nikolopoulos","Here's a summary of the research paper for a general audience:

**Making AI Image Generation Faster and More Efficient**

Artificial intelligence (AI) models that generate images, known as diffusion models, can produce high-quality images but are often slow and require a lot of computing power. This is because they need to perform many complex calculations to refine the image.

Researchers have developed a new framework called DiffPro, which makes these AI models more efficient without sacrificing image quality. DiffPro works by optimizing two key aspects of the model: the number of steps it takes to generate an image and the level of precision required for each step.

**Key Achievements:**

* DiffPro can reduce the number of steps needed to generate an image by up to 50%.
* It can compress the model to use up to 6.25 times less memory.
* It can speed up the image generation process by up to 2.8 times.

**What's Important About DiffPro:**

DiffPro is a practical solution that can be applied to existing AI models without requiring any additional training. It's also designed to work with the specific hardware used in real-world applications, making it a useful tool for deploying AI models in a wide range of settings. By making AI image generation faster and more efficient, DiffPro has the potential to enable new applications and improve the overall user experience."
cs.LG,Retrofit: Continual Learning with Bounded Forgetting for Security Applications,"Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.   We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.",https://arxiv.org/abs/2511.11439v1,2025-11-14T16:07:03Z,"Yiling He, Junchi Lei, Hongyu She, Shuo Shao, Xinran Zheng, Yiping Liu, Zhan Qin, Lorenzo Cavallaro","**Improving AI Security Models with RETROFIT**

Artificial intelligence (AI) plays a crucial role in modern security analytics, but its performance can degrade over time as threats and data change. To address this issue, researchers have proposed a new method called RETROFIT, which enables AI models to learn continuously without requiring access to old data.

The RETROFIT method allows AI models to consolidate old and new knowledge without needing to retrain on historical data. This is achieved through a parameter-level merging technique that combines the strengths of previously trained and newly fine-tuned models. To minimize interference between old and new knowledge, RETROFIT applies low-rank and sparse updates that confine parameter changes to independent subspaces. Additionally, a knowledge arbitration mechanism dynamically balances the contributions of the old and new models based on their confidence.

**Key Benefits of RETROFIT**

* **Mitigates Forgetting**: RETROFIT consistently reduces the forgetting of old knowledge, allowing AI models to retain their effectiveness over time.
* **Maintains Adaptability**: RETROFIT enables AI models to adapt to new data and threats, ensuring they remain effective in changing environments.
* **Improved Performance**: RETROFIT achieves significant improvements in two security applications: malware detection and binary summarization.

**Real-World Impact**

The RETROFIT method has the potential to significantly improve the performance and reliability of AI-powered security analytics. By enabling AI models to learn continuously without requiring access to old data, RETROFIT can help organizations stay ahead of evolving threats and maintain the effectiveness of their security systems."
cs.LG,BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning,"Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace"" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.",https://arxiv.org/abs/2511.11421v1,2025-11-14T15:51:40Z,"Lan Li, Tao Hu, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan","**Breakthrough in AI Learning: A New Method for Continual Learning**

Imagine you're trying to teach a computer to recognize different types of animals, but you don't want it to forget what it learned previously. This is a challenge in AI known as Class-Incremental Learning (CIL). Researchers have been exploring ways to use powerful vision-language models like CLIP to tackle this problem.

The new method, called BOFA, offers a promising solution. BOFA uses CLIP's existing abilities to learn new categories without forgetting old ones, and it does so without adding extra complexity to the model. This is achieved by adapting the model's ""bridge-layer"" - a critical component that connects visual and textual information.

BOFA's key innovations include:

* **Efficient adaptation**: BOFA adapts to new tasks without adding extra parameters or slowing down the model's performance.
* **Preventing forgetting**: BOFA uses a mathematical technique to prevent the model from forgetting what it learned previously, ensuring stable knowledge accumulation.
* **Improved classification**: BOFA combines visual and textual information to enhance classification performance.

Extensive tests on standard benchmarks show that BOFA outperforms existing methods in terms of accuracy and efficiency. This breakthrough has significant implications for AI applications that require continual learning, such as image recognition, natural language processing, and more."
cs.LG,"Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching","Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.",https://arxiv.org/abs/2511.11418v1,2025-11-14T15:49:36Z,"Dara Varam, Diaa A. Abuhani, Imran Zualkernan, Raghad AlDamani, Lujain Khalil","**Advancing AI Efficiency: A Breakthrough in Quantizing Generative Models**

Generative models, a type of artificial intelligence (AI), have shown great promise in simulating complex systems and generating realistic data. However, these models often require a large number of precise parameters, making them difficult to deploy on devices with limited computing resources. A recent study has made a significant breakthrough in addressing this challenge.

The researchers developed a new method called optimal transport (OT)-based quantization, which reduces the precision of the model's parameters while preserving its performance. They applied this method to a specific type of generative model called Flow Matching (FM) and compared it to other quantization schemes.

The study found that OT-based quantization outperformed other methods, maintaining the model's ability to generate high-quality data even when the precision of the parameters was reduced to just 2-3 bits per parameter. This is a significant improvement over existing methods, which often fail at such low precision levels.

The implications of this research are substantial. By enabling the deployment of generative models on devices with limited computing resources, such as smartphones or embedded systems, OT-based quantization can facilitate the widespread adoption of AI in various applications, including edge AI and embedded systems. This breakthrough has the potential to transform the way we interact with AI and unlock new possibilities for AI-powered innovation."
cs.CV,Bridging Hidden States in Vision-Language Models,"Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities ""think"". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.",https://arxiv.org/abs/2511.11526v1,2025-11-14T17:55:25Z,"Benjamin Fein-Ashley, Jacob Fein-Ashley","**Unlocking Better Understanding between Images and Text**

Imagine you're trying to describe a picture to someone who can't see it. You might point out specific objects, like a cat on a couch, and use words to explain what's happening. Computers are getting better at understanding images and text, but they're not always good at connecting the two. Researchers have developed a new approach called BRIDGE, which helps computers align what they ""see"" in an image with what they understand from text.

The key insight is that both images and text have their own internal structures, like the layout of objects in an image or the syntax of a sentence. BRIDGE uses a lightweight module to help computers compare these internal structures directly, allowing them to better understand the relationships between images and text.

In tests, BRIDGE outperformed other models that try to connect images and text, while also being more efficient. This approach could lead to improvements in applications like image search, visual question answering, and more. The researchers have made their code publicly available, which could help accelerate progress in this area."
cs.CV,CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation,"Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move",https://arxiv.org/abs/2511.11522v1,2025-11-14T17:50:35Z,"Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra","**Converting Physical Chess to Digital: A Breakthrough AI Framework**

Imagine being able to take a photo of a physical chessboard and instantly get suggestions for your next move. Researchers have developed a deep learning framework called CVChess, which can convert images of chessboards into a digital format that can be analyzed by online chess engines.

The system uses a smartphone camera to capture an image of the board, which is then processed through several steps to identify the pieces on the board. A type of artificial neural network called a convolutional neural network (CNN) with residual layers is used to recognize the pieces, even in varying lighting conditions and angles.

The researchers tested their framework using a dataset of 10,800 annotated images and achieved promising results. The system can encode the chessboard state into a digital format called Forsyth-Edwards Notation (FEN), which can be fed into online chess engines to provide recommendations for the best next move.

This innovation has the potential to bridge the gap between physical and digital chess experiences, making it easier for players to analyze and improve their game using digital tools. With CVChess, chess enthusiasts can now access the benefits of digital chess analysis, regardless of whether they're playing on a physical or digital board."
cs.CV,"Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities","Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.",https://arxiv.org/abs/2511.11512v1,2025-11-14T17:34:20Z,"Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen","**Breakthrough in Multimodal Learning: TLV-CoRe Enhances Robot Perception**

Imagine robots that can not only see and understand language but also feel and grasp objects with precision. A recent research paper introduces TLV-CoRe, a novel method that combines tactile, language, and vision data to improve robot perception. Tactile sensing provides valuable information about an object's texture, shape, and properties, which is essential for tasks like grasping and manipulation.

The challenge lies in integrating data from different sensors, which often produce redundant and incompatible features. TLV-CoRe addresses this issue by:

1. **Unifying tactile features**: A Sensor-Aware Modulator ensures that tactile data from various sensors is consistent and comparable.
2. **Enhancing multimodal interaction**: A Unified Bridging Adapter facilitates communication between tactile, language, and vision modalities, enabling a more comprehensive understanding of objects.

The researchers also propose a new evaluation framework, RSS, to assess the performance of tactile models. Experimental results show that TLV-CoRe significantly improves:

* **Sensor-agnostic representation learning**: The ability to learn from one sensor and apply it to others.
* **Cross-modal alignment**: The integration of tactile, language, and vision data.

This breakthrough has the potential to advance robotics and artificial intelligence, enabling robots to better understand and interact with their environment. The TLV-CoRe method offers a new direction for multimodal tactile representation, paving the way for more sophisticated and capable robots."
cs.CV,OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning,"Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.",https://arxiv.org/abs/2511.11510v1,2025-11-14T17:31:18Z,"Xiaoyu Zheng, Xu Chen, Awais Rauf, Qifan Fu, Benedetta Monosi, Felice Rivellese, Myles J. Lewis, Shaogang Gong, Gregory Slabaugh","**Breakthrough in Ultrasound Image Analysis: OpenUS Revolutionizes Medical Imaging**

Ultrasound imaging is a widely used medical tool due to its affordability, portability, and safety. However, interpreting ultrasound images can be challenging, as it requires expertise and can vary greatly depending on the body part being imaged, the device used, and the technician's skill level. These variations make it difficult to develop artificial intelligence (AI) models that can accurately analyze ultrasound images.

To address this challenge, researchers have developed OpenUS, the first open-source foundation model for ultrasound image analysis. OpenUS is a cutting-edge AI model that can learn from a large dataset of ultrasound images and adapt to different types of images and tasks. The model uses a novel technique called self-adaptive masked contrastive learning, which allows it to focus on the most clinically relevant parts of the images.

The researchers compiled a massive dataset of over 308,000 ultrasound images from 42 publicly available sources, covering various body parts, institutions, devices, and disease types. They then trained OpenUS on this dataset, enabling it to extract rich features and learn from the images.

The OpenUS model has the potential to revolutionize ultrasound image analysis by:

* Improving the accuracy and efficiency of ultrasound image interpretation
* Enabling label-efficient fine-tuning for specific tasks, reducing the need for large amounts of labeled data
* Facilitating the development of more accurate AI models for medical imaging

The code for OpenUS is now publicly available, making it accessible to researchers and clinicians worldwide. This breakthrough has the potential to improve healthcare outcomes and advance the field of medical imaging."
cs.CV,PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models,"Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.",https://arxiv.org/abs/2511.11502v1,2025-11-14T17:23:55Z,"Nhat Hoang-Xuan, Minh Vu, My T. Thai, Manish Bhattarai","**Detecting Object Hallucinations in AI Models: A New Breakthrough**

Large vision-language models (LVLMs) are AI systems that can process both images and text. While they're powerful, they can sometimes produce unreliable results, including ""object hallucinations"" - where the model claims to see objects in an image that aren't really there.

Researchers have discovered that when LVLMs hallucinate, they often ignore the actual image and instead rely on their previous outputs to infer new objects. To detect this behavior, the team introduced a new metric called the Prelim Attention Score (PAS). PAS is a simple, efficient way to identify when an LVLM is likely to be hallucinating.

The good news is that PAS can be computed in real-time, without requiring additional computations or training. Tests showed that PAS is highly effective in detecting object hallucinations across multiple models and datasets. This breakthrough has the potential to improve the reliability of LVLMs and enable real-time filtering and intervention to prevent hallucinations."
cs.CV,Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images,"Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.",https://arxiv.org/abs/2511.11486v1,2025-11-14T17:05:13Z,"Roman Kinakh, Gonzalo R. Ríos-Muñoz, Arrate Muñoz-Barrutia","Here's a summary of the research paper for a general audience:

**Title:** A New Way to Analyze Cancer Images for Better Treatment

**Summary:** Researchers have developed a new method to analyze images of cancer tissue to predict how well a patient will respond to a certain type of cancer treatment called immunotherapy. Currently, doctors use a labor-intensive process to assess the expression of a protein called PD-L1, which helps determine if a patient is likely to benefit from immunotherapy. The new method uses artificial intelligence to analyze standard histology images (H&E-stained) to infer PD-L1 expression, providing accurate results and uncertainty estimates.

**Key Findings:** The researchers tested their method on a dataset of lung cancer images and found that it performed well, with accuracy comparable to existing methods. Additionally, their method provides a map of uncertainty for each image, which can help doctors understand how confident they should be in the results. The uncertainty estimates were strongly correlated with errors in the segmentation, suggesting that this method could be a useful tool for doctors to make more informed decisions.

**Implications:** This research has the potential to make cancer treatment more efficient and effective by providing a less resource-intensive way to assess PD-L1 expression. The method could also provide doctors with more information about the reliability of the results, which could lead to better treatment outcomes."
cs.CV,ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation,"Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.",https://arxiv.org/abs/2511.11483v1,2025-11-14T17:00:29Z,"Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang","Here's a summary of the research paper for a general audience:

**Introducing ImAgent: A Smarter Way to Generate Images**

Imagine you're trying to generate an image based on a text description, but the result doesn't quite match what you had in mind. This can happen because the description is vague or the image generation model gets it wrong. To fix this, researchers have been trying different approaches, but they often require extra modules and can be slow and inefficient.

A team of researchers has now developed ImAgent, a new framework that combines image generation, reasoning, and self-evaluation into one system. This allows it to generate images that are more accurate and match the text description better, without needing extra modules or being slow.

ImAgent works by using a ""policy controller"" to guide the image generation process. This controller helps the system to dynamically adjust and refine the image until it meets the desired quality. The best part is that ImAgent doesn't require any additional training or external models, making it efficient and scalable.

The researchers tested ImAgent on various image generation and editing tasks and found that it consistently produced better results than the existing models. This breakthrough has the potential to revolutionize the way we generate images, making it faster, more efficient, and more accurate."
cs.CV,Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective,"As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.",https://arxiv.org/abs/2511.11478v1,2025-11-14T16:56:01Z,"Nhat Chung, Taisei Hanyu, Toan Nguyen, Huy Le, Frederick Bumgarner, Duy Minh Ho Nguyen, Khoa Vo, Kashu Yamazaki, Chase Rainwater, Tung Kieu, Anh Nguyen, Ngan Le","Here's a summary of the research paper for a general audience:

**Teaching Robots to Remember and Understand Objects**

Imagine you're trying to assemble a piece of furniture with many similar-looking parts. You need to keep track of which parts you've already used, where they went, and how they changed. This is a challenge for robots too. As robots operate in complex environments, they need to be able to perceive, track, and reason about individual objects over time.

Researchers have created a new task suite called LIBERO-Mem to test robots' ability to remember and understand objects. They found that current vision-language-action models struggle with this task, especially when dealing with long sequences of interactions.

To address this challenge, the researchers proposed a new framework called Embodied-SlotSSM. This framework helps robots maintain a consistent understanding of objects over time by:

1. Keeping track of short-term history of object interactions
2. Aligning object information with action planning

The results show that Embodied-SlotSSM performs well on LIBERO-Mem and other tasks, offering a scalable solution for robots to reason about objects and make decisions in complex environments. This research has implications for developing more sophisticated robotic manipulation systems that can interact with objects in a more human-like way."
cs.CV,Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery,"Recent advances in generative modeling have substantially enhanced 3D urban generation, enabling applications in digital twins, virtual cities, and large-scale simulations. However, existing methods face two key challenges: (1) the need for large-scale 3D city assets for supervised training, which are difficult and costly to obtain, and (2) reliance on semantic or height maps, which are used exclusively for generating buildings in virtual worlds and lack connection to real-world appearance, limiting the realism and generalizability of generated cities. To address these limitations, we propose Sat2RealCity, a geometry-aware and appearance-controllable framework for 3D urban generation from real-world satellite imagery. Unlike previous city-level generation methods, Sat2RealCity builds generation upon individual building entities, enabling the use of rich priors and pretrained knowledge from 3D object generation while substantially reducing dependence on large-scale 3D city assets. Specifically, (1) we introduce the OSM-based spatial priors strategy to achieve interpretable geometric generation from spatial topology to building instances; (2) we design an appearance-guided controllable modeling mechanism for fine-grained appearance realism and style control; and (3) we construct an MLLM-powered semantic-guided generation pipeline, bridging semantic interpretation and geometric reconstruction. Extensive quantitative and qualitative experiments demonstrate that Sat2RealCity significantly surpasses existing baselines in structural consistency and appearance realism, establishing a strong foundation for real-world aligned 3D urban content creation. The code will be released soon.",https://arxiv.org/abs/2511.11470v1,2025-11-14T16:42:03Z,"Yijie Kang, Xinliang Wang, Zhenyu Wu, Yifeng Shi, Hailong Zhu","**Breakthrough in 3D City Generation: Creating Realistic Virtual Cities from Satellite Imagery**

Imagine being able to create detailed, realistic virtual cities for use in video games, simulations, and urban planning. Researchers have made a significant step towards making this a reality with the development of Sat2RealCity, a new framework that generates 3D urban environments from real-world satellite imagery.

The challenge in creating virtual cities lies in the need for large amounts of 3D data to train AI models, which is difficult and expensive to obtain. Existing methods also rely on simplified maps that don't accurately reflect real-world appearances. Sat2RealCity addresses these limitations by using satellite images to generate 3D cities, allowing for more realistic and diverse virtual environments.

The Sat2RealCity framework consists of three key innovations:

1. **Geometry-aware generation**: It uses spatial information from satellite images to create buildings and cities with accurate shapes and structures.
2. **Appearance control**: It allows for fine-grained control over the appearance of generated buildings and cities, enabling realistic textures, colors, and styles.
3. **Semantic-guided generation**: It uses a large language model to interpret the meaning of satellite images and generate 3D cities that match the real-world semantics.

The results are impressive, with Sat2RealCity outperforming existing methods in terms of structural consistency and appearance realism. This breakthrough has the potential to revolutionize the creation of virtual cities, enabling applications in fields such as urban planning, architecture, and entertainment."
cs.CV,Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents,"The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.",https://arxiv.org/abs/2511.11468v1,2025-11-14T16:41:10Z,"Davide Napolitano, Luca Cagliero, Fabrizio Battiloro","**Can AI Models Handle Tricky Questions on Complex Documents?**

Imagine you're trying to ask a question about a document that includes both text and images. A new type of AI model, called Visual Large Language Models (VLLMs), is designed to understand and answer questions about these types of documents. But what happens when the question can't be answered, even if it seems like it should be?

Researchers have created a new benchmark, called VRD-UQA, to test how well VLLMs can detect when a question can't be answered. They generated tricky questions by subtly changing the wording or replacing certain words with similar ones, making the questions seem valid but unanswerable.

The study tested 12 VLLMs and found that they have limitations in detecting unanswerable questions, especially when the questions are corrupted in certain ways. The researchers also explored different strategies to help VLLMs improve their performance.

The findings of this study can help develop more robust AI models that can better handle complex documents and tricky questions. The VRD-UQA benchmark provides a new tool for evaluating and improving the performance of VLLMs, which can have applications in areas such as document analysis, information retrieval, and more."
cs.CV,Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification,"Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.",https://arxiv.org/abs/2511.11460v1,2025-11-14T16:31:37Z,"Qinghao Gao, Jianhai Qu, Yunsong Li, Weiqiang Dong","**Improving Classification in Remote Sensing with a New AI Framework**

Remote sensing technologies, such as satellite imaging, often struggle with incomplete data due to environmental interference, sensor failures, or atmospheric effects. This can significantly reduce the accuracy of classification models, which are used to identify features or patterns in the data. To address this challenge, researchers have developed a new AI framework called Missing-aware Mixture-of-Loras (MaMOL).

**What is MaMOL?**

MaMOL is a novel approach that reformulates the problem of missing data as a multi-task learning problem. It uses a dual-routing mechanism to adaptively activate experts for different missing patterns and maintain stable cross-modal knowledge sharing. This allows MaMOL to efficiently adapt to different types of missing data, without requiring separate networks for each scenario.

**How does MaMOL work?**

MaMOL's framework consists of two main components:

1. **Task-oriented dynamic router**: This component adaptively activates experts for different missing patterns, allowing the model to focus on the most relevant information for each specific task.
2. **Modality-specific-shared static router**: This component maintains stable cross-modal knowledge sharing, ensuring that the model can leverage information from multiple sources, even when some data is missing.

**Key benefits of MaMOL**

The MaMOL framework offers several advantages over existing methods:

* **Improved robustness and generalization**: MaMOL demonstrates superior performance under varying missing rates, making it a reliable solution for real-world applications.
* **Minimal computational overhead**: MaMOL achieves efficient adaptation with minimal computational costs, making it suitable for large-scale remote sensing applications.
* **Scalability and cross-domain applicability**: MaMOL can be applied to other domains, such as natural image datasets, highlighting its potential as a general and efficient solution for incomplete multimodal learning.

**Conclusion**

The MaMOL framework offers a promising solution for improving classification accuracy in remote sensing applications with incomplete data. Its ability to adapt to different types of missing data, while maintaining efficiency and scalability, makes it an attractive approach for a wide range of applications."
cs.CV,Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer,"Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.",https://arxiv.org/abs/2511.11452v1,2025-11-14T16:20:44Z,"Seth Alain Chang, Muhammad Mueez Amjad, Noorul Wahab, Ethar Alzaid, Nasir Rajpoot, Adam Shephard","Here's a summary of the research paper for a general audience:

**Title:** Improving Cancer Predictions by Combining Different Types of Data

**Summary:** Researchers are exploring ways to improve the accuracy of cancer predictions by combining different types of data, such as medical images and patient information. They studied the effectiveness of combining data from histopathology (tissue analysis), radiology (imaging tests), and clinical data to predict the likelihood of prostate cancer recurrence. Their findings showed that combining high-quality data from multiple sources can lead to better predictions. However, adding low-quality data can actually decrease the accuracy of predictions. This suggests that it's not always beneficial to combine multiple types of data, and that careful selection of data sources is crucial to achieve better results. These findings have important implications for the development of artificial intelligence models in cancer research and treatment."
cs.CV,VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation,"We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell",https://arxiv.org/abs/2511.11450v1,2025-11-14T16:20:07Z,"Maximilian Rokuss, Moritz Langenberg, Yannick Kirchhoff, Fabian Isensee, Benjamin Hamm, Constantin Ulrich, Sebastian Regnery, Lukas Bauer, Efthimios Katsigiannopulos, Tobias Norajitra, Klaus Maier-Hein","**Breakthrough in Medical Imaging: AI Model Enables Precise 3D Segmentation with Simple Text Prompts**

Researchers have developed a revolutionary AI model called VoxTell, which can accurately segment 3D medical images using simple text prompts. This technology has the potential to greatly improve medical diagnosis and treatment.

**What does it do?**

VoxTell can take a short text description, such as a single word or a sentence, and use it to identify specific areas of interest in 3D medical images, like CT scans or MRIs. This allows doctors to quickly and precisely segment images, which is a crucial step in diagnosing and treating diseases.

**How does it work?**

VoxTell was trained on a massive dataset of over 62,000 medical images and can understand a wide range of medical terminology. The model uses a sophisticated technique called vision-language fusion to align text and image features, allowing it to accurately segment images even when it encounters new or unseen data.

**Why is it important?**

VoxTell's ability to generalize to new data and modalities makes it a game-changer in medical imaging. It can:

* Perform well on images from different medical modalities, such as CT, MRI, and PET scans
* Understand variations in language and clinical terminology
* Accurately segment images from real-world text prompts

**What's next?**

The VoxTell model is open-source and available for use by the medical community. Its development has the potential to improve medical diagnosis and treatment, and researchers are excited to explore its applications in various medical fields."
cs.CV,From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs,"Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.",https://arxiv.org/abs/2511.11440v1,2025-11-14T16:07:18Z,"Massimo Rizzoli, Simone Alghisi, Seyed Mahed Mousavi, Giuseppe Riccardi","**Improving AI's Understanding of Visual Scenes**

Researchers have made a breakthrough in enhancing the ability of artificial intelligence (AI) models to reason about visual scenes. The AI models in question, called Vision-Language Models (VLMs), are trained to understand and describe images. However, their performance can be improved by fine-tuning them on specific datasets.

The problem with traditional fine-tuning methods is that they rely on real-world data, which can be biased, contain errors, or be imbalanced. This can lead to AI models that perform well on some scenes but poorly on others.

To address this issue, the researchers developed a new approach. They created a synthetic dataset, which is a collection of artificially generated images, with precise control over the objects' attributes, such as color, shape, size, and position. This allowed them to ensure that the data is balanced, free from biases, and accurately annotated.

The researchers then fine-tuned state-of-the-art VLMs on this synthetic dataset and tested their performance on both synthetic and real-world data. The results showed that:

1. Fine-tuning on balanced synthetic data leads to consistent performance across different visual scenes and reduces biases.
2. Fine-tuning on synthetic data significantly improves the AI models' performance on real-world data, outperforming models fine-tuned on real-world data.

This research has important implications for the development of AI models that can accurately understand and describe visual scenes, with potential applications in areas such as computer vision, robotics, and healthcare."
cs.CV,VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models,"Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use ""visual prompts"" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.",https://arxiv.org/abs/2511.11438v1,2025-11-14T16:06:25Z,"Mingjie Xu, Jinpeng Chen, Yuzhi Zhao, Jason Chun Lok Li, Yue Qiu, Zekang Du, Mengyang Wu, Pingping Zhang, Kun Li, Hongzheng Yang, Wenao Ma, Jiaheng Wei, Qinbin Li, Kangcheng Liu, Wenqiang Lei","**Unlocking the Power of Visual Prompts in AI Models**

Imagine you're trying to ask a question about a specific object in an image. You might naturally point to the object or draw a box around it to help clarify what you're referring to. This intuitive way of communicating is called a ""visual prompt"" (VP). However, researchers have found that current AI models, known as multimodal large language models (MLLMs), may not be effective at understanding these visual prompts.

To address this issue, researchers have created a new benchmark called VP-Bench. This benchmark is designed to test how well MLLMs can interpret visual prompts, such as bounding boxes, and use them to solve problems. The benchmark consists of two stages:

1. **Perceiving Visual Prompts**: The model is shown 30,000 images with various visual prompts, such as different shapes and attributes, and is asked to identify the object or region being referred to.
2. **Solving Real-World Problems**: The model is presented with real-world scenarios and asked to use visual prompts to solve problems, such as answering questions about specific objects in an image.

The researchers used VP-Bench to evaluate 28 different MLLMs, including popular proprietary and open-source models. They found that the ability of these models to understand visual prompts varies greatly, and that factors such as the type of visual prompt, the arrangement of questions, and the size of the model can affect their performance.

The development of VP-Bench provides a new framework for studying how MLLMs comprehend and respond to visual prompts. This research has important implications for the development of more intuitive and effective AI models that can understand and respond to human communication."
cs.CV,Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping,"Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.",https://arxiv.org/abs/2511.11437v1,2025-11-14T16:05:44Z,"Guowei Zhang, Yun Zhao, Moein Khajehnejad, Adeel Razi, Levin Kuhlmann","**Unlocking the Secrets of the Human Brain: A New Approach to Reconstructing Visual Images**

Imagine being able to see what someone is thinking about just by looking at their brain activity. Researchers have made a significant step towards making this a reality. They've developed a new method called Hi-DREAM, which uses brain-inspired technology to reconstruct visual images from brain scans.

The human brain processes visual information in a hierarchical way, with different areas responsible for different aspects of vision, such as edges, objects, and meaning. Current methods for reconstructing images from brain scans overlook this hierarchical processing, which can lead to blurry or inaccurate results.

Hi-DREAM addresses this limitation by organizing brain activity into a pyramid-like structure, mimicking the brain's own processing hierarchy. This allows the model to identify the roles of different visual areas and use this information to reconstruct images more accurately.

In tests, Hi-DREAM outperformed existing methods in reconstructing high-level semantic features, such as objects and scenes, while maintaining good low-level details, like edges and layout. This breakthrough provides a new window into understanding how the brain processes visual information and could have significant implications for fields like neuroscience, psychology, and computer vision.

The Hi-DREAM approach offers a promising new way to study the visual cortex and could lead to a better understanding of how our brains work. By developing more brain-inspired technology, researchers can gain a deeper understanding of the complex processes that underlie human vision and cognition."
cs.CV,Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation,"Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.",https://arxiv.org/abs/2511.11436v1,2025-11-14T16:05:37Z,"Xuanyu Tian, Lixuan Chen, Qing Wu, Xiao Wang, Jie Feng, Yuyao Zhang, Hongjiang Wei","**Breakthrough in Cardiac MRI Reconstruction**

Researchers have developed a new method called MoCo-INR to improve cardiac magnetic resonance (MRI) imaging. Cardiac MRI is a crucial tool for diagnosing and monitoring heart conditions, but it can be slow and may not produce high-quality images, especially in patients with irregular heartbeats or breathing patterns.

The MoCo-INR method uses artificial intelligence to reconstruct high-quality cardiac MRI images from limited data, which can significantly speed up the imaging process. Unlike previous methods, MoCo-INR does not require a large amount of pre-labeled data to learn from, making it more practical for clinical use.

In tests using simulated and real patient data, MoCo-INR outperformed existing methods, producing detailed and accurate images even when the data was severely limited (e.g., 20 times less data than usual). This breakthrough has the potential to enable real-time cardiac MRI imaging, which could revolutionize the diagnosis and treatment of heart conditions.

The MoCo-INR method works by decomposing cardiac motion into its underlying components and using this information to reconstruct high-quality images. The researchers also developed a new neural network architecture that helps stabilize the optimization process, making the method more reliable and efficient.

Overall, MoCo-INR represents a significant advancement in cardiac MRI reconstruction, with promising implications for improving patient care and outcomes."
cs.CV,The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models,"Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.",https://arxiv.org/abs/2511.11435v1,2025-11-14T16:03:10Z,"Maria-Teresa De Rosa Palmini, Eva Cetinic","**The Power of Cultural Memory in AI: How Machines Learn from Art and Culture**

Imagine you're shown a picture and a caption, and suddenly, you're reminded of a famous movie or artwork. This connection between images and texts is called ""multimodal iconicity."" Researchers have been studying how AI models, specifically text-to-image diffusion models, learn and remember cultural references. These models can generate images based on text prompts, but do they truly understand the cultural context?

The study introduces a new framework to evaluate how well these AI models recognize and recreate cultural references. The researchers tested five AI models using over 767 cultural references, such as famous artworks, movie scenes, and historical events. They found that these models can not only recognize cultural references but also transform and recontextualize them in new and creative ways.

The study reveals that the AI models' ability to understand cultural references is influenced by factors such as how often they appear in training data, their uniqueness, popularity, and age. The researchers also discovered that even when the text prompts are altered, the models can still reproduce iconic visual structures.

This research highlights the importance of understanding how AI models learn from culture and how they can be used to create new and innovative content. It also shows that the value of these models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge. Ultimately, this study advances our understanding of how AI models can be used to evaluate and create rich and contextualized content."
cs.CV,WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation,"Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.",https://arxiv.org/abs/2511.11434v1,2025-11-14T16:02:38Z,"Wei Chow, Jiachun Pan, Yongyuan Liang, Mingze Zhou, Xue Song, Liyu Jia, Saining Zhang, Siliang Tang, Juncheng Li, Fengda Zhang, Weijia Wu, Hanwang Zhang, Tat-Seng Chua","**Unlocking the Power of Context-Aware Image Generation and Editing**

Imagine being able to have a conversation with a computer about an image, where you can ask it to edit the image based on your previous requests. This is a challenging task that requires the computer to understand the context of the conversation and generate new images accordingly. Researchers have made significant progress in developing unified multimodal models (UMMs) that can comprehend and generate images. However, existing datasets and benchmarks only focus on single-turn interactions, which limits their ability to capture the complexity of real-world image creation and editing.

To address this gap, researchers have introduced WEAVE, a new dataset and benchmark that enables computers to learn from multi-turn conversations about images. WEAVE consists of a large-scale dataset of 100,000 interleaved samples, spanning over 370,000 dialogue turns and 500,000 images. This dataset covers various tasks, including comprehension, editing, and generation, which require reasoning over historical context.

The researchers also created WEAVEBench, a human-annotated benchmark with 100 tasks based on 480 images. This benchmark evaluates models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains.

**Key Findings:**

* Training on WEAVE enables vision comprehension, image editing, and comprehension-generation collaboration capabilities.
* WEAVE facilitates the development of emergent visual-memory capabilities in UMMs.
* Evaluations on WEAVEBench reveal the limitations and challenges of current approaches in multi-turn, context-aware image generation and editing.

**What does this mean?**

The development of WEAVE and WEAVEBench provides a foundation for studying in-context interleaved comprehension and generation for multi-modal community. This research has the potential to unlock new applications in areas such as:

* Image editing and manipulation
* Visual question answering
* Multi-modal dialogue systems

Overall, this research takes a significant step towards enabling computers to understand and generate images in a more human-like way, which could have a wide range of applications in fields such as art, design, and education."
cs.CV,Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs,"Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.",https://arxiv.org/abs/2511.11427v1,2025-11-14T15:54:34Z,"Francisco Nogueira, Alexandre Bernardino, Bruno Martins","**Breaking Down Language Barriers in Image Understanding**

Imagine you're trying to point out a specific object in a picture to someone who speaks a different language. You use a phrase to describe it, but can the listener understand what you're referring to? This is a challenge in the field of artificial intelligence, where computers need to comprehend natural language descriptions to identify objects in images.

Researchers have made significant progress in this area, but most of the existing research focuses on English. To bridge the language gap, a team of researchers has created a massive dataset of images with descriptions in 10 different languages, totaling around 8 million expressions across 177,620 images. They've also developed a new neural network architecture that can understand these multilingual descriptions.

The researchers tested their approach on standard benchmarks and achieved competitive results, demonstrating that their system can effectively identify objects in images based on descriptions in multiple languages. This breakthrough has significant implications for the development of more inclusive and accessible AI systems that can be used globally.

The dataset and model are now publicly available, paving the way for further research and applications in areas such as image search, robotics, and human-computer interaction."
cs.AI,Experience-Guided Adaptation of Inference-Time Reasoning Strategies,"Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",https://arxiv.org/abs/2511.11519v1,2025-11-14T17:45:28Z,"Adam Stein, Matthew Trager, Benjamin Bowman, Michael Kleinman, Aditya Chattopadhyay, Wei Xia, Stefano Soatto","**Breakthrough in AI Adaptation: Experience-Guided Reasoner (EGuR)**

Imagine a computer system that can learn and adapt to new problems on its own, without needing to be re-trained from scratch. Researchers have made a significant step towards creating such a system, called Experience-Guided Reasoner (EGuR). EGuR is a type of artificial intelligence (AI) that can adjust its problem-solving approach based on its past experiences, allowing it to become more accurate and efficient over time.

**How it works**

EGuR uses a two-part system:

1. A ""Guide"" that generates multiple possible solutions to a problem, based on what it has learned from past experiences.
2. A ""Consolidator"" that takes feedback from the solutions and uses it to improve future problem-solving.

**Key benefits**

* **Improved accuracy**: EGuR achieves up to 14% better accuracy than existing systems on challenging problems.
* **Reduced computational costs**: EGuR can reduce computing resources needed to solve problems by up to 111 times.
* **Flexibility**: EGuR can adapt to different types of problems and adjust its approach as it gains experience.

**Potential impact**

The development of EGuR has significant implications for the creation of more intelligent and adaptable AI systems. This technology could be used in a wide range of applications, from solving complex scientific problems to improving decision-making in industries such as healthcare and finance."
cs.AI,PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models,"Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.",https://arxiv.org/abs/2511.11502v1,2025-11-14T17:23:55Z,"Nhat Hoang-Xuan, Minh Vu, My T. Thai, Manish Bhattarai","**Detecting Object Hallucinations in AI Models: A New Breakthrough**

Large vision-language models (LVLMs) are AI systems that can process both images and text. While they're powerful, they can sometimes produce unreliable results, including ""object hallucinations"" - where the model claims to see objects in an image that aren't actually there.

Researchers have discovered that when LVLMs hallucinate, they often ignore the image and rely on their previous outputs to infer new objects. To detect this behavior, they've developed a new metric called the Prelim Attention Score (PAS). PAS is a simple, efficient way to identify when an LVLM is likely to be hallucinating.

The good news is that PAS can be computed in real-time, without requiring additional computations or training. In tests, PAS has achieved state-of-the-art results in detecting object hallucinations across multiple models and datasets. This breakthrough has the potential to improve the reliability of LVLMs and enable real-time filtering and intervention to prevent hallucinations."
cs.AI,Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models,"In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.",https://arxiv.org/abs/2511.11490v1,2025-11-14T17:09:01Z,"Joan Font-Quer Roset, Devina Mohan, Anna Scaife","Here's a summary of the research paper for a general audience:

**Understanding Radio Galaxy Images with AI**

Scientists have developed a new method to analyze images of radio galaxies, which are galaxies that emit strong radio waves. They used a type of artificial intelligence (AI) called a diffusion model to estimate the ""intrinsic dimension"" (iD) of a dataset of radio galaxy images, known as Radio Galaxy Zoo (RGZ).

The iD is a measure of how complex or detailed the images are. The researchers found that images that are very different from the typical radio galaxy images (called ""out-of-distribution"" sources) have higher iD values, indicating they are more complex. They also discovered that the overall iD of the RGZ dataset is higher than that of natural image datasets, such as photos of everyday objects.

The researchers also explored how iD varies across different types of radio galaxies (classified as Fanaroff-Riley classes) and how it relates to the quality of the image (measured by signal-to-noise ratio, or SNR). While they didn't find a strong connection between iD and galaxy type, they did find a weak trend: images with higher SNR (better quality) tend to have lower iD values.

This research has implications for future studies using the RGZ dataset. By understanding the relationship between iD and image characteristics, scientists can improve AI algorithms that learn to represent and classify radio galaxy images. This could lead to better insights into the properties and behavior of radio galaxies."
cs.AI,ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation,"Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.",https://arxiv.org/abs/2511.11483v1,2025-11-14T17:00:29Z,"Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang","**Breakthrough in Image Generation: Introducing ImAgent**

Imagine being able to generate highly realistic images from text descriptions with just a few clicks. Recent advancements in text-to-image models have made this possible, but they often struggle with inconsistencies and randomness, especially when the text descriptions are vague. To address this challenge, researchers have developed ImAgent, a unified multimodal agent framework that streamlines the image generation process.

**What makes ImAgent unique?**

ImAgent integrates three key components - reasoning, generation, and self-evaluation - into a single framework. This allows it to dynamically interact and self-organize multiple generation actions to produce high-quality images that align with the given text prompts. The best part? ImAgent doesn't require any additional modules or training, making it efficient and scalable.

**How does ImAgent work?**

ImAgent uses a policy controller to guide the generation process. This controller enables the framework to adapt and refine its output in real-time, ensuring that the generated images are both visually realistic and semantically coherent.

**What are the benefits?**

ImAgent has been extensively tested on various image generation and editing tasks, and the results are impressive. It consistently outperforms the backbone model and even surpasses other strong baselines in certain cases. This demonstrates the potential of unified multimodal agents like ImAgent for adaptive and efficient image generation.

**What's next?**

The development of ImAgent marks an exciting step forward in the field of image generation. As researchers continue to refine and expand this technology, we can expect to see significant advancements in areas such as art, design, and even healthcare. With ImAgent, the possibilities are endless, and the future of image generation looks brighter than ever."
cs.AI,Inferring response times of perceptual decisions with Poisson variational autoencoders,"Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",https://arxiv.org/abs/2511.11480v1,2025-11-14T16:58:04Z,"Hayden R. Johnson, Anastasia N. Krouglova, Hadi Vafaii, Jacob L. Yates, Pedro J. Gonçalves","**Understanding How We Make Quick Decisions**

Researchers have developed a new model to explain how our brains make fast decisions based on what we see. Currently, most computer models of decision-making treat choices as instantaneous, but our brains actually take time to process information and make a decision. This new model uses a type of artificial intelligence called a Poisson variational autoencoder to simulate how our brains encode and decode visual information from the activity of individual neurons.

The model works by:

1. **Encoding visual information**: The model takes in visual data, like images, and breaks it down into simple representations that can be understood by individual neurons.
2. **Decoding neural activity**: As the model receives simulated neural activity, it tries to figure out what action to take (e.g., which digit was shown).
3. **Making a decision**: The model uses a ""stopping rule"" to decide when it has enough information to make a choice, and then it outputs a decision.

When tested on a simple image classification task (recognizing handwritten digits), the model produced results that matched real-world patterns of human decision-making, including:

* **Variability in choices**: The model made different choices on different trials, just like humans do.
* **Response time distributions**: The model's response times were skewed to the right, meaning that most decisions were made quickly, but some took longer.
* **Hick's law**: As the number of possible choices increased, the model's response times increased logarithmically, just like humans.
* **Speed-accuracy trade-offs**: The model could balance speed and accuracy, just like humans can.

This research provides a more realistic and detailed understanding of how our brains make quick decisions based on visual information."
cs.AI,Context-aware Adaptive Visualizations for Critical Decision Making,"Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.",https://arxiv.org/abs/2511.11476v1,2025-11-14T16:53:15Z,"Angela Lopez-Cardona, Mireia Masias Bruns, Nuwan T. Attygalle, Sebastian Idesis, Matteo Salvatori, Konstantinos Raftopoulos, Konstantinos Oikonomou, Saravanakumar Duraisamy, Parvin Emami, Nacera Latreche, Alaa Eddine Anis Sahraoui, Michalis Vakallelis, Jean Vanderdonckt, Ioannis Arapakis, Luis A. Leiva","**Unlocking Better Decision-Making with Adaptive Visualizations**

Imagine being able to make critical decisions with ease, thanks to visualizations that adjust in real-time to your mental state. Researchers have developed Symbiotik, a cutting-edge system that uses brain signals to detect mental workload and adapt visual dashboards accordingly. In a study with 120 participants, Symbiotik was shown to improve task performance and engagement.

**How it works:**

* Symbiotik uses neurophysiological signals (like brain activity) to estimate mental workload.
* Based on this information, the system dynamically adapts visual dashboards to reduce mental strain and improve comprehension.
* The system uses reinforcement learning, a type of artificial intelligence, to learn and adapt to individual users.

**The benefits:**

* Improved task performance: Symbiotik helped participants complete tasks more efficiently.
* Increased engagement: Participants were more engaged and interested in the visualizations.

**The future:**

* Symbiotik offers a scalable architecture for real-time adaptation, making it a promising solution for various applications.
* This research paves the way for the development of neuroadaptive user interfaces that can improve decision-making in various fields."
cs.AI,Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents,"The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.",https://arxiv.org/abs/2511.11468v1,2025-11-14T16:41:10Z,"Davide Napolitano, Luca Cagliero, Fabrizio Battiloro","**Can AI Models Handle Tricky Questions on Complex Documents?**

Imagine you're trying to ask a question about a document that contains both text and images. A new type of AI model, called Visual Large Language Models (VLLMs), is designed to understand these types of documents and answer questions about them. But what happens when the question can't be answered, even if it seems like it should be?

Researchers have created a new benchmark, called VRD-UQA, to test how well VLLMs can detect when a question can't be answered. They found that these models have limitations and can be tricked into giving answers to questions that are actually unanswerable.

The researchers tested 12 different VLLMs and found that they struggled to detect unanswerable questions, especially when the questions were slightly altered or corrupted. They also looked at how different types of corruption and knowledge injection strategies affected the models' performance.

The study's findings are important because they highlight the need for more resilient AI models that can handle complex documents and tricky questions. The VRD-UQA benchmark can be used to evaluate and improve VLLMs, leading to better document understanding and question-answering systems."
cs.AI,Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies,"Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.",https://arxiv.org/abs/2511.11461v1,2025-11-14T16:32:42Z,"Riku Green, Huw Day, Zahraa S. Abdallah, Telmo M. Silva Filho","**Understanding Time Series Forecasting: A New Perspective on Recursive and Direct Strategies**

Time series forecasting involves predicting future values in a sequence of data. There are two main approaches: recursive and direct. Recursive strategies predict one step ahead and then use that prediction to forecast the next step, and so on. Direct strategies, on the other hand, predict multiple steps ahead directly.

For a long time, researchers believed that recursive strategies tend to be more biased ( consistently off-target) but less variable (more consistent), while direct strategies are less biased but more variable. However, a new study challenges this conventional wisdom.

The researchers broke down the error in multi-step forecasting into three parts: 

1. **Irreducible noise**: the inherent randomness in the data that can't be predicted.
2. **Structural approximation gap**: the error due to the model's inability to perfectly capture the underlying patterns.
3. **Estimation-variance term**: the error due to the uncertainty in estimating the model's parameters.

The study found that:

* For simple (linear) models, the structural gap is zero, meaning that the model's limitations don't add extra error.
* For complex (nonlinear) models, the recursive approach can actually increase the model's expressiveness, but also amplify errors.
* The recursive strategy's variance (unpredictability) at any horizon can be expressed as the one-step variance multiplied by a factor that measures how sensitive the composed predictor is to parameter error.

The study's experiments with multilayer perceptrons (a type of neural network) on a real-world dataset confirmed these findings. The results provide practical guidance for choosing between recursive and direct strategies based on the model's complexity and the data's noise characteristics, rather than relying on traditional bias-variance intuition.

In simple terms, this study offers a new perspective on time series forecasting, highlighting that the choice between recursive and direct strategies depends on the specific characteristics of the model and data, rather than a simple rule of thumb."
cs.AI,Retrofit: Continual Learning with Bounded Forgetting for Security Applications,"Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.   We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.",https://arxiv.org/abs/2511.11439v1,2025-11-14T16:07:03Z,"Yiling He, Junchi Lei, Hongyu She, Shuo Shao, Xinran Zheng, Yiping Liu, Zhan Qin, Lorenzo Cavallaro","**Improving AI Models for Cybersecurity: A New Approach**

Deep learning models are increasingly used in cybersecurity to detect threats and analyze data. However, as threat landscapes evolve and data changes over time, these models can become less effective. To address this issue, researchers have been exploring continual learning (CL), which allows models to learn from new data without forgetting what they previously learned.

The challenge is that many CL approaches require access to old data or repeated retraining, which can be impractical in cybersecurity settings where data is sensitive or constantly changing. To overcome this, a team of researchers has developed a new method called RETROFIT.

**What is RETROFIT?**

RETROFIT is a novel approach to continual learning that enables AI models to learn from new data without forgetting their previous knowledge. It does this by:

1. Merging old and new model knowledge without needing access to old data.
2. Updating model parameters in a way that minimizes interference between old and new knowledge.
3. Dynamically balancing the contributions of old and new knowledge based on model confidence.

**How does RETROFIT work?**

RETROFIT works by consolidating previously trained and newly fine-tuned models, serving as teachers of old and new knowledge. This is achieved through parameter-level merging, which eliminates the need for historical data. To mitigate interference, RETROFIT applies low-rank and sparse updates that confine parameter changes to independent subspaces. A knowledge arbitration mechanism dynamically balances the teacher contributions guided by model confidence.

**What are the benefits of RETROFIT?**

The researchers tested RETROFIT on two cybersecurity applications: malware detection and binary summarization. The results show that RETROFIT:

* Significantly improves the model's ability to retain its performance over time, even as the threat landscape evolves.
* Outperforms existing CL approaches and transfer learning methods in terms of accuracy and adaptability.

**Implications and Future Directions**

The development of RETROFIT has significant implications for the field of cybersecurity. By enabling AI models to learn from new data without forgetting their previous knowledge, RETROFIT can help improve the accuracy and effectiveness of cybersecurity systems. Future research directions may include exploring the application of RETROFIT to other domains and developing more advanced continual learning methods.

**In simple terms**

Imagine you have a security system that uses AI to detect malware. As new malware emerges, the AI model needs to learn to recognize it. However, as it learns, it might forget how to detect old malware. RETROFIT is a new approach that helps the AI model learn to detect new threats without forgetting what it already knows. This can help keep the security system effective over time, even as threats evolve."
cs.AI,The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models,"Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.",https://arxiv.org/abs/2511.11435v1,2025-11-14T16:03:10Z,"Maria-Teresa De Rosa Palmini, Eva Cetinic","**The Power of Cultural Memory in AI: How Machines Learn and Recall Cultural References**

Imagine you're looking at a picture and suddenly, a famous movie or artwork comes to mind. This connection between images and cultural references is called multimodal iconicity. Researchers have been studying how AI models, specifically text-to-image diffusion models, learn and recall these cultural references.

The study found that these AI models are not just good at generating new images, but also at recognizing and reproducing cultural references. However, the models don't just copy and paste; they also transform and recontextualize these references in creative ways.

The researchers developed a new evaluation framework to assess how well these models recognize and depict cultural references. They tested five AI models using over 767 cultural references, including images from movies, artworks, and more. The results showed that the models can identify and replicate cultural references, but also that they can transform them in interesting ways.

The study also found that the models' ability to recall cultural references is influenced by factors such as how often the reference appears in the training data, how unique the text description is, and how popular the reference is. This suggests that AI models are not just learning from data, but also from the cultural context in which the data is presented.

Overall, the study highlights the importance of understanding how AI models learn and recall cultural references, and how they can be used to transform and recontextualize cultural knowledge in creative and meaningful ways. This has implications for the development of more sophisticated AI models that can better understand and engage with human culture."
cs.AI,CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction,"Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.",https://arxiv.org/abs/2511.11423v1,2025-11-14T15:52:22Z,"Cong-Tinh Dao, Nguyen Minh Thao Phan, Jun-En Ding, Chenwei Wu, David Restrepo, Dongsheng Luo, Fanyi Zhao, Chun-Chieh Liao, Wen-Chih Peng, Chi-Te Wang, Pei-Fu Chen, Ling Chen, Xinglong Ju, Feng Liu, Fang-Ming Hung","**Breakthrough in Chronic Disease Prediction: CURENet Revolutionizes Healthcare**

Imagine a system that can accurately predict chronic diseases, such as diabetes or heart disease, by combining different types of patient data. Researchers have developed CURENet, a cutting-edge model that integrates electronic health records (EHRs) to improve disease prediction.

EHRs contain a wealth of information, including doctor's notes, lab test results, and patient visit history. However, most predictive models focus on a single type of data, missing out on valuable insights from other sources. CURENet changes this by combining unstructured clinical notes, lab tests, and time-series data using advanced language models and transformer encoders.

**What makes CURENet special?**

* It captures complex interactions between different types of clinical data.
* It creates a more reliable predictive model for chronic illnesses.
* It achieved over 94% accuracy in predicting top 10 chronic conditions using two separate datasets.

**What does this mean for healthcare?**

CURENet has the potential to enhance clinical decision-making and improve patient outcomes. By integrating multiple data sources, healthcare professionals can make more informed decisions, leading to better treatment and care for patients with chronic diseases. This innovation could revolutionize the way we approach disease prediction and management, ultimately improving the lives of millions of people worldwide."
cs.AI,Variational Quantum Algorithms for Particle Track Reconstruction,"Quantum Computing is a rapidly developing field with the potential to tackle the increasing computational challenges faced in high-energy physics. In this work, we explore the potential and limitations of variational quantum algorithms in solving the particle track reconstruction problem. We present an analysis of two distinct formulations for identifying straight-line tracks in a multilayer detection system, inspired by the LHCb vertex detector. The first approach is formulated as a ground-state energy problem, while the second approach is formulated as a system of linear equations. This work addresses one of the main challenges when dealing with variational quantum algorithms on general problems, namely designing an expressive and efficient quantum ansatz working on tracking events with fixed detector geometry. For this purpose, we employed a quantum architecture search method based on Monte Carlo Tree Search to design the quantum circuits for different problem sizes. We provide experimental results to test our approach on both formulations for different problem sizes in terms of performance and computational cost.",https://arxiv.org/abs/2511.11397v1,2025-11-14T15:24:59Z,"Vincenzo Lipardi, Xenofon Chiotopoulos, Jacco A. de Vries, Domenica Dibenedetto, Kurt Driessens, Marcel Merk, Mark H. M. Winands","Here's a summary of the research paper for a general audience:

**Unlocking the Power of Quantum Computing for Particle Detection**

Scientists at the Large Hadron Collider (LHC) face a major challenge: detecting and tracking particles produced in high-energy collisions. The sheer amount of data generated by these collisions requires powerful computers to process and analyze. Quantum computing, a rapidly evolving field, offers a promising solution to this problem.

In a recent study, researchers explored the potential of variational quantum algorithms to reconstruct particle tracks in a multilayer detection system, similar to the one used in the LHCb experiment. They developed two different approaches to identify straight-line tracks and tested them using quantum computers.

The researchers faced a key challenge: designing efficient quantum circuits that can handle the complex geometry of the detector. To overcome this, they used a sophisticated method called Monte Carlo Tree Search to optimize the quantum circuits for different problem sizes.

The study provides encouraging results, demonstrating the performance and computational cost of the two approaches for various problem sizes. While there are still significant technical hurdles to overcome, this research suggests that quantum computing could become a valuable tool for particle track reconstruction in high-energy physics.

**In simple terms:** Imagine trying to find the path of a particle through a complex detector. Quantum computers can help solve this problem faster and more efficiently than classical computers. This study shows that quantum algorithms can be used to reconstruct particle tracks, but more work is needed to make them practical and scalable."
cs.AI,Robust and Efficient Communication in Multi-Agent Reinforcement Learning,"Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.",https://arxiv.org/abs/2511.11393v1,2025-11-14T15:23:11Z,"Zejiao Liu, Yi Li, Jiali Wang, Junqi Tu, Yitian Hong, Fangfei Li, Yang Liu, Toshiharu Sugawara, Yang Tang","**Improving Communication in Teamwork among Autonomous Agents**

Imagine a team of self-driving cars working together to navigate through a busy intersection. They need to communicate with each other to avoid collisions and ensure smooth traffic flow. However, in the real world, communication between autonomous agents can be imperfect - messages might get delayed, distorted, or lost. This can lead to errors and accidents.

Researchers have been working on developing more robust and efficient communication strategies for multi-agent reinforcement learning (MARL), which enables autonomous agents to learn and work together. The goal is to make communication between agents more reliable, even in challenging conditions.

This survey reviews recent advances in MARL communication strategies that can handle issues like message errors, delays, and limited bandwidth. The researchers focus on three practical applications:

1. **Cooperative autonomous driving**: self-driving cars working together to navigate through complex traffic scenarios.
2. **Distributed simultaneous localization and mapping**: multiple robots working together to create accurate maps of their environment.
3. **Federated learning**: multiple agents learning from each other's data without sharing sensitive information.

The researchers identify key challenges that need to be addressed, such as ensuring low-latency communication, managing bandwidth-intensive data sharing, and balancing communication with privacy concerns. They propose a unified approach that combines communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

Overall, this research aims to enable more efficient and reliable communication among autonomous agents, which is crucial for developing safe and effective teamwork in real-world applications."
cs.AI,MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism,"Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",https://arxiv.org/abs/2511.11373v1,2025-11-14T14:52:34Z,"Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu","**Breakthrough in Artificial Intelligence: MarsRL Enhances Reasoning Capabilities**

Imagine a team of AI agents working together to solve complex problems. Researchers have developed a new framework called MarsRL, which enables these agents to learn from each other and improve their reasoning abilities. This innovation has the potential to significantly enhance the performance of artificial intelligence (AI) systems.

The challenge with current AI models is that they can only process information in short sequences, limiting their ability to reason deeply. To overcome this, the researchers created a system with multiple AI agents, each with a specific role: Solver, Verifier, and Corrector. These agents work together to iteratively refine solutions to complex problems.

The MarsRL framework uses a novel approach called agentic pipeline parallelism, which allows the agents to learn from each other and improve their performance. The researchers tested MarsRL on a large language model and achieved significant improvements in accuracy on two challenging reasoning tasks.

The results are impressive: MarsRL improved accuracy from 86.5% to 93.3% on one task and from 64.9% to 73.8% on another. These findings suggest that MarsRL has the potential to advance multi-agent reasoning systems and enable AI to tackle more complex tasks.

**In simple terms:** MarsRL is a new AI framework that enables multiple agents to work together to solve complex problems. It has shown promising results in improving the reasoning capabilities of AI systems, which could have significant implications for various applications in the future."
cs.AI,KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics,"We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.",https://arxiv.org/abs/2511.11357v1,2025-11-14T14:44:14Z,"Haixin Li, Yanke Li, Diego Paez-Granados","**Introducing KarmaTS: A Powerful Tool for Simulating Complex Time Series Data**

Imagine being able to simulate complex systems that change over time, like weather patterns or human health metrics, in a way that's both realistic and flexible. Researchers have developed a new platform called KarmaTS, which allows scientists to create simulated data that mimics real-world patterns, while also incorporating expert knowledge and causal relationships.

**What is KarmaTS?**

KarmaTS is an interactive framework that helps researchers build models of complex systems that change over time. It's designed to simulate multivariate time series data, which involves multiple variables that are related to each other and change over time.

**Key Features of KarmaTS**

* **Simulates complex systems**: KarmaTS can simulate complex systems with multiple variables that change over time.
* **Incorporates expert knowledge**: The platform allows experts to add their knowledge to the simulation, making it more realistic and accurate.
* **Flexible and customizable**: KarmaTS can handle different types of data and relationships between variables, making it a versatile tool for researchers.

**Why is KarmaTS important?**

KarmaTS is particularly useful when working with sensitive or restricted data, such as medical records. By generating synthetic data that mimics real-world patterns, researchers can develop and test new algorithms without compromising patient confidentiality. Additionally, KarmaTS enables researchers to validate and benchmark causal discovery algorithms, which are used to identify cause-and-effect relationships in complex systems.

**Real-World Applications**

KarmaTS has the potential to be used in a variety of fields, including:

* **Healthcare**: Simulating patient outcomes and disease progression to inform treatment decisions.
* **Environmental science**: Modeling climate patterns and predicting the effects of climate change.
* **Finance**: Simulating stock prices and portfolio performance to inform investment decisions.

Overall, KarmaTS is a powerful tool for simulating complex time series data, and its flexibility and customizability make it a valuable asset for researchers across various fields."
cs.AI,"Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions","Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",https://arxiv.org/abs/2511.11347v1,2025-11-14T14:33:58Z,"Shaowei Guan, Hin Chi Kwok, Ngai Fong Law, Gregor Stiglic, Vivian Hui","**The Double-Edged Sword of AI in Healthcare: Balancing Benefits and Patient Privacy**

The integration of Artificial Intelligence (AI) in healthcare has the potential to revolutionize patient care, but it also raises significant concerns about patient privacy. A recent review of 23 research articles on Retrieval-Augmented Generation (RAG) - a type of AI technology used in healthcare chatbots - highlights the need for better protection of sensitive patient data.

**The Risks: Exposing Sensitive Patient Information**

The review found that RAG systems, which use large language models to generate human-like responses, pose significant risks to patient privacy. These risks include the exposure of Protected Health Information (PHI), such as medical history, diagnoses, and treatment plans. The researchers identified potential vulnerabilities in RAG systems at various stages, including data storage, transmission, retrieval, and generation.

**The Gaps: Insufficient Validation and Lack of Standards**

The review revealed several critical gaps in current RAG systems, including:

1. **Insufficient clinical validation**: Many RAG systems have not been thoroughly tested in real-world clinical settings.
2. **Lack of standardized evaluation frameworks**: There is no widely accepted framework for evaluating the performance and safety of RAG systems.
3. **Limited automated assessment tools**: Few tools are available to automatically detect and mitigate potential privacy risks.

**The Way Forward: Protecting Patient Data and Promoting Effective AI Systems**

To address these challenges, the researchers propose several actionable directions, including:

1. **Developing more effective data protection mechanisms**, such as encryption and secure data storage.
2. **Establishing standardized evaluation frameworks** to ensure the safety and efficacy of RAG systems.
3. **Creating automated assessment tools** to detect and mitigate potential privacy risks.

Ultimately, the goal is to develop RAG systems that not only provide high-quality patient care but also prioritize patient privacy and data protection. By acknowledging the potential risks and taking proactive steps to mitigate them, researchers and practitioners can work together to create a safer and more effective AI-powered healthcare system."
cs.AI,M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text,"The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",https://arxiv.org/abs/2511.11340v1,2025-11-14T14:26:31Z,"Salima Lamsiyah, Saad Ezzini, Abdelkader El Mahdaouy, Hamza Alami, Abdessamad Benlahbib, Samir El Amrany, Salmane Chafik, Hicham Hammouchi","**Detecting AI-Generated Text: A New Challenge**

The rise of artificial intelligence (AI) has made it possible for computers to generate highly fluent text, similar to what humans write. However, this poses a challenge to the integrity of information and academic research. A team of researchers has launched a shared task called M-DAIGT, which aims to detect AI-generated text across multiple domains, such as news articles and academic writing.

To support this task, the researchers created a large dataset of 30,000 samples, consisting of both human-written and AI-generated texts. The AI-generated content was produced using advanced language models like GPT-4 and Claude. The researchers then invited teams to participate in the shared task, which involved two subtasks: detecting AI-generated news articles and detecting AI-generated academic writing.

Four teams participated in the final round, and their methods included using machine learning algorithms to analyze the linguistic features of the text. The results of the shared task provide insights into the challenges of detecting AI-generated text and highlight the need for further research in this area.

Overall, the M-DAIGT shared task is an important step towards developing effective methods for detecting AI-generated text, which is essential for maintaining the integrity of information and academic research."
cs.AI,NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery,"Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",https://arxiv.org/abs/2511.11324v1,2025-11-14T14:01:18Z,"Anurag J. Vaidya, Felix Meissen, Daniel C. Castro, Shruthi Bannur, Tristan Lazard, Drew F. K. Williamson, Faisal Mahmood, Javier Alvarez-Valle, Stephanie L. Hyland, Kenza Bouzid","Here's a summary of the research paper for a general audience:

**Introducing NOVA: A Game-Changer in Histopathology Analysis**

Histopathology is the study of tissue samples to diagnose and understand diseases. However, analyzing these samples requires specialized expertise and can be a time-consuming process. Researchers have developed a new framework called NOVA, which uses artificial intelligence to automate and simplify histopathology analysis.

**What does NOVA do?**

NOVA is a computer program that can understand scientific questions and translate them into a series of steps to analyze tissue samples. It uses a library of 49 specialized tools to perform tasks such as identifying cell nuclei and analyzing tissue images. What's more, NOVA can even create new tools on the fly to tackle complex problems.

**How well does NOVA work?**

To test NOVA's abilities, researchers created a benchmark called SlideQuest, which consists of 90 challenging questions that require multi-step reasoning and problem-solving. NOVA outperformed other computer programs designed for coding and analysis, demonstrating its potential to accelerate discovery in histopathology.

**A real-world application**

In a case study, a pathologist used NOVA to analyze tissue samples and discovered a link between tissue morphology (the study of the shape and structure of tissues) and specific subtypes of cancer. This demonstrates NOVA's ability to help researchers make new discoveries and gain insights into diseases.

Overall, NOVA has the potential to make histopathology analysis more accessible, efficient, and scalable, which could lead to new breakthroughs in disease diagnosis and treatment."
cs.AI,RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms,"Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.",https://arxiv.org/abs/2511.11323v1,2025-11-14T13:59:40Z,"Yitian Kou, Yihe Gu, Chen Zhou, DanDan Zhu, Shuguang Kuai","**Introducing RLSLM: A New Framework for Socially Aware Agents**

Imagine being in a crowded space, like a shopping mall or a train station, and having a robot or virtual agent navigate around you without bumping into you or making you feel uncomfortable. This is a challenging task, as it requires the agent to understand and follow human social norms.

Researchers have proposed a new framework called RLSLM, which combines the strengths of two approaches: rule-based models, which are based on predefined principles, and data-driven methods, which learn from large datasets. RLSLM integrates a rule-based social locomotion model into a reinforcement learning framework, allowing agents to navigate through human-populated environments in a socially aware way.

**How RLSLM Works**

The social locomotion model generates a ""social comfort field"" that maps out the comfort level of humans in a given space. This field takes into account how people feel when others approach them or intrude on their personal space. The reinforcement learning framework then uses this field to optimize the agent's navigation policy, balancing mechanical energy and social comfort.

**Real-World Testing**

In a virtual reality experiment, human participants interacted with agents using RLSLM and other state-of-the-art models. The results showed that RLSLM outperformed the other models in terms of user experience, allowing agents to navigate through spaces in a way that was more comfortable and respectful of human social norms.

**Key Benefits**

RLSLM offers several advantages over existing approaches:

* **Improved interpretability**: RLSLM provides a clear understanding of why the agent made certain decisions, making it more transparent and trustworthy.
* **Scalability**: RLSLM can be applied to real-world social navigation scenarios, making it a practical solution for various applications.
* **Human-centered design**: RLSLM is designed with human social norms in mind, ensuring that agents behave in a way that is respectful and considerate of human feelings.

Overall, RLSLM represents a significant step forward in developing socially aware agents that can navigate complex human environments with ease and respect."
cs.AI,LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models,"Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",https://arxiv.org/abs/2511.11315v1,2025-11-14T13:57:46Z,"Jawad Ibn Ahad, Muhammad Rafsan Kabir, Robin Krambroeckers, Sifat Momen, Nabeel Mohammed, Shafin Rahman","**Unlocking the Power of Large Language Models for Finance**

Large language models (LLMs) have revolutionized the way we analyze and understand text in the financial industry. However, these powerful models require significant computational resources, making them inaccessible to many organizations.

To address this challenge, researchers have developed a novel framework called Layer-wise Adaptive Ensemble Tuning (LAET). LAET allows for efficient fine-tuning of pre-trained LLMs by selectively updating only the most critical layers, while keeping less important layers unchanged.

The results are impressive: LAET outperforms existing benchmarks and state-of-the-art LLMs, including GPT-4, on various financial NLP tasks, such as sentiment analysis and stock movement prediction. What's more, LAET achieves these results using smaller LLMs with only 3 billion parameters, making it a more accessible and scalable solution for financial applications.

By bridging the gap between cutting-edge research and real-world deployment, LAET has the potential to democratize access to advanced NLP capabilities for organizations in the financial industry, enabling them to make more informed decisions and drive business growth."
cs.CL,W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search,"Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.",https://arxiv.org/abs/2511.11518v1,2025-11-14T17:42:02Z,"Zhenyu Ding, Yuhao Wang, Tengyue Xiao, Haoying Wang, Guojun Ma, Mingyang Wan, Caigui Jiang, Ning Ding","**Improving Large Language Models: A New Approach to Aligning with Human Preferences**

Large Language Models (LLMs) have shown impressive capabilities, but their outputs often don't match what humans want. This is because they are typically trained on weak supervision and lack fine-grained control. Current methods to align LLMs with human preferences, such as Reinforcement Learning from Human Feedback (RLHF), are expensive and not scalable.

To address this issue, researchers have proposed a new framework called W2S-AlignTree. This framework uses a technique called Monte Carlo Tree Search (MCTS) to help LLMs generate outputs that are more in line with human preferences. W2S-AlignTree works by formulating LLM alignment as a search problem, where the model explores different possible outputs and selects the best one.

The key innovation of W2S-AlignTree is that it uses a ""weak"" model to provide real-time feedback to a ""strong"" model, guiding it to generate better outputs. This approach allows for fine-grained control and dynamic balancing of exploration and exploitation.

**Key Findings:**

* W2S-AlignTree consistently outperforms strong baselines across various tasks, including sentiment generation, summarization, and instruction-following.
* On a summarization task, W2S-AlignTree improved the performance of a large language model (Llama3-8B) by 15.9%, from 1.89 to 2.19.

**Implications:**

* W2S-AlignTree offers a scalable and adaptable solution for aligning LLMs with human preferences, which could lead to more accurate and helpful language models.
* This approach has the potential to improve the performance of LLMs in various applications, such as chatbots, language translation, and text summarization."
cs.CL,Proactive Hearing Assistants that Isolate Egocentric Conversations,"We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/",https://arxiv.org/abs/2511.11473v1,2025-11-14T16:44:48Z,"Guilin Hu, Malek Itani, Tuochao Chen, Shyamnath Gollakota","**Breakthrough in Hearing Assistants: Isolating Conversations in Real-Time**

Imagine being able to focus on a conversation in a noisy room, without having to ask your hearing assistant to tune in to a specific speaker. Researchers have made a significant step towards making this a reality with the development of proactive hearing assistants that can automatically identify and isolate the wearer's conversation partners.

These innovative assistants use advanced audio processing technology to analyze the wearer's own speech and the sounds around them. By leveraging the natural flow of conversation, they can infer who the wearer is talking to and separate their voice from others in real-time. This is achieved through a dual-model architecture that enables fast and accurate processing of audio signals.

In tests with real-world conversations involving 2-3 speakers, the system demonstrated impressive results in identifying and isolating conversation partners, even in multi-conversation settings. This technology has the potential to revolutionize the way hearing assistants work, allowing them to adapt proactively to conversational dynamics and engagement.

The researchers behind this project aim to create hearing assistants that can seamlessly integrate into daily life, enabling people to focus on the conversations that matter most. With further development, this technology could significantly improve the lives of individuals with hearing impairments."
cs.CL,From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs,"Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.",https://arxiv.org/abs/2511.11440v1,2025-11-14T16:07:18Z,"Massimo Rizzoli, Simone Alghisi, Seyed Mahed Mousavi, Giuseppe Riccardi","Here's a summary of the research paper for a general audience:

**Improving AI's Understanding of Visual Scenes**

Researchers have found a way to improve the performance of artificial intelligence (AI) models that understand visual scenes and language. These models, called Vision-Language Models (VLMs), are often fine-tuned on real-world data to improve their performance. However, this process can be flawed due to biases, errors, and imbalances in the data.

To address this issue, the researchers created a new method to fine-tune VLMs using synthetic data, which is artificially generated. They controlled the creation of this synthetic data to ensure it was free from biases and errors. They then used this data to fine-tune state-of-the-art VLMs and tested their performance on both synthetic and real-world data.

The results showed two important findings:

1. **Balanced performance**: Fine-tuning on synthetic data that is balanced and free from biases leads to uniform performance across visual scenes, reducing common biases.
2. **Improved real-world performance**: Fine-tuning on synthetic data significantly improves the model's performance on real-world data, outperforming models fine-tuned on real-world data.

This research has the potential to improve AI's understanding of visual scenes and language, leading to more accurate and reliable performance in various applications."
cs.CL,MajinBook: An open catalogue of digital world literature with likes,"This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.",https://arxiv.org/abs/2511.11412v1,2025-11-14T15:44:27Z,"Antoine Mazières, Thierry Poibeau","Here's a summary of the research paper for a general audience:

**Introducing MajinBook: A New Resource for Literary Research**

Imagine a vast library that contains millions of books from around the world, including hard-to-find titles and classics. Researchers have created a new tool called MajinBook, which combines data from online libraries like Library Genesis and Z-Library with information from book review websites like Goodreads.

**What does MajinBook do?**

MajinBook creates a huge database of over 539,000 books, including details like publication dates, genres, and popularity ratings. This database can be used by researchers to study literature and culture in new and exciting ways.

**Why is MajinBook important?**

Traditional libraries often have limited collections, and many books are hard to find or out of print. MajinBook helps to fill this gap by providing access to a vast collection of digital books. It also addresses biases in traditional libraries, which often lack diverse perspectives and voices.

**What's next?**

The researchers behind MajinBook have made all of their data openly available, which means that anyone can use it for their own research. They've also evaluated the accuracy of their approach and explored the legal implications of using this data for research purposes. With MajinBook, researchers can now explore new questions about literature, culture, and society in a more comprehensive and inclusive way."
cs.CL,Studies with impossible languages falsify LMs as models of human language,"According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.",https://arxiv.org/abs/2511.11389v1,2025-11-14T15:18:26Z,"Jeffrey S. Bowers, Jeff Mitchell","Here's a summary of the research paper for a general audience:

**Language Models Don't Truly Understand Human Language**

Researchers have been testing whether computer language models (LMs) can accurately mimic human language learning. To do this, they compared how well LMs and human infants learn real languages versus ""impossible languages"" that have unnatural grammar and structures.

The surprising finding is that LMs can learn both types of languages equally well, whereas human infants (and presumably adults) find it much harder to learn impossible languages. This suggests that LMs lack the built-in biases and abilities that humans have to help us learn language.

In other words, while LMs can process and generate text that sounds like human language, they don't truly understand the underlying rules and structures of language like humans do. This has implications for how we develop and use LMs, and highlights the complexity and uniqueness of human language acquisition."
cs.CL,On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization,"On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.",https://arxiv.org/abs/2511.11362v1,2025-11-14T14:46:29Z,"Prabodh Katti, Sangwoo Park, Bipin Rajendran, Osvaldo Simeone","Here's a summary of the research paper for a general audience:

**Making AI Models Smarter on Small Devices**

Artificial intelligence (AI) is being used in many devices, such as smartphones and smart home devices. These devices have limited memory, which makes it challenging to update AI models to perform new tasks. Researchers have found a way to make it possible to update AI models on small devices without running out of memory.

**The Problem: Updating AI Models Requires Too Much Memory**

When updating an AI model, the device needs to store a lot of information, which takes up a lot of memory. This limits the size of the AI model that can be used on small devices.

**The Solution: A New Way to Update AI Models**

The researchers developed a new method called Memory-efficient Zeroth-order Optimization (MeZO). This method estimates how to update the AI model without storing all the intermediate information, which reduces the memory requirements. This allows larger AI models to be used on small devices.

**The Trade-Off: Time vs. Memory**

While MeZO uses less memory, it may take longer to update the AI model. The researchers found that MeZO can achieve similar or better accuracy than traditional methods, but it requires more time to fine-tune the model.

**The Impact**

This research has significant implications for the development of AI-powered devices, such as smartphones, smart home devices, and edge AI systems. With MeZO, these devices can support more complex AI models, enabling them to perform a wider range of tasks, such as image recognition, natural language processing, and decision-making. For example, MeZO could enable smartphones to run more advanced AI-powered apps, or allow smart home devices to learn and adapt to new environments.

**The Benefits**

The benefits of MeZO include:

* **Increased model size**: MeZO allows for larger AI models to be used on small devices, enabling more complex tasks to be performed.
* **Improved accuracy**: MeZO can achieve similar or better accuracy than traditional methods, given sufficient fine-tuning time.
* **Reduced memory requirements**: MeZO reduces the memory requirements for updating AI models, making it possible to use AI on devices with limited memory.

Overall, this research provides a promising solution for making AI models smarter on small devices, which can lead to more efficient and effective AI-powered applications in various industries."
cs.CL,M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text,"The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",https://arxiv.org/abs/2511.11340v1,2025-11-14T14:26:31Z,"Salima Lamsiyah, Saad Ezzini, Abdelkader El Mahdaouy, Hamza Alami, Abdessamad Benlahbib, Samir El Amrany, Salmane Chafik, Hicham Hammouchi","**Detecting AI-Generated Text: A New Challenge in the Digital Age**

As artificial intelligence (AI) becomes increasingly sophisticated, it's getting harder to tell whether a piece of text was written by a human or a machine. This poses a significant threat to the integrity of information and academic research. To address this challenge, researchers have launched a new shared task called M-DAIGT, which aims to develop and test methods for detecting AI-generated text across multiple domains, including news articles and academic writing.

**The Challenge: Two Subtasks**

The M-DAIGT shared task consists of two subtasks:

1. **News Article Detection (NAD)**: Can you tell whether a news article was written by a human or an AI?
2. **Academic Writing Detection (AWD)**: Can you identify whether an academic text, such as a research paper, was written by a human or an AI?

**A Large-Scale Benchmark Dataset**

To support this task, researchers have created a massive dataset of 30,000 samples, with an equal number of human-written and AI-generated texts. The AI-generated content was produced using state-of-the-art language models, such as GPT-4 and Claude, and diverse prompting strategies.

**Results and Future Directions**

Forty-six teams registered for the shared task, and four teams submitted their results. While the details of their methods are still being studied, the research highlights the need for continued innovation in detecting AI-generated text. As AI technology advances, it's essential to develop effective tools to distinguish between human and machine-generated content.

The M-DAIGT shared task is an important step towards addressing this challenge, and future research directions will likely focus on improving detection methods and exploring new applications in various domains."
cs.CL,LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models,"The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.",https://arxiv.org/abs/2511.11334v1,2025-11-14T14:13:07Z,"Jian Gao, Richeng Xuan, Zhaolu Kang, Dingshi Liao, Wenxin Huang, Zongmou Huang, Yangdi Xu, Bowen Qin, Zheqi He, Xi Yang, Changjin Li","Here's a summary of the research paper for a general audience:

**Improving AI Language Models for Underrepresented Languages**

Large language models (LLMs) are computer programs that can understand and generate human-like language. However, most of these models have been tested on widely spoken languages like English, Spanish, and Chinese, leaving behind languages with fewer speakers, such as Lao.

To address this gap, researchers have created a new benchmark dataset called LaoBench, specifically designed to evaluate LLMs' language understanding and reasoning abilities in Lao, a Southeast Asian language. The dataset consists of over 17,000 samples that test LLMs' knowledge application, educational content, and translation abilities in Lao, Chinese, and English.

The researchers used a combination of human curation and automated verification to ensure the accuracy and cultural relevance of the dataset. They then tested several state-of-the-art LLMs on LaoBench and found that these models still struggle to master the Lao language across various tasks.

The creation of LaoBench aims to encourage further research and development of AI technologies for underrepresented languages like Lao. This work has the potential to improve the performance of LLMs in these languages, enabling more people to benefit from AI-powered language tools and applications."
cs.CL,NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery,"Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",https://arxiv.org/abs/2511.11324v1,2025-11-14T14:01:18Z,"Anurag J. Vaidya, Felix Meissen, Daniel C. Castro, Shruthi Bannur, Tristan Lazard, Drew F. K. Williamson, Faisal Mahmood, Javier Alvarez-Valle, Stephanie L. Hyland, Kenza Bouzid","Here's a summary of the research paper for a general audience:

**Introducing NOVA: A Game-Changer in Histopathology Analysis**

Histopathology is the study of diseased tissues under a microscope, which is crucial for understanding and diagnosing diseases like cancer. However, analyzing histopathology images is a time-consuming and complex process that requires specialized expertise. To make it more accessible, researchers have developed NOVA, a new framework that automates histopathology analysis.

**What does NOVA do?**

NOVA is a computer program that can translate scientific questions into a series of steps to analyze histopathology images. It uses a library of 49 specialized tools to perform tasks such as identifying cell nuclei and analyzing entire slides. What's more, NOVA can even create new tools on the fly to tackle specific research questions.

**How well does NOVA work?**

To test NOVA's abilities, researchers created a benchmark called SlideQuest, which consists of 90 questions that require multi-step reasoning and problem-solving. NOVA outperformed other computer programs designed for coding and analysis. In a real-world case study, NOVA helped a pathologist link tissue morphology to specific cancer subtypes, demonstrating its potential for discovering new insights.

**Why is NOVA important?**

NOVA has the potential to revolutionize histopathology analysis by making it faster, more accessible, and more scalable. By automating complex workflows, NOVA can help researchers and clinicians focus on high-level questions and discoveries, rather than tedious manual analysis. This could lead to new breakthroughs in disease diagnosis and treatment."
cs.CL,LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models,"Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",https://arxiv.org/abs/2511.11315v1,2025-11-14T13:57:46Z,"Jawad Ibn Ahad, Muhammad Rafsan Kabir, Robin Krambroeckers, Sifat Momen, Nabeel Mohammed, Shafin Rahman","**Unlocking the Power of Large Language Models for Finance**

Large language models have revolutionized the way we analyze and understand text in the financial industry. However, these models require significant computational power, making them inaccessible to many organizations. A team of researchers has developed a new framework called Layer-wise Adaptive Ensemble Tuning (LAET) to make these models more efficient and effective.

**The Problem: High Computational Demands**

Large language models, like those used in finance, are incredibly powerful but require a lot of computing power. This makes them difficult to use for many organizations, limiting their ability to leverage the latest advancements in natural language processing.

**The Solution: LAET Framework**

The LAET framework selectively fine-tunes the most important parts of pre-trained language models, while leaving less critical parts unchanged. This approach significantly reduces the computational overhead required to achieve state-of-the-art results.

**The Results: Improved Performance and Efficiency**

The researchers tested LAET on various financial natural language processing tasks, such as sentiment analysis and stock movement prediction. The results show that LAET outperforms existing benchmarks and even rivals the performance of much larger language models, like GPT-4. The best part? LAET achieves these results using smaller language models with around 3 billion parameters, making it a more accessible and scalable solution for financial applications.

**The Impact: Democratizing Access to Advanced NLP**

The LAET framework has the potential to democratize access to advanced natural language processing capabilities for financial organizations, enabling them to make more informed decisions and drive business growth. By making large language models more efficient and effective, LAET can help bridge the gap between cutting-edge research and real-world deployment in the financial industry."
cs.CL,iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference,"Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).",https://arxiv.org/abs/2511.11306v1,2025-11-14T13:50:51Z,"Wei Fan, JinYi Yoon, Bo Ji","**Improving AI Reasoning with Intelligent Debate**

Large language models (LLMs) have made tremendous progress in recent years, but they can still struggle with complex tasks. One approach to improve their accuracy is to have multiple LLMs engage in a structured debate, known as Multi-Agent Debate (MAD). However, using MAD for every query can be inefficient and costly.

To address this issue, researchers have developed a new framework called intelligent Multi-Agent Debate (iMAD). iMAD selectively triggers MAD only when it's likely to improve the accuracy of the answer. Here's how it works:

1. A single LLM agent provides an initial answer and a self-critique response.
2. The response is analyzed to extract features that indicate the agent's confidence in its answer.
3. A lightweight classifier then decides whether to trigger a debate among multiple LLM agents.

The results are impressive: iMAD reduces the computational cost of MAD by up to 92% while improving the accuracy of the final answer by up to 13.5%. This approach has the potential to make AI systems more efficient and accurate, especially for complex tasks.

**Key Takeaways:**

* iMAD is a new framework that selectively uses multi-agent debate to improve AI reasoning.
* It reduces computational cost and improves accuracy by triggering debates only when necessary.
* iMAD has been tested on six datasets and outperforms existing baselines."
cs.CL,Building the Web for Agents: A Declarative Framework for Agent-Web Interaction,"The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",https://arxiv.org/abs/2511.11287v1,2025-11-14T13:23:34Z,"Sven Schultze, Meike Verena Kietzmann, Nils-Lucas Schönfeld, Ruth Stock-Homburg","**Building a Better Web for AI Agents**

Imagine a future where artificial intelligence (AI) agents can interact with websites as smoothly and securely as humans do. Currently, AI agents struggle to understand the capabilities of websites, leading to clumsy and insecure interactions. To solve this problem, researchers have developed VOIX, a new framework that allows websites to clearly communicate their capabilities to AI agents.

VOIX introduces simple HTML elements, similar to those used to build web pages, that enable websites to explicitly define what actions AI agents can take and what information is available. This creates a clear contract between the website and the AI agent, ensuring that interactions are reliable, secure, and private.

In a recent study, 16 developers were able to quickly build functional web applications using VOIX, regardless of their prior experience. The results suggest that VOIX has the potential to enable seamless and secure interactions between humans, AI agents, and websites. This could lead to a more collaborative and efficient web, where AI agents can assist humans in a wide range of tasks."
cs.CL,Language-Aided State Estimation,"Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.",https://arxiv.org/abs/2511.11285v1,2025-11-14T13:18:37Z,"Yuki Miyoshi, Masaki Inoue, Yusuke Fujimoto","Here's a summary of the research paper for a general audience:

**Using Human Observations to Improve Predictions**

Imagine trying to predict the water level in a canal, but you don't have sensors to measure it directly. You could rely on complex computer models, but they might not be accurate. What if you could tap into the observations of people who are familiar with the canal, like farmers or maintenance workers? That's what researchers have done in a new study.

By analyzing natural language data, such as text or speech, from people who have observed the canal, researchers developed a new method called Language-Aided State Estimation. This method uses a type of artificial intelligence called natural language processing to understand human observations and incorporate them into computer models.

The researchers tested their approach on a real-world problem: estimating the water level in an irrigation canal. They found that by using human observations, they could make more accurate predictions about the water level. This approach has the potential to improve predictions in a wide range of fields, from environmental monitoring to traffic management.

**In simple terms:** This study shows that by combining computer models with human observations expressed in natural language, we can make more accurate predictions about complex systems. This could lead to better decision-making in many areas of our lives."
cs.CL,SQuaD: The Software Quality Dataset,"Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",https://arxiv.org/abs/2511.11265v1,2025-11-14T12:57:22Z,"Mikel Robredo, Matteo Esposito, Davide Taibi, Rafael Peñaloza, Valentina Lenarduzzi","**Introducing SQuaD: A Game-Changing Dataset for Software Quality Research**

Imagine being able to analyze the quality of thousands of software projects, from popular open-source programs like the Linux kernel to widely-used applications like Apache and Mozilla. A team of researchers has created a massive dataset called SQuaD (Software Quality Dataset) that makes this possible. SQuaD is a comprehensive collection of software quality metrics extracted from 450 mature open-source projects, covering a wide range of software ecosystems.

**What makes SQuaD special?**

* **Multi-dimensional**: SQuaD measures various aspects of software quality, including maintainability, technical debt, and vulnerability to security threats.
* **Time-aware**: The dataset spans multiple versions of each project, allowing researchers to track changes and evolution over time.
* **Large-scale**: SQuaD contains data from 63,586 project releases, making it one of the largest software quality datasets available.

**What kind of data does SQuaD contain?**

* **Software metrics**: SQuaD includes over 700 unique metrics, such as code complexity, bug density, and refactoring activity.
* **Version control and issue-tracking histories**: The dataset provides a record of changes made to each project over time.
* **Software vulnerability data**: SQuaD includes information on known security vulnerabilities (CVE/CWE) and process metrics that can help predict defects.

**What can researchers do with SQuaD?**

* **Study software evolution**: Analyze how software projects change and evolve over time.
* **Assess software quality**: Evaluate the maintainability, technical debt, and vulnerability of software projects.
* **Improve defect prediction**: Develop models that predict where defects are likely to occur in software projects.

The SQuaD dataset is publicly available and has the potential to accelerate research in software quality, maintenance, and evolution. Its creators envision future applications, such as automated dataset updates and cross-project quality modeling, to support the continuous evolution of software analytics."
cs.CL,Discovering Meaningful Units with Visually Grounded Semantics from Image Captions,"Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.",https://arxiv.org/abs/2511.11262v1,2025-11-14T12:56:18Z,"Melika Behjati, James Henderson","Here's a summary of the research paper for a general audience:

**Unlocking the Secrets of Images and Text**

Imagine you're looking at a picture of a sunny beach with palm trees and a sailboat. A computer model can look at this image and read a caption that describes it, but can it really understand what's happening in the scene? Researchers are working to improve computer models that can interpret both images and text. They've made a breakthrough by developing a model that can identify meaningful groups of words in captions that correspond to specific objects or scenes in an image.

For example, in the beach scene, the model might identify the words ""palm trees"" or ""sailboat"" as a group that corresponds to a specific part of the image. This allows the model to gain a more detailed understanding of the relationship between the image and the text. The researchers found that their model can discover these meaningful groups of words, which are similar to phrases that humans would use to describe the image.

This research has the potential to improve applications such as image search, object detection, and even self-driving cars, which rely on computer models to understand visual data. By developing more sophisticated models that can interpret both images and text, we can create more intelligent and capable machines that can better understand our world."
cs.CL,KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement,"The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.",https://arxiv.org/abs/2511.11258v1,2025-11-14T12:54:01Z,"Sania Nayab, Marco Simoni, Giulio Rossolini, Andrea Saracino","**Generating Accurate Questions and Answers from Knowledge Graphs**

Imagine a vast library of information, where computers can quickly retrieve answers to your questions. This library is called a knowledge graph (KG). Researchers have been working on creating a system that can automatically generate questions and answers from KGs, which can be used to develop educational platforms, chatbots, and other language models.

The challenge is to create questions and answers that are not only accurate but also easy to understand and fluent in language. Existing methods have struggled with this, often producing low-quality or inconsistent results.

To address this issue, researchers have developed a new approach called KGQuest. This approach uses a template-based system to generate questions and answers from KGs. Here's how it works:

1. **Clustering similar information**: The system groups similar information from the KG into clusters based on their relationships.
2. **Creating reusable templates**: It then creates reusable templates using natural language rules to generate questions and answers.
3. **Refining with language models**: A large language model (LLM) refines these templates to improve their clarity, coherence, and linguistic quality.
4. **Adding answer options**: The system then generates answer options, including distractors (incorrect answers) to make the questions more challenging.

The results show that KGQuest is a scalable and efficient approach that generates high-quality questions and answers, combining accuracy with fluency and linguistic precision. This breakthrough has the potential to improve the development of educational platforms, chatbots, and other language models."
cs.CL,LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation,"Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.",https://arxiv.org/abs/2511.11234v1,2025-11-14T12:37:20Z,"Jader Martins Camboim de Sá, Jooyoung Lee, Cédric Pruski, Marcos Da Silveira","**Improving Language Models' Understanding of Word Meanings**

Researchers have developed a new method called LANE to help language models better understand the nuances of word meanings. Currently, these models often struggle to distinguish between similar words or phrases, leading to inaccurate interpretations. LANE addresses this challenge by creating ""adversarial"" training examples that intentionally confuse the model, forcing it to focus on the specific word being used.

The LANE method works by marking certain words in training sentences and then asking the model to distinguish between sentences with and without the marked words. This process helps the model learn to create more distinct representations of words with different meanings.

In tests, the LANE approach improved the model's performance on tasks such as detecting changes in word meanings and disambiguating words with multiple senses. The researchers also found that the method led to more nuanced and accurate representations of word meanings, even in complex contexts.

The best part about LANE is that it's a flexible method that can be used with existing language models, making it a promising tool for improving the accuracy and reliability of natural language processing applications."
cs.CL,Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy,"WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.",https://arxiv.org/abs/2511.11214v1,2025-11-14T12:12:10Z,"Jooyoung Lee, Jader Martins Camboim de Sá","**Improving Language Understanding: A New Classification System for Adverbs**

Researchers have made a significant update to WordNet, a large database of English words and their meanings. While WordNet already provides detailed classifications for nouns and verbs, it lacked a systematic way to categorize adverbs. Adverbs are words that modify verbs, adjectives, or other adverbs, and they play a crucial role in conveying meaning in language.

The researchers developed a new classification system for adverbs, which groups them into categories such as:

* Manner (e.g., quickly, loudly)
* Time (e.g., yesterday, soon)
* Frequency (e.g., often, rarely)
* Degree (e.g., very, extremely)
* Domain (e.g., technically, financially)
* Speaker-oriented (e.g., honestly, frankly)
* Subject-oriented (e.g., deliberately, intentionally)

To test the effectiveness of this new system, the researchers asked human annotators to categorize adverbs from natural text. The results showed that the categories are clear and easy to understand, and they provide broad coverage of adverbs in everyday language.

This update to WordNet has significant implications for natural language processing (NLP) applications, such as:

* Word sense disambiguation (determining the correct meaning of a word in context)
* Event extraction (identifying events and their details from text)
* Sentiment analysis (understanding the emotional tone of text)
* Discourse modeling (analyzing how language is used to convey meaning in different contexts)

Overall, this research improves our understanding of language and has the potential to enhance various NLP applications."
cs.CL,Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning,"Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like ""Who is Undercover?"". MUG reframes MAD as a process of detecting ""undercover"" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.",https://arxiv.org/abs/2511.11182v1,2025-11-14T11:27:55Z,"Dayong Liang, Xiao-Yong Wei, Changmeng Zheng","**Improving AI Reasoning: A New Game-Like Approach**

Large language models (LLMs) are powerful AI tools that can process and generate human-like text. However, they often struggle with ""hallucination,"" where they provide incorrect or made-up information. To address this issue, researchers have proposed a new approach called Multi-agent Undercover Gaming (MUG).

In MUG, multiple AI agents engage in a game-like discussion to identify which agents are providing incorrect information (or ""hallucinating""). The agents are presented with images and asked to discuss and verify the information. To make the test more robust, the researchers modify the images to introduce ""counterfactual evidence"" - essentially, they change the image to see if the agents can detect the changes.

The MUG approach has three key advantages:

1. **Factual verification**: It allows for a more accurate check of facts beyond just relying on what the majority of agents agree on.
2. **Cross-evidence reasoning**: Agents can use multiple sources of information to make their decisions, rather than just relying on a single input.
3. **Active reasoning**: Agents engage in a more dynamic and interactive discussion, rather than simply answering questions.

By using this game-like approach, researchers hope to create more reliable and effective AI models that can accurately process and generate information. The source code for MUG is now available, making it possible for others to build on this research."
cs.CL,PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases,"Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.",https://arxiv.org/abs/2511.11141v1,2025-11-14T10:19:04Z,"Udo Schlegel, Franziska Weeber, Jian Lan, Thomas Seidl","Here's a summary of the research paper for a general audience:

**Evaluating the Reliability of AI Models: A New Metric**

Imagine you're searching for an image on a website, and you type in a sentence to describe what you're looking for. The website uses an AI model to find images that match your description. But what if you rephrase your sentence slightly? Would the AI model still find the same images?

Researchers have developed a new metric, called the Paraphrase Ranking Stability Metric (PRSM), to evaluate how well AI models, specifically a type called CLIP, can handle rephrased sentences. CLIP is a powerful model that can match text and images, but its ability to handle variations in language, such as paraphrasing, is crucial for reliable deployment, especially in sensitive areas like social media.

The researchers tested CLIP's robustness to paraphrasing using a dataset designed to reveal biases in AI models. They found that CLIP's performance varies depending on the type of paraphrasing used, and that there are subtle but consistent differences in how well the model handles sentences associated with men versus women. This has important implications for ensuring that AI models are fair and equitable, particularly in socially sensitive contexts.

In simple terms, this research aims to make AI models more reliable and fair by evaluating their ability to handle variations in language, and to identify potential biases in their performance."
stat.ML,Multicalibration yields better matchings,"Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.   In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.",https://arxiv.org/abs/2511.11413v1,2025-11-14T15:45:07Z,"Riccardo Colini Baldeschi, Simone Di Gregorio, Simone Fioravanti, Federico Fusco, Ido Guy, Daniel Haimovich, Stefano Leonardi, Fridolin Linder, Lorenzo Perini, Matteo Russo, Niek Tax","**Improving Decision-Making with Better Predictions**

Imagine you're trying to find the best route for a delivery truck to take, but you don't know the exact traffic conditions. You rely on a predictor, like a traffic app, to give you an estimate of the traffic. However, this predictor is not always accurate. How can you make the best decision with imperfect information?

Researchers have proposed a solution called ""multicalibration"" to improve decision-making in situations like this. Multicalibration is a way to adjust predictions to ensure they are unbiased for different groups of contexts. For example, in the traffic scenario, it would ensure that the predictor is accurate for different types of roads, times of day, or weather conditions.

The study shows that by using multicalibration, you can create a more accurate predictor that leads to better decisions. Specifically, when trying to find the best matching (or route) in a complex system, using a multicalibrated predictor can perform as well as or better than traditional methods that rely on perfect information.

The researchers also provide guidelines on how to implement multicalibration and estimate how much data is needed to achieve accurate results. This work has implications for a wide range of applications, from logistics and transportation to healthcare and finance, where making informed decisions with imperfect information is a common challenge."
stat.ML,Model Class Selection,"Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided.",https://arxiv.org/abs/2511.11355v1,2025-11-14T14:43:26Z,"Ryan Cecil, Lucas Mentch","**Model Class Selection: A New Approach to Choosing the Best Statistical Model**

When analyzing data, researchers often use statistical models to make sense of the information. The goal is to find the best model that fits the data. Traditionally, the focus has been on finding a single ""best"" model. However, a new approach called model class selection (MCS) takes it a step further. MCS allows researchers to compare multiple groups of models and identify all the groups that contain at least one good model.

This approach is useful because it enables researchers to ask questions like: Can simpler models, which are easier to understand, perform just as well as more complex models, like those used in machine learning? MCS provides a way to answer this question by evaluating multiple model collections and identifying those that contain near-optimal models.

The researchers behind this study developed a method called data splitting, which helps to identify the best model classes. They tested their approach using simulated and real-world data, and the results show that it works well. This new approach has the potential to help researchers choose the best statistical models for their data, and to compare the performance of different types of models."
stat.ML,Dual Riemannian Newton Method on Statistical Manifolds,"In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference.",https://arxiv.org/abs/2511.11318v1,2025-11-14T13:58:34Z,"Derun Zhou, Keisuke Yano, Mahito Sugiyama","**Unlocking Faster and More Efficient Optimization in Statistical Modeling**

Imagine you're trying to fit a complex model to a large dataset. You want to find the best parameters for your model, but the process can be slow and inefficient. Researchers have proposed a new optimization algorithm, called the dual Riemannian Newton method, to speed up this process.

**What's the problem with current methods?**

Current optimization methods, like the natural gradient, are ""first-order"" methods, which means they only look at the slope of the error surface. However, these methods can be slow to converge, especially when getting close to the optimal solution.

**How does the new method work?**

The dual Riemannian Newton method is a ""second-order"" method, which means it takes into account both the slope and the curvature of the error surface. This allows it to make more informed updates and converge faster. The method is designed to work on curved spaces, called manifolds, which are common in statistical modeling.

**The key innovation: duality**

The dual Riemannian Newton method leverages a fundamental concept in information geometry called duality. This means that the algorithm uses two related connections (or ""ways of measuring distance"") on the manifold to make updates. This allows it to respect the underlying structure of the problem and converge quickly.

**What does this mean for statistical modeling?**

The dual Riemannian Newton method offers a more efficient and effective way to optimize complex statistical models. This can lead to faster convergence, more accurate results, and improved performance in a wide range of applications, from machine learning to signal processing.

**In summary**

The dual Riemannian Newton method is a new optimization algorithm that combines the benefits of second-order optimization with the flexibility of working on curved spaces. By leveraging duality, the algorithm achieves faster convergence and improved efficiency, making it a valuable tool for statistical modeling and machine learning."
stat.ML,Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint,"Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.",https://arxiv.org/abs/2511.11294v1,2025-11-14T13:27:54Z,"Bertille Tierny, Arthur Charpentier, François Hu","**Fairness in AI: Uncovering Hidden Biases in Linear Models**

When making important decisions, such as hiring or lending, AI models are often used to predict outcomes. However, these models can perpetuate biases and unfairness, particularly against certain demographic groups. To address this issue, researchers have proposed fairness constraints, such as demographic parity, which aim to ensure that the model's predictions are fair and unbiased.

But how do these fairness constraints affect the model's behavior? A new study provides a framework to analyze and understand the biases in linear models, a type of AI model widely used in high-stakes decision-making. The researchers developed a method to break down the biases in these models into two types:

1. **Direct bias**: bias that comes from the sensitive attribute (e.g., age, sex, or ethnicity) used in the model.
2. **Indirect bias**: bias that comes from other features that are correlated with the sensitive attribute.

The study found that fairness constraints, such as demographic parity, can reshape the model's coefficients, affecting how predictive bias is distributed across features. The researchers' framework provides a transparent and feature-level interpretation of fairness interventions, revealing how bias may persist or shift through correlated variables.

The good news is that this framework can be applied to any linear model without requiring retraining, making it a practical tool for model auditing and mitigation. The study's experiments on synthetic and real-world datasets demonstrated that the method captures fairness dynamics that were missed by prior work.

Overall, this research provides a valuable tool for understanding and addressing biases in AI models, enabling more responsible and fair deployment of these models in high-stakes decision-making applications."
stat.ML,A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates,"In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.",https://arxiv.org/abs/2511.11211v1,2025-11-14T12:10:23Z,"Wei-Cheng Lee, Francesco Orabona","**Breaking Down a Complex Algorithm: A Simpler Proof**

Imagine you're trying to choose the best option from several alternatives, but you're not sure which one will work best. This is known as a ""multi-armed bandit"" problem, a classic challenge in machine learning. Researchers have developed algorithms to help solve this problem, and one of them is called Tsallis-INF.

In a recent paper, scientists J. Zimmert and Y. Seldin introduced Tsallis-INF, an algorithm that can handle both random and adversarial situations. However, their proof of the algorithm's effectiveness was complex and relied on advanced mathematical tools.

In this new note, researchers have found a simpler way to prove that Tsallis-INF works well in both random and adversarial situations. They used modern tools from online convex optimization, a field that deals with making decisions in complex situations. The best part? They avoided using complicated mathematical functions called ""conjugate functions,"" making the proof easier to understand.

The new proof doesn't focus on getting the most precise results, but rather on providing a clear and simple explanation of why Tsallis-INF works. This breakthrough can help make the algorithm more accessible and useful for a wider range of applications."
stat.ML,Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths,"This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.",https://arxiv.org/abs/2511.11161v1,2025-11-14T10:56:52Z,"Yuzhen Zhao, Yating Liu, Marc Hoffmann","**Unlocking Hidden Patterns in Data: A New Method for Estimating Drift in Complex Systems**

Imagine you're trying to understand how a complex system, like a population of animals or a financial market, changes over time. One key aspect of these systems is the ""drift"" - a kind of underlying trend that drives their behavior. But what if you only have limited, scattered data points to work with?

A new research paper proposes a solution to this problem using neural networks, a type of artificial intelligence inspired by the human brain. The authors developed a method to estimate the drift function in complex systems, even when the data is limited and noisy.

**The Problem and the Solution**

The researchers focused on a type of complex system called a diffusion process, which is commonly used to model everything from stock prices to animal migration patterns. They wanted to estimate the drift function, which describes the underlying trend that drives the system's behavior.

The twist is that they only had high-frequency discrete observations from multiple independent trajectories - think of it like having many short snapshots of the system's behavior, but not a continuous video. To tackle this challenge, they turned to neural networks.

**How it Works**

The researchers proposed a neural network-based estimator that can learn the drift function from the limited data. They showed that their method converges to the true drift function at a certain rate, which depends on the number of data points, the complexity of the system, and the quality of the neural network.

**What the Results Show**

In numerical experiments, the researchers tested their method on a complex drift function with local fluctuations. They found that their neural network estimator outperformed a traditional method (B-spline) in terms of convergence rate and ability to capture local features, especially in high-dimensional settings.

**Why it Matters**

This research has important implications for fields like finance, ecology, and medicine, where understanding complex systems is crucial. The proposed method offers a powerful tool for estimating drift functions in these systems, even when data is limited and noisy. By unlocking hidden patterns in data, this research can help scientists and practitioners make more informed decisions and predictions."
stat.ML,Learning bounds for doubly-robust covariate shift adaptation,"Distribution shift between the training domain and the test domain poses a key challenge for modern machine learning. An extensively studied instance is the \emph{covariate shift}, where the marginal distribution of covariates differs across domains, while the conditional distribution of outcome remains the same. The doubly-robust (DR) estimator, recently introduced by \cite{kato2023double}, combines the density ratio estimation with a pilot regression model and demonstrates asymptotic normality and $\sqrt{n}$-consistency, even when the pilot estimates converge slowly. However, the prior arts has focused exclusively on deriving asymptotic results and has left open the question of non-asymptotic guarantees for the DR estimator.   This paper establishes the first non-asymptotic learning bounds for the DR covariate shift adaptation. Our main contributions are two-fold: (\romannumeral 1) We establish \emph{structure-agnostic} high-probability upper bounds on the excess target risk of the DR estimator that depend only on the $L^2$-errors of the pilot estimates and the Rademacher complexity of the model class, without assuming specific procedures to obtain the pilot estimate, and (\romannumeral 2) under \emph{well-specified parameterized models}, we analyze the DR covariate shift adaptation based on modern techniques for non-asymptotic analysis of MLE, whose key terms governed by the Fisher information mismatch term between the source and target distributions. Together, these findings bridge asymptotic efficiency properties and a finite-sample out-of-distribution generalization bounds, providing a comprehensive theoretical underpinnings for the DR covariate shift adaptation.",https://arxiv.org/abs/2511.11003v1,2025-11-14T06:46:23Z,"Jeonghwan Lee, Cong Ma","**Improving Machine Learning Models for Real-World Applications**

Machine learning models are often trained on data from one environment, but used in another. This can lead to poor performance because the data distributions may differ. Researchers have been working to adapt models to new environments, a problem known as ""covariate shift."" A promising approach called the doubly-robust (DR) estimator has shown great potential, but its theoretical guarantees were only understood in the limit of infinite data.

A new study provides a deeper understanding of the DR estimator's performance in real-world scenarios with limited data. The researchers derived **non-asymptotic learning bounds**, which provide a guarantee on the DR estimator's performance with a finite amount of data. These bounds show that the DR estimator can adapt to new environments and provide good performance, even when the initial estimates are not very accurate.

The study's findings have two key implications:

1. **Flexibility**: The DR estimator's performance can be understood without relying on specific methods for initial estimates. This makes it a flexible tool for various applications.
2. **Theoretical foundation**: The study provides a comprehensive theoretical framework for the DR estimator, bridging the gap between its asymptotic efficiency and finite-sample performance. This foundation can help build trust in the DR estimator's ability to generalize to new environments.

Overall, this research provides a crucial step towards developing machine learning models that can adapt to real-world changes and improve their performance in new environments."
stat.ML,Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data,"Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.",https://arxiv.org/abs/2511.10919v1,2025-11-14T03:15:31Z,"Jialei Liu, Jun Liao, Kuangnan Fang","**Improving Predictions with Limited Data: A New Transfer Learning Approach**

In many high-stakes fields like healthcare and finance, making accurate predictions can be challenging due to limited data, especially when there is a lack of clear ""negative"" examples. A new research paper proposes a solution to this problem by developing a transfer learning framework that combines information from different data sources without sharing the data directly.

The approach uses a technique called model averaging, which allows researchers to merge predictions from different models trained on various data sets, including those with fully labeled data, semi-supervised data, and data with only positive examples (known as Positive-Unlabeled or PU data). The method determines the optimal weights for combining these models to produce more accurate predictions.

The researchers tested their approach through simulations and real-world analyses of credit risk data. The results show that their method outperforms other approaches in terms of predictive accuracy and robustness, particularly when there is limited labeled data and diverse data sources.

This new approach has the potential to improve predictions in various fields where data is scarce or sensitive, enabling more accurate and reliable decision-making."
stat.ML,Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters,"Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.",https://arxiv.org/abs/2511.10898v1,2025-11-14T02:21:30Z,"Chenghao Duan, Chuanyi Ji","**Predicting Power Outage Durations with Graph Attention Networks**

Natural disasters like hurricanes and wildfires can cause widespread power outages, leading to significant economic and social impacts. A new study proposes a machine learning approach to predict how long it will take to restore power after such events. The researchers developed a Graph Attention Network (GAT) model that uses data on weather patterns, geography, and past power outages to estimate recovery times.

The study used data from four major hurricanes that affected over 500 counties in the southeastern United States. The results show that the GAT model is highly accurate, with an accuracy rate of over 93%. This is a significant improvement over existing methods, outperforming them by 2-15%.

The study's findings have important implications for power grid resilience and disaster response. By accurately predicting power outage durations, utilities and emergency responders can better prepare for and respond to natural disasters, ultimately reducing the impact on communities and economies."
stat.ML,LLM enhanced graph inference for long-term disease progression modelling,"Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.",https://arxiv.org/abs/2511.10890v1,2025-11-14T02:03:10Z,"Tiantian He, An Zhao, Elinor Thompson, Anna Schroder, Ahmed Abdulaal, Frederik Barkhof, Daniel C. Alexander","**Unlocking the Secrets of Brain Disease Progression**

Researchers have developed a new framework to better understand how neurodegenerative diseases, such as Alzheimer's, progress over time. The framework combines advanced machine learning techniques with expert knowledge to model the complex interactions between different brain regions.

Currently, methods for predicting disease progression oversimplify the connections between brain regions, leading to inaccurate predictions. The new approach uses Large Language Models (LLMs) to guide the learning process, incorporating diverse disease-driving mechanisms and multi-modal relationships. This allows researchers to:

1. **Reconstruct long-term disease trajectories**: By analyzing irregularly sampled data from individual patients, the framework creates personalized disease progression paths.
2. **Map brain region interactions**: The framework identifies the biologically-constrained graph structure that captures interactions among brain regions, providing a more accurate representation of disease spread.

In a test using tau-PET imaging data from Alzheimer's patients, the new framework outperformed traditional approaches in predicting disease progression and provided new insights into disease-driving factors. This breakthrough has the potential to improve our understanding of neurodegenerative diseases and ultimately lead to more effective treatments."
stat.ML,Private Zeroth-Order Optimization with Public Data,"One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.",https://arxiv.org/abs/2511.10859v1,2025-11-13T23:51:24Z,"Xuchen Gong, Tian Li","Here's a summary of the research paper for a general audience:

**Improving Private Machine Learning with Public Data**

Machine learning algorithms are increasingly used in various applications, but they often require access to sensitive data, which raises privacy concerns. To address this, researchers have developed methods to make these algorithms ""differentially private"" (DP), meaning they protect individual data while still providing useful insights. However, popular DP algorithms can be computationally expensive and require a lot of memory.

This study explores a new approach to private machine learning called ""zeroth-order optimization,"" which uses function evaluations to approximate gradients, making it easier to protect data. The researchers propose using public data to guide and improve the performance of these private zeroth-order algorithms. They developed a suite of algorithms, called PAZO, which leverage public data to achieve better results.

The study shows that PAZO algorithms outperform existing state-of-the-art methods, especially in situations where high levels of privacy are required. Additionally, PAZO algorithms are significantly faster, offering up to 16 times runtime speedup. The results demonstrate the potential of using public data to improve private machine learning, making it more efficient and effective.

**In simple terms:** Researchers have found a way to make machine learning algorithms more private and efficient by using public data to help guide the learning process. This approach shows promise in protecting sensitive data while still providing accurate results, and could lead to faster and more effective machine learning models."
stat.ML,Neural Local Wasserstein Regression,"We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.",https://arxiv.org/abs/2511.10824v1,2025-11-13T21:54:18Z,"Inga Girshfeld, Xiaohui Chen","**Unlocking Complex Relationships between Distributions**

Imagine you're trying to understand how different factors, like age or income, affect the distribution of people's heights. Traditional statistical methods often fall short in capturing these complex relationships, especially when dealing with high-dimensional data. A new approach, called Neural Local Wasserstein Regression, offers a more flexible and accurate way to model these relationships.

**The Problem: Distribution-on-Distribution Regression**

In many real-world scenarios, both the inputs (predictors) and outputs (responses) are probability distributions, rather than single numbers. For instance, you might want to predict the distribution of exam scores based on the distribution of hours studied. Existing methods often rely on oversimplified assumptions, which can lead to inaccurate results.

**The Solution: Neural Local Wasserstein Regression**

This new method uses a combination of neural networks and optimal transport theory to model the relationships between distributions. The key innovation is to focus on local neighborhoods in the data, rather than trying to find a global solution. This allows the model to adapt to complex geometries and capture nonlinear relationships.

**How it Works**

The method uses a technique called kernel weighting, which assigns more importance to nearby data points when making predictions. Neural networks are used to parameterize the transport operators, which describe how to transform one distribution into another. The approach also uses a clever strategy to select reference points, making it more scalable and efficient.

**Results and Applications**

The authors tested their method on synthetic data and real-world tasks, such as predicting the distribution of handwritten digits (MNIST). The results show that Neural Local Wasserstein Regression can effectively capture complex relationships between distributions, outperforming existing methods.

**Implications and Future Directions**

This research has the potential to unlock new insights in various fields, such as economics, finance, and healthcare, where understanding distributional relationships is crucial. The method can be applied to a wide range of problems, from predicting the distribution of disease outcomes to modeling the impact of policy interventions on economic distributions."
stat.ML,Potential Outcome Rankings for Counterfactual Decision Making,"Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.",https://arxiv.org/abs/2511.10776v1,2025-11-13T19:58:19Z,"Yuta Kawakami, Jin Tian","**Making Better Decisions in Uncertain Situations**

Imagine you're faced with a tough decision, like choosing a new treatment for a medical condition or selecting a different investment option. You want to make the best choice, but the outcome is uncertain. Researchers have developed a new approach to help decision-makers in such situations.

The approach uses a technique called counterfactual decision-making, which involves analyzing what would happen if you chose one option over another. The researchers introduced two new metrics:

1. **Probability of Potential Outcome Ranking (PoR)**: This metric shows the most likely ranking of possible outcomes for a given decision. For example, if you're considering two treatments, PoR might tell you that Treatment A is likely to have a better outcome than Treatment B 60% of the time.
2. **Probability of Achieving the Best Potential Outcome (PoB)**: This metric identifies the action that is most likely to lead to the best outcome. Using the same example, PoB might tell you that Treatment A has an 80% chance of achieving the best outcome.

The researchers developed mathematical formulas to calculate these metrics and established rules for estimating them from data. They also tested their approach using computer simulations and applied it to a real-world dataset.

This new approach can help decision-makers in various fields, such as medicine, finance, and policy-making, to make more informed choices in uncertain situations. By providing a clearer understanding of the potential outcomes of different actions, it can lead to better decision-making and more effective outcomes."
stat.ML,Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem,"The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.",https://arxiv.org/abs/2511.10619v1,2025-11-13T18:46:56Z,"Avrim Blum, Marten Garicano, Kavya Ravichandran, Dravyansh Sharma","**Improving Decision-Making under Uncertainty: A New Approach**

Imagine you're a researcher trying to decide which new technology to invest in, or a doctor trying to determine the best treatment for a patient. You're faced with multiple options, and each one has uncertain outcomes. How do you make the best decision?

Researchers have developed a mathematical model called the ""improving multi-armed bandits problem"" to help with such decisions. The model represents each option as an ""arm"" that can be ""pulled"" to get a reward. The reward increases over time, but at a decreasing rate.

Previous algorithms for solving this problem have had limited success, with guarantees that are not very strong. However, a new study proposes two new families of algorithms that can make better decisions.

The first family of algorithms can achieve stronger guarantees, meaning they can make more accurate decisions, if the reward curves have certain properties. For example, if the rewards increase smoothly and predictably, the algorithm can make better choices.

The second family of algorithms is more robust and can handle a wider range of situations. It can identify the best option if the rewards are well-behaved, and it can fall back to a more conservative approach if the rewards are uncertain.

The study takes a statistical learning perspective, which allows for stronger guarantees without requiring the algorithm to verify certain assumptions. This approach can lead to better decision-making under uncertainty, with applications in areas such as research and development, clinical trials, and machine learning.

**In simple terms:** This study proposes new algorithms for making decisions under uncertainty. The algorithms can make more accurate choices if the rewards are predictable, and they can handle uncertain situations. The approach has the potential to improve decision-making in various fields."
stat.ML,Continuum Dropout for Neural Differential Equations,"Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.",https://arxiv.org/abs/2511.10446v1,2025-11-13T16:10:45Z,"Jonghun Lee, YongKyung Oh, Sungil Kim, Dong-Young Lim","**Improving Neural Differential Equations with Continuum Dropout**

Neural Differential Equations (NDEs) are a powerful tool for modeling how things change over time, handling challenges like irregular data and noise. However, they can suffer from overfitting, where the model becomes too specialized to the training data and fails to generalize well to new data. To address this issue, researchers have introduced a new regularization technique called Continuum Dropout.

**What is Continuum Dropout?**

Continuum Dropout is a method that adapts the popular dropout technique, commonly used in deep learning, to work with NDEs. Dropout randomly ""drops out"" or sets to zero some of the model's neurons during training, preventing the model from relying too heavily on any individual neuron. Continuum Dropout takes this idea a step further by applying it in continuous time, allowing the model to switch between active and inactive states.

**Benefits of Continuum Dropout**

The researchers found that Continuum Dropout outperforms existing regularization methods for NDEs, achieving better performance on various tasks, including time series forecasting and image classification. Additionally, Continuum Dropout provides a way to quantify predictive uncertainty, which is essential for making informed decisions in many applications. By using Continuum Dropout, models can provide more accurate and trustworthy predictions.

**Why does it matter?**

The development of Continuum Dropout has significant implications for a wide range of applications, from finance and economics to climate modeling and healthcare. By improving the accuracy and reliability of NDEs, Continuum Dropout can help researchers and practitioners make more informed decisions and predictions, ultimately leading to better outcomes."
stat.ML,Diffusion annealed Langevin dynamics: a theoretical study,"In this work we study the diffusion annealed Langevin dynamics, a score-based diffusion process recently introduced in the theory of generative models and which is an alternative to the classical overdamped Langevin diffusion. Our goal is to provide a rigorous construction and to study the theoretical efficiency of these models for general base distribution as well as target distribution. As a matter of fact these diffusion processes are a particular case of Nelson processes i.e. diffusion processes with a given flow of time marginals.   Providing existence and uniqueness of the solution to the annealed Langevin diffusion leads to proving a Poincaré inequality for the conditional distribution of $X$ knowing $X+Z=y$ uniformly in $y$, as recently observed by one of us and her coauthors. Part of this work is thus devoted to the study of such Poincaré inequalities. Additionally we show that strengthening the Poincaré inequality into a logarithmic Sobolev inequality improves the efficiency of the model.",https://arxiv.org/abs/2511.10406v1,2025-11-13T15:26:42Z,"Patrick Cattiaux, Paula Cordero-Encinar, Arnaud Guillin","Here's a summary of the research paper for a general audience:

**Understanding a New Method for Generating Models**

Researchers have been exploring a new approach called ""diffusion annealed Langevin dynamics"" to create generative models, which are a type of artificial intelligence that can generate new data, such as images or text. This method is an alternative to traditional approaches and has shown promise, but its underlying math and effectiveness were not well understood.

**What did the researchers do?**

The researchers studied this new approach in depth, providing a solid foundation for its use and analyzing its efficiency. They found that this method is connected to a broader class of mathematical models called Nelson processes. By proving that the new approach has a unique solution, the researchers also discovered a key mathematical property called a Poincaré inequality.

**What does this mean?**

In simple terms, the researchers made progress in understanding a new tool for generating models. They showed that this tool can be effective and even more efficient under certain conditions. This work has implications for the development of more accurate and efficient generative models, which could lead to advancements in areas such as image and speech recognition, and data generation.

**Why is this important?**

Generative models have many practical applications, such as generating synthetic data for training AI models, creating new images or music, and even helping to analyze complex data. By improving our understanding of these models and their underlying math, researchers can develop more powerful and efficient tools for a wide range of applications."
stat.ML,Operator Models for Continuous-Time Offline Reinforcement Learning,"Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.",https://arxiv.org/abs/2511.10383v1,2025-11-13T14:58:30Z,"Nicolas Hoischen, Petar Bevanda, Max Beier, Stefan Sosnowski, Boris Houska, Sandra Hirche","**Advancements in Offline Reinforcement Learning: A Breakthrough in Decision-Making**

Imagine you're trying to teach a self-driving car how to navigate through busy streets without actually driving it. This is essentially what offline reinforcement learning is all about - learning from existing data to make decisions in complex situations. Researchers have made significant progress in this field by developing a new approach that links reinforcement learning to a fundamental mathematical equation, the Hamilton-Jacobi-Bellman equation.

The team proposes an algorithm that uses a dynamic programming recursion to learn from historical data. By representing the world model in terms of the infinitesimal generator of controlled diffusion processes, they establish a robust framework for offline reinforcement learning. This approach integrates statistical learning methods and operator theory, providing a solid foundation for making accurate predictions.

The study's key findings include:

* **Global convergence of the value function**: The algorithm can accurately learn from offline data and make reliable decisions.
* **Finite-sample guarantees**: The researchers derived bounds on the algorithm's performance, which are tied to system properties such as smoothness and stability.

The results suggest that operator-based approaches hold promise in solving offline reinforcement learning problems using continuous-time optimal control. This breakthrough has significant implications for various fields, including healthcare, autonomous driving, and industrial control, where direct interaction with the environment is often impractical or unsafe. By leveraging offline data, this approach can lead to more efficient and effective decision-making in complex systems."
stat.ML,Masking criteria for selecting an imputation model,"The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models.",https://arxiv.org/abs/2511.10048v1,2025-11-13T07:47:41Z,"Yanjiao Yang, Daniel Suen, Yen-Chi Chen","**Improving Data Analysis: A New Way to Choose the Right Model**

When working with data, it's common to have missing values. To fill in these gaps, researchers use ""imputation models."" But how do they choose the best model? A popular method, called ""masking-one-out"" (MOO), involves hiding a known data point, imputing its value, and comparing it to the original value. However, this method has limitations.

Researchers have found that MOO measures prediction accuracy, not the model's overall quality. To address this, they've developed three new criteria that consider the random nature of data. These criteria use rank transformation, energy distance, and likelihood principles to evaluate imputation models.

The likelihood approach offers a robust framework for learning imputation models from data. It also provides a way to assess the model's performance and choose the best one. Additionally, researchers have created a visual tool, the ""prediction-imputation diagram,"" which compares the prediction and imputation abilities of different models.

This study provides a more comprehensive understanding of how to select the best imputation model, leading to more accurate data analysis and better decision-making. By using these new criteria and tools, researchers can ensure that their results are reliable and trustworthy."
stat.ML,A Novel Data-Dependent Learning Paradigm for Large Hypothesis Classes,"We address the general task of learning with a set of candidate models that is too large to have a uniform convergence of empirical estimates to true losses. While the common approach to such challenges is SRM (or regularization) based learning algorithms, we propose a novel learning paradigm that relies on stronger incorporation of empirical data and requires less algorithmic decisions to be based on prior assumptions. We analyze the generalization capabilities of our approach and demonstrate its merits in several common learning assumptions, including similarity of close points, clustering of the domain into highly label-homogeneous regions, Lipschitzness assumptions of the labeling rule, and contrastive learning assumptions. Our approach allows utilizing such assumptions without the need to know their true parameters a priori.",https://arxiv.org/abs/2511.09996v1,2025-11-13T06:02:55Z,"Alireza F. Pour, Shai Ben-David","**Unlocking Efficient Learning with Large Model Sets**

Imagine trying to find the best model to predict something, like how well a new employee will perform, based on a huge set of possible models. Traditional methods often struggle with this task because they rely on simplifying assumptions or complex algorithms. A new approach, proposed by researchers, aims to change this by making better use of the data itself.

The researchers' novel learning paradigm, which they call data-dependent learning, incorporates more information from the data and requires fewer decisions based on prior assumptions. This approach allows for more efficient learning, even when dealing with a large set of possible models.

The benefits of this approach are significant. It can handle situations where:

* Similar things tend to behave similarly
* The data can be grouped into clear categories
* The relationships between variables are smooth and continuous
* Data can be learned in a more nuanced, contrastive way

The best part? This approach doesn't require knowing the exact details of these relationships beforehand. Instead, it can discover them from the data itself. This breakthrough has the potential to improve the accuracy and efficiency of machine learning models in a wide range of applications."
stat.ML,Global Convergence of Four-Layer Matrix Factorization under Random Initialization,"Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.",https://arxiv.org/abs/2511.09925v1,2025-11-13T03:40:10Z,"Minrui Luo, Weihang Xu, Xiang Gao, Maryam Fazel, Simon Shaolei Du","Here's a summary of the research paper for a general audience:

**Breakthrough in Deep Learning: A New Understanding of How Matrix Factorization Works**

Imagine you're trying to break down a complex puzzle into simpler pieces. This is similar to what matrix factorization does in computer science. Researchers have been studying a specific type of matrix factorization, called deep matrix factorization, which is like breaking down the puzzle into multiple layers.

The goal is to understand how a popular algorithm, called gradient descent, works with this type of matrix factorization. Gradient descent is like a navigation system that helps the algorithm find the best solution. However, until now, there was no guarantee that this algorithm would always find the correct solution, especially when started with a random initial guess.

This research provides a major breakthrough by proving that, under certain conditions, the algorithm will always converge to the correct solution in a reasonable amount of time, even when started with a random initial guess. This is a significant advancement in the field of deep learning, which is a subset of artificial intelligence.

The researchers achieved this by developing new techniques to analyze the behavior of the algorithm and showing that it can avoid getting stuck in ""saddle points"" - situations where the algorithm appears to be making progress, but is actually not getting closer to the solution. This work lays the foundation for further research and could lead to improvements in various applications of deep learning, such as image and speech recognition, natural language processing, and more."
