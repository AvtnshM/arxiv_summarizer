category,title,summary,link,published,authors,summary_short
cs.LG,Generative View Stitching,"Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\""ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.",http://arxiv.org/abs/2510.24718v1,2025-10-28T17:59:58Z,"Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann","Here's a summary of the research paper ""Generative View Stitching"" for a general audience:

**Imagine a Video that Never Ends (or Loops Back on Itself)**

Researchers have made progress in creating artificial videos that can be generated on the fly, frame by frame. However, these videos often have limitations, such as objects colliding with each other or the video ""breaking"" when the camera moves in certain ways.

To address these issues, a team of researchers has developed a new technique called Generative View Stitching (GVS). This technique allows for the creation of seamless, long videos that can be guided by a predefined camera path. In other words, imagine a video that can follow a camera as it moves through a virtual scene, without any glitches or collisions.

**The Key Innovation: Stitching Together Frames**

The GVS technique works by generating all frames of the video at once, rather than one by one. This allows the researchers to ""stitch"" together different parts of the video to create a seamless and coherent scene. The technique is compatible with existing video generation models, making it a flexible and powerful tool.

**The Result: Smooth, Collision-Free Videos**

The researchers have demonstrated the effectiveness of GVS by generating videos with complex camera paths, including one that follows a famous ""impossible staircase"" illusion. The resulting videos are smooth, collision-free, and frame-to-frame consistent, even when the camera loops back on itself.

Overall, Generative View Stitching represents a significant advance in video generation technology, with potential applications in fields such as computer graphics, animation, and virtual reality."
cs.LG,A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel   Optimization,"We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.",http://arxiv.org/abs/2510.24710v1,2025-10-28T17:58:17Z,"Wei Shen, Jiawei Zhang, Minhui Huang, Cong Shen","**Breakthrough in Optimization Algorithm**

Imagine you're trying to optimize a complex system with two interconnected parts. The top part depends on the bottom part, which has its own constraints. This is known as a bilevel optimization problem. Researchers have made a significant advancement in solving such problems, particularly when the bottom part has strict rules and is well-behaved.

The team developed a new algorithm, called SFLCB, which simplifies the problem by transforming it into a single, manageable part. This approach uses clever mathematical techniques to ensure that the solution to the simplified problem is close to the original one.

**Key Benefits:**

* **Faster computation**: The SFLCB algorithm is faster than previous methods, requiring less computational power.
* **Improved accuracy**: The algorithm provides a more accurate solution, with a better convergence rate.
* **Practical applications**: The researchers demonstrated the effectiveness of SFLCB through experiments, showcasing its potential for real-world applications.

**Impact:**

The SFLCB algorithm has the potential to accelerate progress in various fields, such as:

* Machine learning
* Operations research
* Economics

By providing a more efficient and accurate way to solve complex optimization problems, this breakthrough can lead to better decision-making and optimization in a wide range of industries. The simulation code for the SFLCB algorithm is publicly available, allowing others to build upon and apply this research."
cs.LG,Does Object Binding Naturally Emerge in Large Pretrained Vision   Transformers?,"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.",http://arxiv.org/abs/2510.24709v1,2025-10-28T17:57:05Z,"Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording","**Unlocking Object Binding in AI: A Breakthrough in Vision Transformers**

Imagine you're looking at a picture of a cat wearing a hat. Your brain automatically groups the features of the cat (its fur, eyes, whiskers) and the hat (its shape, color, texture) into two separate objects. This ability, called object binding, is crucial for human cognition and perception.

Researchers investigated whether a type of artificial intelligence (AI) model, called Vision Transformers (ViTs), can also perform object binding. ViTs are trained on vast amounts of image data and can learn to recognize objects, but it's unclear if they can naturally group features into coherent objects.

The study found that ViTs, when trained using certain methods (self-supervised learning), can indeed perform object binding. This means they can identify which parts of an image belong to the same object, without being explicitly told to do so. The researchers decoded this information from the model's internal representations and found that it was surprisingly accurate.

Interestingly, this ability was not present in ViTs trained on labeled data (e.g., ImageNet), suggesting that object binding is acquired through specific training objectives. The study also found that object binding guides the model's attention and is essential for its performance.

These findings challenge the view that ViTs lack object binding and demonstrate that this symbolic knowledge can emerge naturally in a connectionist system (a type of AI model). This research has implications for the development of more sophisticated AI models that can better understand and interact with the world."
cs.LG,Tongyi DeepResearch Technical Report,"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",http://arxiv.org/abs/2510.24701v1,2025-10-28T17:53:02Z,"Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang","Here's a summary of the research paper for a general audience:

**Introducing Tongyi DeepResearch: A Powerful AI Model for In-Depth Research**

Imagine having a super-smart research assistant that can dig deep into complex topics, find relevant information, and provide insightful answers. That's what Tongyi DeepResearch is - a cutting-edge AI model designed to perform in-depth research tasks.

**What makes it special?**

Tongyi DeepResearch is a large language model that can reason and seek information across complex tasks. It's been trained using a unique framework that allows it to learn autonomously, without relying on human annotation. This makes it scalable and efficient.

**Key achievements:**

* Tongyi DeepResearch has 30.5 billion parameters, but only uses 3.3 billion per task, making it efficient.
* It has achieved top-notch performance on various research benchmarks, outperforming other models.
* The model, framework, and solutions are being open-sourced, making it accessible to the wider research community.

**What does this mean?**

Tongyi DeepResearch has the potential to revolutionize the way we conduct research, making it faster, more efficient, and more accurate. Its open-source nature will enable researchers and developers to build upon this technology, leading to new breakthroughs and innovations."
cs.LG,Greedy Sampling Is Provably Efficient for RLHF,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.",http://arxiv.org/abs/2510.24700v1,2025-10-28T17:52:08Z,"Di Wu, Chengshuai Shi, Jing Yang, Cong Shen","Here's a summary of the research paper for a general audience:

**Improving AI Training with a Simple yet Powerful Technique**

Researchers have made a breakthrough in training large language models, which are a type of artificial intelligence (AI) used in applications like chatbots and language translation. The technique, called Reinforcement Learning from Human Feedback (RLHF), helps fine-tune these models by incorporating feedback from human users.

The challenge with RLHF is that it relies on people's preferences, rather than explicit rewards or penalties. Previous methods for optimizing RLHF have been complex and not very efficient. However, this new research shows that a surprisingly simple approach, called ""greedy sampling,"" can be highly effective.

Greedy sampling involves choosing actions based on the most immediate and obvious benefits, rather than trying to plan ahead or make optimistic/pessimistic predictions. The researchers found that this approach can lead to significant improvements in the performance of RLHF, outperforming previous methods.

The implications of this research are exciting, as it could lead to more efficient and effective training of large language models. This, in turn, could enable more accurate and helpful AI systems that can be used in a wide range of applications."
cs.LG,AgentFold: Long-Horizon Web Agents with Proactive Context Management,"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.",http://arxiv.org/abs/2510.24699v1,2025-10-28T17:51:50Z,"Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang","Here's a summary of the research paper ""AgentFold: Long-Horizon Web Agents with Proactive Context Management"" for a general audience:

**Improving Web Agents' Ability to Complete Complex Tasks**

Imagine you're trying to book a flight and hotel for a trip, but you need to gather information from multiple websites. A web agent, like a computer program, can help you with this task. However, these agents often struggle with complex tasks that require many steps.

The problem lies in how they manage their ""memory"" or context. Current agents either store too much information, making it hard to focus on important details, or they summarize their progress too much, losing crucial information.

To address this issue, researchers introduced AgentFold, a new type of web agent that actively manages its context, similar to how humans process information. AgentFold can condense or abstract its past actions at different levels of detail, allowing it to preserve important information while still making progress.

**Promising Results**

The researchers tested AgentFold on two challenging tasks and achieved impressive results. With a relatively small amount of fine-tuning, AgentFold outperformed or matched much larger and more complex models, including proprietary ones from leading companies like OpenAI.

In simple terms, AgentFold is a more efficient and effective way for web agents to complete complex tasks, and it has the potential to improve how we interact with websites and gather information online."
cs.LG,Learning to Drive Safely with Hybrid Options,"Out of the many deep reinforcement learning approaches for autonomous driving, only few make use of the options (or skills) framework. That is surprising, as this framework is naturally suited for hierarchical control applications in general, and autonomous driving tasks in specific. Therefore, in this work the options framework is applied and tailored to autonomous driving tasks on highways. More specifically, we define dedicated options for longitudinal and lateral manoeuvres with embedded safety and comfort constraints. This way, prior domain knowledge can be incorporated into the learning process and the learned driving behaviour can be constrained more easily. We propose several setups for hierarchical control with options and derive practical algorithms following state-of-the-art reinforcement learning techniques. By separately selecting actions for longitudinal and lateral control, the introduced policies over combined and hybrid options obtain the same expressiveness and flexibility that human drivers have, while being easier to interpret than classical policies over continuous actions. Of all the investigated approaches, these flexible policies over hybrid options perform the best under varying traffic conditions, outperforming the baseline policies over actions.",http://arxiv.org/abs/2510.24674v1,2025-10-28T17:40:04Z,"Bram De Cooman, Johan Suykens","**Learning to Drive Safely with Hybrid Options**

Imagine a self-driving car navigating through busy highways. To achieve this, researchers are exploring ways to teach autonomous vehicles to make safe and comfortable decisions. A recent study focused on using a technique called ""options framework"" to improve the learning process.

The researchers found that by breaking down driving tasks into smaller, manageable parts (called ""options"") such as accelerating, braking, or turning, the learning process becomes more efficient and safer. They defined specific options for controlling the vehicle's speed (longitudinal) and steering (lateral) while incorporating safety and comfort constraints.

The study proposed new algorithms that combine these options to create flexible and interpretable driving policies. The results showed that these hybrid options outperformed traditional methods, especially in varying traffic conditions. This approach enables self-driving cars to make decisions similar to human drivers, with the added benefits of improved safety and comfort.

**In simple terms:** This research aims to make self-driving cars safer and more efficient by teaching them to make decisions in a more human-like way, using a combination of smaller, manageable tasks. The findings have the potential to improve the performance of autonomous vehicles on highways."
cs.LG,Eigenfunction Extraction for Ordered Representation Learning,"Recent advances in representation learning reveal that widely used objectives, such as contrastive and non-contrastive, implicitly perform spectral decomposition of a contextual kernel, induced by the relationship between inputs and their contexts. Yet, these methods recover only the linear span of top eigenfunctions of the kernel, whereas exact spectral decomposition is essential for understanding feature ordering and importance. In this work, we propose a general framework to extract ordered and identifiable eigenfunctions, based on modular building blocks designed to satisfy key desiderata, including compatibility with the contextual kernel and scalability to modern settings. We then show how two main methodological paradigms, low-rank approximation and Rayleigh quotient optimization, align with this framework for eigenfunction extraction. Finally, we validate our approach on synthetic kernels and demonstrate on real-world image datasets that the recovered eigenvalues act as effective importance scores for feature selection, enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional representations.",http://arxiv.org/abs/2510.24672v1,2025-10-28T17:37:12Z,"Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar","Here's a summary of the research paper ""Eigenfunction Extraction for Ordered Representation Learning"" for a general audience:

**Unlocking the Secrets of Machine Learning Models**

Imagine you're trying to understand how a machine learning model works. You feed it a bunch of images, and it learns to recognize patterns and features that are important for identifying objects. But have you ever wondered how the model decides which features are more important than others?

Researchers have made a breakthrough in understanding how machine learning models work by discovering that they can be broken down into simpler components, called ""eigenfunctions."" These eigenfunctions are like the building blocks of the model, and they help the model identify the most important features.

The problem is that current methods can only recover some of these eigenfunctions, and not in a way that shows their relative importance. This new research proposes a framework for extracting these eigenfunctions in a way that shows their order and importance. This allows researchers to understand which features are most important for the model, and to make more efficient and accurate predictions.

The researchers tested their approach on synthetic and real-world image datasets, and found that it works well. They were able to use the extracted eigenfunctions to identify the most important features, and to make more efficient predictions by selecting only the most important ones. This has big implications for machine learning, as it could lead to more efficient and accurate models that are better at understanding complex data."
cs.LG,Pearl: A Foundation Model for Placing Every Atom in the Right Location,"Accurately predicting the three-dimensional structures of protein-ligand complexes remains a fundamental challenge in computational drug discovery that limits the pace and success of therapeutic design. Deep learning methods have recently shown strong potential as structural prediction tools, achieving promising accuracy across diverse biomolecular systems. However, their performance and utility are constrained by scarce experimental data, inefficient architectures, physically invalid poses, and the limited ability to exploit auxiliary information available at inference. To address these issues, we introduce Pearl (Placing Every Atom in the Right Location), a foundation model for protein-ligand cofolding at scale. Pearl addresses these challenges with three key innovations: (1) training recipes that include large-scale synthetic data to overcome data scarcity; (2) architectures that incorporate an SO(3)-equivariant diffusion module to inherently respect 3D rotational symmetries, improving generalization and sample efficiency, and (3) controllable inference, including a generalized multi-chain templating system supporting both protein and non-polymeric components as well as dual unconditional/conditional modes. Pearl establishes a new state-of-the-art performance in protein-ligand cofolding. On the key metric of generating accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold 3 and other open source baselines on the public Runs N' Poses and PoseBusters benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the next best model. In the pocket-conditional cofolding regime, Pearl delivers $3.6\times$ improvement on a proprietary set of challenging, real-world drug targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate that model performance correlates directly with synthetic dataset size used in training.",http://arxiv.org/abs/2510.24670v2,2025-10-28T17:36:51Z,"Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan","Here's a summary of the research paper for a general audience:

**Breakthrough in Predicting 3D Structures of Proteins and Ligands**

Scientists have made a significant advancement in predicting the 3D structures of proteins and ligands, which is crucial for designing new medicines. The researchers introduced a new AI model called Pearl, which can accurately predict how proteins and ligands fit together. This is a challenging task because it requires understanding the precise arrangement of atoms in 3D space.

**The Problem with Current Methods**

Current methods for predicting protein-ligand structures have limitations. They often rely on scarce experimental data, which can lead to inaccurate predictions. Additionally, these methods can produce physically invalid poses, which are not feasible in reality.

**Pearl: A New Foundation Model**

Pearl addresses these challenges with three key innovations:

1. **Large-scale synthetic data**: Pearl uses a massive dataset of simulated structures to learn from, which helps overcome the scarcity of experimental data.
2. **Improved architecture**: Pearl's architecture is designed to respect 3D rotational symmetries, which improves its ability to generalize and make accurate predictions.
3. **Controllable inference**: Pearl allows for more control over the prediction process, enabling researchers to generate multiple possible structures and select the most promising ones.

**Results and Impact**

Pearl has achieved state-of-the-art performance in predicting protein-ligand structures, outperforming other leading models. On benchmark tests, Pearl delivered 14.5% and 14.2% improvements over the next best model. In a more challenging test with real-world drug targets, Pearl showed a 3.6-fold improvement. These results have significant implications for the design of new medicines, as accurate predictions of protein-ligand structures can accelerate the discovery of new treatments."
cs.LG,The Cost of Robustness: Tighter Bounds on Parameter Complexity for   Robust Memorization in ReLU Nets,"We study the parameter complexity of robust memorization for $\mathrm{ReLU}$ networks: the number of parameters required to interpolate any given dataset with $\epsilon$-separation between differently labeled points, while ensuring predictions remain consistent within a $\mu$-ball around each training sample. We establish upper and lower bounds on the parameter count as a function of the robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain tighter upper and lower bounds that improve upon existing results. Our findings reveal that the parameter complexity of robust memorization matches that of non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.",http://arxiv.org/abs/2510.24643v1,2025-10-28T17:09:43Z,"Yujun Kim, Chaewon Moon, Chulhee Yun","**Understanding the Cost of Robustness in Artificial Intelligence**

Imagine you're teaching a child to recognize different objects, like cats and dogs. You show them pictures and tell them which is which. But what if the child misinterprets a picture that's slightly blurry or rotated? That's a problem in artificial intelligence (AI), where machines learn to recognize patterns, just like humans do.

Researchers have been working on making AI systems more robust, meaning they can correctly identify objects even if they're distorted or unclear. This is called ""robust memorization."" However, there's a trade-off: making AI systems more robust requires more information, which can be costly.

A recent study looked at how many ""parameters"" (think of them as pieces of information) are needed to make a type of AI system called a ReLU network robust. The researchers found that the number of parameters required depends on how much robustness is needed. If you only need a little robustness, it's not too costly. But if you need a lot of robustness, it becomes much more expensive.

The study provides new insights into the relationship between robustness and the number of parameters required. The findings have implications for designing more efficient and effective AI systems that can learn and make accurate predictions, even in uncertain environments.

**In simple terms:** Robust AI systems that can handle unclear or distorted inputs require more information, which can be costly. Researchers have found that the cost of robustness depends on how much robustness is needed, and have provided new guidelines for designing more efficient AI systems."
cs.LG,Causal Ordering for Structure Learning From Time Series,"Predicting causal structure from time series data is crucial for understanding complex phenomena in physiology, brain connectivity, climate dynamics, and socio-economic behaviour. Causal discovery in time series is hindered by the combinatorial complexity of identifying true causal relationships, especially as the number of variables and time points grow. A common approach to simplify the task is the so-called ordering-based methods. Traditional ordering methods inherently limit the representational capacity of the resulting model. In this work, we fix this issue by leveraging multiple valid causal orderings, instead of a single one as standard practice. We propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based causal discovery for temporal data. By integrating multiple orderings, DOTS effectively recovers the transitive closure of the underlying directed acyclic graph, mitigating spurious artifacts inherent in single-ordering approaches. We formalise the problem under standard assumptions such as stationarity and the additive noise model, and leverage score matching with diffusion processes to enable efficient Hessian estimation. Extensive experiments validate the approach. Empirical evaluations on synthetic and real-world datasets demonstrate that DOTS outperforms state-of-the-art baselines, offering a scalable and robust approach to temporal causal discovery. On synthetic benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the best on individual datasets, DOTS attains the highest average summary-graph $F1$ while halving runtime relative to graph-optimisation methods. These results establish DOTS as a scalable and accurate solution for temporal causal discovery.",http://arxiv.org/abs/2510.24639v1,2025-10-28T17:06:15Z,"Pedro P. Sanchez, Damian Machlanski, Steven McDonagh, Sotirios A. Tsaftaris","**Unlocking Hidden Relationships in Time Series Data**

Understanding how different factors influence each other over time is crucial in various fields, such as physiology, climate science, and economics. Researchers have developed methods to discover causal relationships from time series data, but the complexity of the data can make it challenging to identify true relationships.

A new approach, called DOTS (Diffusion Ordered Temporal Structure), addresses this challenge by leveraging multiple valid causal orderings. Unlike traditional methods that rely on a single ordering, DOTS integrates multiple orderings to effectively recover the underlying causal relationships. This approach has been shown to outperform state-of-the-art methods in extensive experiments on synthetic and real-world datasets.

The results demonstrate that DOTS can accurately identify causal relationships in time series data, even with a large number of variables and time points. Specifically, DOTS improves the mean window-graph F1 score from 0.63 to 0.81 on synthetic benchmarks and achieves the highest average summary-graph F1 score on a real-world benchmark. Additionally, DOTS reduces runtime by half compared to graph-optimization methods.

Overall, DOTS offers a scalable and robust solution for temporal causal discovery, enabling researchers to gain deeper insights into complex phenomena and make more informed decisions."
cs.LG,Symbolic Snapshot Ensembles,"Inductive logic programming (ILP) is a form of logical machine learning. Most ILP algorithms learn a single hypothesis from a single training run. Ensemble methods train an ILP algorithm multiple times to learn multiple hypotheses. In this paper, we train an ILP algorithm only once and save intermediate hypotheses. We then combine the hypotheses using a minimum description length weighting scheme. Our experiments on multiple benchmarks, including game playing and visual reasoning, show that our approach improves predictive accuracy by 4% with less than 1% computational overhead.",http://arxiv.org/abs/2510.24633v1,2025-10-28T17:01:38Z,"Mingyue Liu, Andrew Cropper","**Improving Machine Learning with Symbolic Snapshot Ensembles**

Imagine you're trying to solve a puzzle, but instead of giving up and starting over when you get stuck, you take a snapshot of your progress and try again from a different angle. Researchers have applied a similar concept to machine learning, specifically a type called inductive logic programming (ILP).

In traditional ILP, a computer learns from data and creates a single solution. However, this approach can be limited. To overcome this, researchers have developed a new method called Symbolic Snapshot Ensembles. Instead of learning a single solution, the computer takes ""snapshots"" of its progress at various points during the learning process. These snapshots are then combined using a clever weighting scheme to create a more accurate and reliable solution.

The exciting part? This approach improves the computer's predictive accuracy by 4% while requiring less than 1% more computational power. This means that Symbolic Snapshot Ensembles can help machines learn and make predictions more effectively, with applications in areas like game playing, visual reasoning, and more."
cs.LG,Coreset for Robust Geometric Median: Eliminating Size Dependency on   Outliers,"We study the robust geometric median problem in Euclidean space $\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact summary of a dataset $P$ of size $n$ that approximates the robust cost for all centers $c$ within a multiplicative error $\varepsilon$. Given an outlier count $m$, we construct a coreset of size $\tilde{O}(\varepsilon^{-2} \cdot \min\{\varepsilon^{-2}, d\})$ when $n \geq 4m$, eliminating the $O(m)$ dependency present in prior work [Huang et al., 2022 & 2023]. For the special case of $d = 1$, we achieve an optimal coreset size of $\tilde{\Theta}(\varepsilon^{-1/2} + \frac{m}{n} \varepsilon^{-1})$, revealing a clear separation from the vanilla case studied in [Huang et al., 2023; Afshani and Chris, 2024]. Our results further extend to robust $(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence under mild data assumptions. The key technical contribution is a novel non-component-wise error analysis, enabling substantial reduction of outlier influence, unlike prior methods that retain them.Empirically, our algorithms consistently outperform existing baselines in terms of size-accuracy tradeoffs and runtime, even when data assumptions are violated across a wide range of datasets.",http://arxiv.org/abs/2510.24621v1,2025-10-28T16:49:03Z,"Ziyi Fang, Lingxiao Huang, Runkai Yang","**Breakthrough in Robust Data Analysis: Coreset Construction for Geometric Median**

Imagine trying to find the middle point of a set of data points, but some of those points are outliers that can skew the result. Researchers have made a significant advancement in solving this problem, known as the robust geometric median problem. They have developed a method to create a compact summary of the data, called a coreset, which can accurately approximate the middle point even when there are outliers.

The key achievement is that the size of the coreset no longer depends on the number of outliers, which was a major limitation of previous methods. This breakthrough enables faster and more accurate analysis of large datasets. The researchers also showed that their method can be extended to more complex data analysis problems, such as clustering.

The new method has been tested on various datasets and has consistently outperformed existing approaches in terms of accuracy, size, and computation time. This advancement has the potential to improve data analysis in many fields, including machine learning, statistics, and data science.

**In simple terms:** The researchers have developed a more efficient and accurate method for finding the middle point of a dataset, even when there are outliers. This method can handle large datasets and has many potential applications in data analysis."
cs.LG,Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation,"With the release of new large language models (LLMs) like Llama and Mistral, zero-shot cross-lingual transfer has become increasingly feasible due to their multilingual pretraining and strong generalization capabilities. However, adapting these decoder-only LLMs to new tasks across languages remains challenging. While parameter-efficient fine-tuning (PeFT) techniques like Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as soft prompt tuning, prefix tuning, and Llama Adapter are less explored, especially for zero-shot transfer in decoder-only models. We present a comprehensive study of three prefix-based methods for zero-shot cross-lingual transfer from English to 35+ high- and low-resource languages. Our analysis further explores transfer across linguistic families and scripts, as well as the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix methods outperform LoRA-baselines by up to 6% on the Belebele benchmark. Similar improvements were observed with Mistral v0.3 7B as well. Despite using only 1.23M learning parameters with prefix tuning, we achieve consistent improvements across diverse benchmarks. These findings highlight the potential of prefix-based techniques as an effective and scalable alternative to LoRA, particularly in low-resource multilingual settings.",http://arxiv.org/abs/2510.24619v1,2025-10-28T16:48:03Z,"Snegha A, Sayambhu Sen, Piyush Singh Pasi, Abhishek Singhania, Preethi Jyothi","**Breaking Language Barriers: New Method for Adapting Large Language Models**

Large language models, like Llama and Mistral, have shown impressive abilities to understand and generate human-like text in multiple languages. However, adapting these models to specific tasks in different languages can be challenging. Researchers have been exploring various techniques to fine-tune these models efficiently.

In a recent study, scientists investigated ""prefix-based adaptation"" methods as an alternative to traditional fine-tuning techniques. They tested three prefix-based methods on a large language model, Llama, and another model, Mistral, across over 35 languages. The results showed that prefix-based methods outperformed traditional fine-tuning techniques by up to 6% on a benchmark test.

The exciting part is that prefix-based methods use significantly fewer learning parameters (only 1.23M) compared to traditional methods, making them more efficient and scalable. This is particularly important for low-resource languages, where adapting large language models can be especially challenging.

The study's findings suggest that prefix-based techniques have the potential to become a go-to approach for adapting large language models to new tasks and languages, enabling more effective and efficient language processing across the globe."
cs.LG,Statistical physics of deep learning: Optimal learning of a multi-layer   perceptron near interpolation,"For three decades statistical physics has been providing a framework to analyse neural networks. A long-standing question remained on its capacity to tackle deep learning models capturing rich feature learning effects, thus going beyond the narrow networks or kernel methods analysed until now. We positively answer through the study of the supervised learning of a multi-layer perceptron. Importantly, (i) its width scales as the input dimension, making it more prone to feature learning than ultra wide networks, and more expressive than narrow ones or with fixed embedding layers; and (ii) we focus on the challenging interpolation regime where the number of trainable parameters and data are comparable, which forces the model to adapt to the task. We consider the matched teacher-student setting. It provides the fundamental limits of learning random deep neural network targets and helps in identifying the sufficient statistics describing what is learnt by an optimally trained network as the data budget increases. A rich phenomenology emerges with various learning transitions. With enough data optimal performance is attained through model's ""specialisation"" towards the target, but it can be hard to reach for training algorithms which get attracted by sub-optimal solutions predicted by the theory. Specialisation occurs inhomogeneously across layers, propagating from shallow towards deep ones, but also across neurons in each layer. Furthermore, deeper targets are harder to learn. Despite its simplicity, the Bayesian-optimal setting provides insights on how the depth, non-linearity and finite (proportional) width influence neural networks in the feature learning regime that are potentially relevant way beyond it.",http://arxiv.org/abs/2510.24616v2,2025-10-28T16:44:34Z,"Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk","Here's a summary of the research paper for a general audience:

**Unlocking the Secrets of Deep Learning**

For decades, scientists have been using statistical physics to understand how neural networks learn. However, most previous studies focused on simple networks or limited types of learning. This new research takes a significant step forward by applying statistical physics to deep learning models, which are capable of learning complex patterns and features.

The study focuses on a type of neural network called a multi-layer perceptron, which is similar to those used in many AI applications. The researchers explored how well this network can learn from data when the number of parameters (or ""knobs"" to adjust) is comparable to the amount of data. This is a challenging scenario, as the network must adapt to the task at hand.

The study reveals some fascinating insights:

* **Learning transitions**: As the network receives more data, it undergoes different phases of learning, eventually becoming specialized to the task. However, this specialization can be hard to achieve with common training algorithms.
* **Layer-by-layer learning**: The network learns inhomogeneously, with shallower layers adapting to the task before deeper ones. Additionally, different neurons within each layer learn at different rates.
* **Depth and complexity**: The study shows that deeper networks are harder to learn, and that the interactions between depth, non-linearity, and network width play a crucial role in feature learning.

These findings provide new insights into how deep learning models work and can help improve their performance. The researchers used a simplified, idealized setting, but their results have implications for more complex and realistic scenarios, potentially leading to better AI systems."
cs.LG,Semi-supervised and unsupervised learning for health indicator   extraction from guided waves in aerospace composite structures,"Health indicators (HIs) are central to diagnosing and prognosing the condition of aerospace composite structures, enabling efficient maintenance and operational safety. However, extracting reliable HIs remains challenging due to variability in material properties, stochastic damage evolution, and diverse damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents (e.g., bird strikes) further complicate this process. This study presents a comprehensive data-driven framework that learns HIs via two learning approaches integrated with multi-domain signal processing. Because ground-truth HIs are unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach augmented with continuous auxiliary labels used as hypothetical damage proxies, which overcomes the limitation of prior binary labels that only distinguish healthy and failed states while neglecting intermediate degradation, and (ii) a degradation-trend-constrained variational autoencoder (DTC-VAE), in which the monotonicity criterion is embedded via an explicit trend constraint. Guided waves with multiple excitation frequencies are used to monitor single-stiffener composite structures under fatigue loading. Time, frequency, and time-frequency representations are explored, and per-frequency HIs are fused via unsupervised ensemble learning to mitigate frequency dependence and reduce variance. Using fast Fourier transform features, the augmented Diversity-DeepSAD model achieved 81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3% performance, outperforming existing baselines.",http://arxiv.org/abs/2510.24614v1,2025-10-28T16:44:11Z,"James Josep Perry, Pablo Garcia-Conde Ortiz, George Konstantinou, Cornelie Vergouwen, Edlyn Santha Kumaran, Morteza Moradi","**Advancing Health Monitoring in Aerospace Composite Structures**

Researchers have developed a new data-driven framework to extract health indicators (HIs) from guided waves in aerospace composite structures. This framework uses machine learning techniques to analyze data from sensors that monitor the structures for damage. The goal is to improve the safety and efficiency of maintenance for these structures.

The challenge lies in detecting and predicting damage in composite materials, which can be affected by various factors such as material properties, damage evolution, and environmental incidents. The researchers proposed two approaches: a semi-supervised approach called Diversity-DeepSAD and an unsupervised approach called DTC-VAE. These methods use signal processing and machine learning to analyze data from guided waves and extract reliable HIs.

In tests using guided waves with multiple excitation frequencies, the framework achieved high performance in extracting HIs, with the DTC-VAE approach delivering the most consistent results. This study demonstrates the potential of machine learning and data-driven approaches to improve health monitoring in aerospace composite structures, enabling more efficient maintenance and operational safety.

**Key Takeaways:**

* A new framework for extracting health indicators from guided waves in aerospace composite structures
* Two machine learning approaches proposed: semi-supervised Diversity-DeepSAD and unsupervised DTC-VAE
* High performance achieved in extracting reliable health indicators, with DTC-VAE delivering the most consistent results
* Potential to improve safety and efficiency of maintenance in aerospace composite structures"
cs.LG,Comparison of generalised additive models and neural networks in   applications: A systematic review,"Neural networks have become a popular tool in predictive modelling, more commonly associated with machine learning and artificial intelligence than with statistics. Generalised Additive Models (GAMs) are flexible non-linear statistical models that retain interpretability. Both are state-of-the-art in their own right, with their respective advantages and disadvantages. This paper analyses how these two model classes have performed on real-world tabular data. Following PRISMA guidelines, we conducted a systematic review of papers that performed empirical comparisons of GAMs and neural networks. Eligible papers were identified, yielding 143 papers, with 430 datasets. Key attributes at both paper and dataset levels were extracted and reported. Beyond summarising comparisons, we analyse reported performance metrics using mixed-effects modelling to investigate potential characteristics that can explain and quantify observed differences, including application area, study year, sample size, number of predictors, and neural network complexity. Across datasets, no consistent evidence of superiority was found for either GAMs or neural networks when considering the most frequently reported metrics (RMSE, $R^2$, and AUC). Neural networks tended to outperform in larger datasets and in those with more predictors, but this advantage narrowed over time. Conversely, GAMs remained competitive, particularly in smaller data settings, while retaining interpretability. Reporting of dataset characteristics and neural network complexity was incomplete in much of the literature, limiting transparency and reproducibility. This review highlights that GAMs and neural networks should be viewed as complementary approaches rather than competitors. For many tabular applications, the performance trade-off is modest, and interpretability may favour GAMs.",http://arxiv.org/abs/2510.24601v1,2025-10-28T16:28:42Z,"Jessica Doohan, Lucas Kook, Kevin Burke","**The Great Debate: Neural Networks vs. Generalised Additive Models**

In the world of data analysis, two powerful tools have emerged: Neural Networks and Generalised Additive Models (GAMs). Neural Networks are a type of machine learning model inspired by the human brain, while GAMs are a type of statistical model that can handle complex relationships between variables. Both have their strengths and weaknesses, but which one is better?

A recent systematic review of 143 research papers and 430 datasets compared the performance of Neural Networks and GAMs on real-world data. The surprising result? There is no clear winner. When looking at common metrics such as accuracy, precision, and recall, both models performed similarly well.

However, the review did find some interesting patterns. Neural Networks tended to do better with larger datasets and more complex problems, but their advantage decreased over time. On the other hand, GAMs remained competitive, especially with smaller datasets, and had the added benefit of being more interpretable, meaning it's easier to understand how they arrived at their predictions.

The review also highlighted a problem with the way research is reported: many studies didn't provide enough information about the data and models used, making it hard to reproduce and compare results.

In conclusion, the review suggests that Neural Networks and GAMs should be seen as complementary tools, rather than competitors. Depending on the specific problem and data, one may be more suitable than the other. And often, the difference in performance is modest, making interpretability a key consideration. GAMs may be a better choice when understanding the relationships between variables is crucial, while Neural Networks may be more suitable for complex, large-scale problems. Ultimately, the choice between these two models depends on the specific needs of the project."
cs.LG,A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity   Dispersion Modeling in MaNGA Galaxies,"Current quantum machine learning approaches often face challenges balancing predictive accuracy, robustness, and interpretability. To address this, we propose a novel quantum adversarial framework that integrates a hybrid quantum neural network (QNN) with classical deep learning layers, guided by an evaluator model with LIME-based interpretability, and extended through quantum GAN and self-supervised variants. In the proposed model, an adversarial evaluator concurrently guides the QNN by computing feedback loss, thereby optimizing both prediction accuracy and model explainability. Empirical evaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE = 0.21, and R^2 = 0.59, delivering the most consistent performance across regression metrics compared to adversarial counterparts. These results demonstrate the potential of combining quantum-inspired methods with classical architectures to develop lightweight, high-performance, and interpretable predictive models, advancing the applicability of QML beyond current limitations.",http://arxiv.org/abs/2510.24598v1,2025-10-28T16:27:10Z,"Sathwik Narkedimilli, N V Saran Kumar, Aswath Babu H, Manjunath K Vanahalli, Manish M, Vinija Jain, Aman Chadha","**Unlocking the Secrets of Galaxy Evolution with AI and Quantum Computing**

Imagine being able to better understand how galaxies, like our own Milky Way, change and evolve over time. A team of researchers has made a significant step towards achieving this goal by developing a new type of artificial intelligence (AI) model that combines the power of quantum computing and machine learning.

The researchers focused on a specific challenge: modeling the velocity dispersion of galaxies, which refers to how the stars and gas within a galaxy are moving. This information can help scientists understand the galaxy's structure, composition, and history.

The new AI model, called a quantum adversarial network, uses a combination of quantum-inspired and classical machine learning techniques to make predictions about galaxy evolution. What's innovative about this model is that it not only provides accurate predictions but also explains how it arrived at those predictions, which is crucial for scientists who want to trust and understand the results.

The results show that this new model outperforms other approaches, achieving high accuracy and reliability in predicting velocity dispersion. This breakthrough demonstrates the potential of combining quantum computing and machine learning to develop more powerful, efficient, and interpretable models that can help us better understand complex phenomena, like galaxy evolution.

**In simple terms:** This research uses AI and quantum computing to improve our understanding of galaxy evolution. The new model provides accurate predictions and explanations, which can help scientists make new discoveries and gain insights into the workings of the universe."
cs.LG,Physics-Informed Extreme Learning Machine (PIELM): Opportunities and   Challenges,"We are very delighted to see the fast development of physics-informed extreme learning machine (PIELM) in recent years for higher computation efficiency and accuracy in physics-informed machine learning. As a summary or review on PIELM is currently not available, we would like to take this opportunity to show our perspective and experience for this promising research direction. We can see many efforts are made to solve PDEs with sharp gradients, nonlinearities, high-frequency behavior, hard constraints, uncertainty, multiphysics coupling. Despite the success, many urgent challenges remain to be tackled, which also provides us opportunities to develop more robust, interpretable, and generalizable PIELM frameworks with applications in science and engineering.",http://arxiv.org/abs/2510.24577v1,2025-10-28T16:11:16Z,"He Yang, Fei Ren, Hai-Sui Yu, Xiaohui Chen, Pei-Zhi Zhuang","Here's a summary of the research paper for a general audience:

**Introducing Physics-Informed Extreme Learning Machine (PIELM)**

Imagine a computer program that can learn from data and also follow the laws of physics. This is the idea behind Physics-Informed Extreme Learning Machine (PIELM), a new approach that's gaining attention in the field of machine learning.

**What is PIELM?**

PIELM combines machine learning with physics to solve complex problems in science and engineering. It's designed to be fast and accurate, making it a promising tool for tasks like simulating weather patterns, modeling ocean currents, or optimizing complex systems.

**Challenges and Opportunities**

While PIELM has shown great potential, there are still many challenges to overcome. For example, it's difficult to model systems with sudden changes, nonlinear behavior, or multiple interacting factors. However, these challenges also present opportunities for researchers to develop more robust and generalizable PIELM frameworks.

**The Future of PIELM**

As PIELM continues to evolve, it could have a significant impact on various fields, from climate modeling to materials science. By combining the power of machine learning with the principles of physics, PIELM has the potential to lead to breakthroughs in our understanding of complex systems and our ability to simulate and predict their behavior."
cs.LG,DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein   Alignment,"Training time-series forecast models requires aligning the conditional distribution of model forecasts with that of the label sequence. The standard direct forecast (DF) approach resorts to minimize the conditional negative log-likelihood of the label sequence, typically estimated using the mean squared error. However, this estimation proves to be biased in the presence of label autocorrelation. In this paper, we propose DistDF, which achieves alignment by alternatively minimizing a discrepancy between the conditional forecast and label distributions. Because conditional discrepancies are difficult to estimate from finite time-series observations, we introduce a newly proposed joint-distribution Wasserstein discrepancy for time-series forecasting, which provably upper bounds the conditional discrepancy of interest. This discrepancy admits tractable, differentiable estimation from empirical samples and integrates seamlessly with gradient-based training. Extensive experiments show that DistDF improves the performance diverse forecast models and achieves the state-of-the-art forecasting performance. Code is available at https://anonymous.4open.science/r/DistDF-F66B.",http://arxiv.org/abs/2510.24574v1,2025-10-28T16:09:59Z,"Hao Wang, Licheng Pan, Yuan Lu, Zhixuan Chu, Xiaoxi Li, Shuting He, Zhichao Chen, Haoxuan Li, Qingsong Wen, Zhouchen Lin","Here's a summary of the research paper for a general audience:

**Improving Time-Series Forecasting with a New Approach**

Time-series forecasting is a crucial task in many fields, such as predicting stock prices, weather, or energy demand. Current forecasting models are trained to minimize the difference between their predictions and the actual values. However, this approach can be flawed, especially when the data is highly correlated over time.

Researchers have proposed a new method called DistDF, which aims to improve forecasting accuracy by aligning the distribution of predicted values with the distribution of actual values. This is achieved by minimizing a specific type of discrepancy between the two distributions, called the joint-distribution Wasserstein discrepancy.

The key innovation of DistDF is that it takes into account the complex relationships between consecutive data points in a time series, which can lead to more accurate predictions. The method has been tested on various forecasting models and has shown to improve their performance, achieving state-of-the-art results.

In simple terms, DistDF is a new approach to time-series forecasting that helps models make more accurate predictions by considering the underlying patterns and relationships in the data. This can have significant implications for various applications, from finance to climate science."
cs.CV,Generative View Stitching,"Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\""ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.",http://arxiv.org/abs/2510.24718v1,2025-10-28T17:59:58Z,"Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann","**Advancements in AI-Generated Video: Generative View Stitching**

Imagine being able to generate videos that are not only visually stunning but also accurately predict the movement of a camera through a scene. Researchers have made a significant breakthrough in this area with the development of Generative View Stitching (GVS), a technique that enables the creation of stable, collision-free, and consistent videos.

The challenge with current video generation models is that they can only generate video frames one after another, without being able to look ahead to future frames. This can lead to problems, such as the camera colliding with objects in the scene, causing the generation process to fail.

GVS addresses this limitation by generating the entire video sequence in parallel, ensuring that the scene is consistent with the camera's trajectory. This is achieved through a novel sampling algorithm that can work with existing video models, making it a widely applicable solution.

The researchers also introduced a technique called Omni Guidance, which enhances the temporal consistency of the generated video by considering both past and future frames. This enables the creation of long-range coherent videos that can even handle complex camera movements, such as navigating a seemingly impossible staircase.

The results of this research are impressive, with GVS achieving stable and collision-free video generation for a variety of predefined camera paths. This has significant implications for applications such as computer-generated imagery, robotics, and virtual reality."
cs.CV,Uniform Discrete Diffusion with Metric Path for Video Generation,"Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA",http://arxiv.org/abs/2510.24717v1,2025-10-28T17:59:57Z,"Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang","**Breakthrough in Video Generation: A New Method for Creating High-Quality Videos**

Researchers have made a significant advancement in video generation technology with the development of Uniform discRete diffuSion with metric pAth (URSA). This new framework enables the creation of high-quality videos and images using a discrete approach, which was previously lagging behind continuous approaches.

**What does it do?**

URSA generates videos and images by iteratively refining a set of discrete tokens, which represent the visual information. This process allows for efficient scaling to high-resolution images and long-duration videos, requiring fewer computational steps.

**Key innovations:**

1. **Linearized Metric Path**: A new design that enables efficient refinement of visual tokens.
2. **Resolution-dependent Timestep Shifting**: A mechanism that adapts to different image and video resolutions.
3. **Asynchronous temporal fine-tuning**: A strategy that allows a single model to perform various tasks, such as interpolation and image-to-video generation.

**Results:**

Extensive experiments have shown that URSA outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. This breakthrough has the potential to improve various applications, including video production, animation, and computer-generated imagery.

**What's next?**

The researchers have made their code and models publicly available, which can accelerate further research and development in the field of video generation."
cs.CV,Routing Matters in MoE: Scaling Diffusion Transformers with Explicit   Routing Guidance,"Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.",http://arxiv.org/abs/2510.24711v1,2025-10-28T17:59:02Z,"Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan","**Improving AI Models with Better Routing: A Breakthrough in Image Generation**

Researchers have made a significant advancement in developing more efficient and effective artificial intelligence (AI) models, particularly in the field of image generation. Their new framework, called ProMoE, addresses a key challenge in scaling up AI models while maintaining their performance.

The challenge lies in the fact that images are made up of many similar pixels, making it difficult for AI models to specialize in specific parts of the image. To overcome this, ProMoE uses a two-step ""router"" that guides the model to focus on specific areas of the image and assign them to the right ""expert"" for processing.

The researchers found that by adding explicit guidance to the router, the model can better understand the relationships between different parts of the image and improve its overall performance. They also introduced a new loss function that helps the model learn to make better assignments.

The results are impressive: ProMoE outperformed existing state-of-the-art methods on a benchmark image dataset, demonstrating its potential to improve image generation tasks such as image synthesis and editing. This breakthrough has significant implications for applications like computer vision, robotics, and more.

**Key Takeaways:**

* ProMoE is a new framework that improves the efficiency and effectiveness of AI models for image generation tasks.
* The framework uses a two-step router with explicit guidance to help the model specialize in specific parts of the image.
* ProMoE outperformed existing methods on a benchmark image dataset, demonstrating its potential for real-world applications."
cs.CV,Does Object Binding Naturally Emerge in Large Pretrained Vision   Transformers?,"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.",http://arxiv.org/abs/2510.24709v1,2025-10-28T17:57:05Z,"Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording","**Breakthrough in AI Research: Object Binding Emerges Naturally in Large Vision Transformers**

Imagine you're looking at a picture of a cat wearing a hat. Your brain automatically groups the features of the cat (its fur, eyes, whiskers) and the hat (its shape, color, texture) into two separate objects. This ability, called object binding, is crucial for human cognition and perception.

Researchers have wondered if artificial intelligence (AI) models, specifically large Vision Transformers (ViTs), can also perform object binding without being explicitly programmed to do so. ViTs are a type of AI model that can process and understand visual data, such as images.

The study found that, surprisingly, object binding does emerge naturally in large ViTs that have been pre-trained on vast amounts of data using certain objectives, such as self-supervised learning. This means that the model can identify which parts of an image belong to the same object, without being explicitly told to do so.

The researchers used a technique called similarity probe to decode this object-binding capability from the model's internal representations. They found that the model can accurately identify which patches in an image belong to the same object, with over 90% accuracy.

However, the study also found that this ability is not present in models trained on labeled data, such as ImageNet. This suggests that object binding is not a trivial property of ViTs, but rather an ability acquired through specific pre-training objectives.

The findings have significant implications for our understanding of how AI models learn and represent visual information. They challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.

**In simple terms:** Large AI models can automatically group features in an image into objects, without being explicitly programmed to do so. This ability emerges when the model is trained on vast amounts of data using certain objectives, and it helps the model perform better on visual tasks."
cs.CV,MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with   Relation-Aware Fusion for 3D Object Detection,"Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based bird's-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: https://github.com/HandsomeYun/MIC-BEV.",http://arxiv.org/abs/2510.24688v1,2025-10-28T17:49:42Z,"Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma","**Breakthrough in 3D Object Detection for Intelligent Transportation Systems**

Imagine a future where roads are safer and more efficient, thanks to advanced technology that helps vehicles and infrastructure work together seamlessly. A team of researchers has made a significant step towards achieving this vision with the development of MIC-BEV, a new framework for 3D object detection.

**The Challenge**

Current camera-based systems often struggle to detect objects accurately in complex scenarios, such as multiple cameras, varying road layouts, and harsh weather conditions. This is a major hurdle for intelligent transportation systems, which rely on accurate and reliable data to function effectively.

**The Solution**

MIC-BEV is a Transformer-based framework that uses a bird's-eye-view (BEV) perception approach to detect objects in 3D space. This innovative framework can handle multiple cameras with different settings and is robust to sensor degradation, making it suitable for real-world deployment.

**Key Features**

* Supports a variable number of cameras with different intrinsic and extrinsic parameters
* Robust to sensor degradation and challenging conditions such as extreme weather
* Graph-enhanced fusion module integrates multi-view image features into the BEV space

**Results and Impact**

The researchers tested MIC-BEV on both synthetic and real-world datasets, and the results are impressive. MIC-BEV achieved state-of-the-art performance in 3D object detection and demonstrated strong robustness under challenging conditions. This breakthrough has the potential to enable more efficient and safer intelligent transportation systems.

**What's Next**

The researchers have made their dataset and source code publicly available, which will facilitate further research and development in this area. With MIC-BEV, we can expect to see significant advancements in intelligent transportation systems, leading to improved road safety and efficiency."
cs.CV,SAGE: Structure-Aware Generative Video Transitions between Diverse Clips,"Video transitions aim to synthesize intermediate frames between two clips, but naive approaches such as linear blending introduce artifacts that limit professional use or break temporal coherence. Traditional techniques (cross-fades, morphing, frame interpolation) and recent generative inbetweening methods can produce high-quality plausible intermediates, but they struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, leaving a gap for content-aware and visually coherent transitions. We address this challenge by drawing on artistic workflows, distilling strategies such as aligning silhouettes and interpolating salient features to preserve structure and perceptual continuity. Building on this, we propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot approach that combines structural guidance, provided via line maps and motion flow, with generative synthesis, enabling smooth, semantically consistent transitions without fine-tuning. Extensive experiments and comparison with current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate that SAGE outperforms both classical and generative baselines on quantitative metrics and user studies for producing transitions between diverse clips. Code to be released on acceptance.",http://arxiv.org/abs/2510.24667v1,2025-10-28T17:35:02Z,"Mia Kan, Yilin Liu, Niloy Mitra","**Advancing Video Editing: A New Method for Seamless Transitions**

Imagine you're editing a video and want to smoothly transition between two completely different scenes. Current video transition methods often produce awkward or choppy results, especially when the scenes are very different. Researchers have now developed a new approach called SAGE (Structure-Aware Generative vidEo transitions) that generates seamless and coherent transitions between diverse video clips.

SAGE works by using structural guidance, such as line maps and motion flow, to help create smooth transitions. This approach is inspired by artistic workflows, where editors carefully align and interpolate features to preserve the structure and continuity of the video. Unlike previous methods, SAGE doesn't require fine-tuning and can be used right out of the box.

In tests, SAGE outperformed both traditional and recent video transition methods in terms of producing high-quality, semantically consistent transitions. This breakthrough has the potential to revolutionize video editing, enabling professionals to create more engaging and polished content. The researchers plan to release their code, making it accessible to others in the field."
cs.CV,Group Relative Attention Guidance for Image Editing,"Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.",http://arxiv.org/abs/2510.24657v1,2025-10-28T17:22:44Z,"Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, An-an Liu","Here's a summary of the research paper ""Group Relative Attention Guidance for Image Editing"" for a general audience:

**Improving Image Editing with AI**

Imagine being able to edit images with precision and control, using simple instructions like ""make the sky bluer"" or ""change the flowers to red"". Researchers have made significant progress in developing AI models that can edit images based on text instructions. However, existing methods often struggle to control the degree of editing, making it difficult to achieve the desired results.

**A Breakthrough in Image Editing Control**

A team of researchers has discovered a way to improve image editing by guiding the attention of AI models. They found that by reweighting the ""delta values"" of different tokens, the model can focus on specific parts of the image and adjust the editing intensity. This leads to more precise and controlled editing results.

**The GRAG Method**

The researchers propose a new method called Group Relative Attention Guidance (GRAG). GRAG is a simple yet effective approach that allows for continuous and fine-grained control over editing intensity. The best part? It can be easily integrated into existing image editing frameworks with just a few lines of code.

**Benefits and Applications**

GRAG offers several benefits over existing methods, including:

* Smoother and more precise control over editing intensity
* Ability to achieve customized results without requiring extensive tuning
* Easy integration with existing frameworks

This research has the potential to revolutionize image editing, enabling users to make precise and controlled edits with ease. The code for GRAG will be made publicly available, allowing developers and researchers to build upon this innovation."
cs.CV,"Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making   Datasets in Digital Pathology","Interpretation of giga-pixel whole-slide images (WSIs) is an important but difficult task for pathologists. Their diagnostic accuracy is estimated to average around 70%. Adding a second pathologist does not substantially improve decision consistency. The field lacks adequate behavioral data to explain diagnostic errors and inconsistencies. To fill in this gap, we present PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual search and decision-making processes of the full diagnostic workflow during cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data collection process emphasizes ecological validity through an application-grounded testbed, called PTAH. In total, we recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In addition, such data could also be used to improve the training of both pathologists and AI systems that might support human experts. All experiments were preregistered at https://osf.io/hj9a7, and the complete dataset along with analysis code is available at https://go.osu.edu/pathogaze.",http://arxiv.org/abs/2510.24653v1,2025-10-28T17:18:43Z,"Veronica Thai, Rui Li, Meng Ling, Shuning Jiang, Jeremy Wolfe, Raghu Machiraju, Yan Hu, Zaibo Li, Anil Parwani, Jian Chen","**Unlocking the Secrets of Decision-Making in Digital Pathology**

Imagine being a detective trying to solve a complex puzzle with millions of pieces. That's similar to what pathologists do when interpreting huge images of tissue samples to diagnose diseases like cancer. However, their accuracy rate is only around 70%, and adding another expert doesn't significantly improve the results. To understand why mistakes happen, researchers have created a massive dataset called PathoGaze1.0.

This dataset contains information on how 19 pathologists visually searched, interacted with, and made decisions about 397 images. The data includes:

* Where they looked on the images (eye-tracking)
* How they moved their mice to navigate the images (mouse tracking)
* How they viewed and navigated the images (stimulus tracking)
* Their final diagnoses

The dataset is like a treasure trove of information that can help us:

* Understand why pathologists make mistakes
* Improve the training of pathologists
* Develop more accurate AI systems to support pathologists

The good news is that this dataset is publicly available, along with the code used to analyze it, which can lead to better diagnosis and treatment of diseases."
cs.CV,A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries,"The rapid advancement of generative AI has enabled the creation of highly realistic forged facial images, posing significant threats to AI security, digital media integrity, and public trust. Face forgery techniques, ranging from face swapping and attribute editing to powerful diffusion-based image synthesis, are increasingly being used for malicious purposes such as misinformation, identity fraud, and defamation. This growing challenge underscores the urgent need for robust and generalizable face forgery detection methods as a critical component of AI security infrastructure. In this work, we propose a novel dual-branch convolutional neural network for face forgery detection that leverages complementary cues from both spatial and frequency domains. The RGB branch captures semantic information, while the frequency branch focuses on high-frequency artifacts that are difficult for generative models to suppress. A channel attention module is introduced to adaptively fuse these heterogeneous features, highlighting the most informative channels for forgery discrimination. To guide the network's learning process, we design a unified loss function, FSC Loss, that combines focal loss, supervised contrastive loss, and a frequency center margin loss to enhance class separability and robustness. We evaluate our model on the DiFF benchmark, which includes forged images generated from four representative methods: text-to-image, image-to-image, face swap, and face edit. Our method achieves strong performance across all categories and outperforms average human accuracy. These results demonstrate the model's effectiveness and its potential contribution to safeguarding AI ecosystems against visual forgery attacks.",http://arxiv.org/abs/2510.24640v1,2025-10-28T17:06:40Z,"Xin Zhang, Yuqi Song, Fei Zuo","Here's a summary of the research paper for a general audience:

**AI-Generated Fake Faces: A Growing Threat**

The rapid advancement of artificial intelligence (AI) has made it possible to create incredibly realistic fake facial images. These fake faces can be used for malicious purposes such as spreading misinformation, identity theft, and defamation. To combat this threat, researchers are developing methods to detect AI-generated fake faces.

**A New Approach to Detecting Fake Faces**

In a recent study, researchers proposed a novel approach to detecting fake faces using a dual-branch convolutional neural network (CNN). This network analyzes images in two ways: one branch looks at the visual information in the image, while the other branch examines the image's frequency domain (think of it like analyzing the image's ""hidden patterns""). By combining these two types of analysis, the network can more effectively detect fake faces.

**How it Works**

The researchers also introduced a few key innovations to improve the network's performance:

* A channel attention module that helps the network focus on the most important features in the image.
* A new loss function that guides the network's learning process and helps it distinguish between real and fake faces.

**Promising Results**

The researchers tested their approach on a benchmark dataset that included fake images generated using four different methods. Their method achieved strong performance across all categories, outperforming the average human accuracy. These results suggest that this approach has the potential to be a valuable tool in safeguarding AI ecosystems against visual forgery attacks.

Overall, this study highlights the importance of developing robust methods to detect AI-generated fake faces and the potential of dual-branch CNNs to address this challenge."
cs.CV,GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization,"In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline designed to localize a mobile robot in large-scale outdoor environments using prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing on the perceived ground area and utilizes the place recognition network R2D2, or alternatively, the non-learning approach Scale-Invariant Feature Transform (SIFT), to identify and select keypoints for BEV image map registration. Our results demonstrate that GroundLoc outperforms state-of-the-art methods on the SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE) well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime requirements. The system supports various sensor models, as evidenced by evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II, and Livox Avia sensors. The prior maps are stored as 2D raster image maps, which can be created from a single drive and require only 4 MB of storage per square kilometer. The source code is available at https://github.com/dcmlr/groundloc.",http://arxiv.org/abs/2510.24623v1,2025-10-28T16:51:50Z,"Nicolai Steinke, Daniel Goehring","**Efficient Outdoor Robot Localization using LiDAR Technology**

Researchers have developed a new system called GroundLoc, which enables robots to accurately determine their location in large outdoor environments using LiDAR (Light Detection and Ranging) technology. LiDAR uses laser light to create high-resolution 3D maps of surroundings.

GroundLoc uses a prior map of the area, created from a single drive, to help the robot localize itself. The system projects LiDAR data onto a 2D image, focusing on the ground area, and then uses a computer algorithm to match this image with the prior map. This approach allows the robot to accurately determine its location, with an average error of less than 50 cm.

The GroundLoc system has several advantages:

* **Efficient storage**: Prior maps require only 4 MB of storage per square kilometer.
* **Flexible sensor compatibility**: The system works with various LiDAR sensor models.
* **Fast processing**: GroundLoc meets online runtime requirements, making it suitable for real-time applications.

The researchers tested GroundLoc on several datasets and found that it outperformed existing methods. The system's source code is now available, making it accessible to developers and researchers. This technology has potential applications in areas such as autonomous vehicles, robotics, and surveying."
cs.CV,Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter   Correction in Cone-Beam CT,"Cone-beam CT (CBCT) employs a flat-panel detector to achieve three-dimensional imaging with high spatial resolution. However, CBCT is susceptible to scatter during data acquisition, which introduces CT value bias and reduced tissue contrast in the reconstructed images, ultimately degrading diagnostic accuracy. To address this issue, we propose a deep learning-based scatter artifact correction method inspired by physical prior knowledge. Leveraging the fact that the observed point scatter probability density distribution exhibits rotational symmetry in the projection domain. The method uses Gaussian Radial Basis Functions (RBF) to model the point scatter function and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides efficient nonlinear mapping capabilities for learning high-dimensional scatter features. By incorporating the physical characteristics of the scattered photon distribution together with the complex function mapping capacity of KAN, the model improves its ability to accurately represent scatter. The effectiveness of the method is validated through both synthetic and real-scan experiments. Experimental results show that the model can effectively correct the scatter artifacts in the reconstructed images and is superior to the current methods in terms of quantitative metrics.",http://arxiv.org/abs/2510.24579v1,2025-10-28T16:13:14Z,"Xu Jiang, Huiying Pan, Ligen Shi, Jianing Sun, Wenfeng Xu, Xing Zhao","**Improving Medical Imaging with AI: A Breakthrough in X-ray Scatter Correction**

Medical imaging technologies like Cone-Beam Computed Tomography (CBCT) provide high-resolution 3D images of the body. However, these images can be affected by ""scatter"" - a type of noise that degrades image quality and diagnostic accuracy. Researchers have developed a new artificial intelligence (AI) method to correct for scatter, inspired by the laws of physics.

The method uses a type of neural network called Kolmogorov-Arnold Networks (KAN) and incorporates Gaussian Radial Basis Functions to model the scatter of X-ray photons. By combining physical knowledge with advanced machine learning capabilities, the model can accurately correct for scatter artifacts in CBCT images.

**Key Findings:**

* The new AI method effectively corrects scatter artifacts in reconstructed images.
* The approach outperforms current methods in terms of image quality and diagnostic accuracy.
* The method was validated through both simulated and real-world experiments.

**Impact:**

This breakthrough has the potential to improve the accuracy of medical diagnoses and treatments. By enhancing the quality of CBCT images, healthcare professionals can better visualize and understand the body's internal structures, leading to more effective care and improved patient outcomes."
cs.CV,OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents,"With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.",http://arxiv.org/abs/2510.24563v1,2025-10-28T15:56:36Z,"Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, Fei Huang","Here's a summary of the research paper for a general audience:

**Advancing AI-Powered Computer Assistants: A New Benchmark for Evaluation**

Imagine having a computer assistant that can help you with various tasks, such as booking a flight, sending an email, or editing a document. To make these assistants more effective, researchers are developing AI-powered agents that can interact with computers like humans do. However, evaluating their performance has been a challenge.

A team of researchers has created a new benchmark called OSWorld-MCP to assess the abilities of these computer-use agents. Specifically, they focused on the agents' ability to use tools, such as software applications, to complete tasks. They developed a comprehensive test suite that includes 158 high-quality tools, covering common applications like email, web browsing, and document editing.

The researchers evaluated state-of-the-art AI agents using OSWorld-MCP and found that the agents' performance improved significantly when they could use tools to complete tasks. For example, one agent's success rate increased from 8.3% to 20.4% when using tools. However, even the best agents still struggled with using tools effectively, with only 36.3% of tool invocations being successful.

The OSWorld-MCP benchmark provides a fair and comprehensive way to evaluate computer-use agents, highlighting areas where they need improvement. By setting a new standard for evaluation, this research aims to advance the development of more effective AI-powered computer assistants. The researchers have made their code, environment, and data publicly available, allowing others to build upon their work."
cs.CV,Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal   Reasoning in MLLMs,"While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.",http://arxiv.org/abs/2510.24514v1,2025-10-28T15:26:20Z,"Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei","**Unlocking Visual Thinking in AI Models**

Imagine being able to sketch out your thoughts to help you plan and solve problems. Researchers have now enabled a similar capability in artificial intelligence (AI) models, specifically in Multimodal Large Language Models (MLLMs). These models are great at understanding images, but struggle with complex scenarios that require visual planning and imagination.

To address this limitation, the researchers introduced ""Latent Sketchpad,"" a framework that allows MLLMs to generate internal visual representations, similar to human sketching. This enables the model to interleave textual reasoning with visual generation, creating a more comprehensive thought process.

The Latent Sketchpad framework consists of two key components:

1. A ""Context-Aware Vision Head"" that generates visual representations based on the context.
2. A ""pretrained Sketch Decoder"" that translates these representations into human-interpretable images.

The researchers tested Latent Sketchpad on a new dataset called MazePlanning and found that it significantly improves the reasoning performance of MLLMs. The framework also works across different state-of-the-art MLLMs, including Gemma3 and Qwen2.5-VL.

This breakthrough opens up new opportunities for more natural and intuitive human-computer interaction, enabling AI models to think more like humans. The Latent Sketchpad framework has the potential to enhance a wide range of applications, from problem-solving to creative tasks."
cs.CV,Local Performance vs. Out-of-Distribution Generalization: An Empirical   Analysis of Personalized Federated Learning in Heterogeneous Data   Environments,"In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.",http://arxiv.org/abs/2510.24503v1,2025-10-28T15:15:14Z,"Mortesa Hussaini, Jan Theiß, Anthony Stein","Here's a summary of the research paper for a general audience:

**The Challenge of Machine Learning in Diverse Data Environments**

Imagine you're trying to train a machine learning model to recognize pictures of animals, but the pictures are taken in different parts of the world, with different lighting conditions, and different types of cameras. This diversity can make it hard for the model to learn and generalize well.

**The Problem with Current Approaches**

Current approaches to machine learning, such as Federated Learning, try to solve this problem by having multiple devices (like phones or computers) learn from their own data and then share their updates with a central server. However, this can lead to a problem where each device's model is optimized for its own data, but not for the data on other devices.

**A New Approach: Balancing Local Performance and Generalization**

This study explores the trade-off between two important aspects of machine learning models: their performance on their own data (local performance) and their ability to generalize to new, unseen data (out-of-distribution generalization). The researchers propose a new approach, called Federated Learning with Individualized Updates (FLIU), which adapts the traditional Federated Learning algorithm to better balance local performance and generalization.

**Key Findings**

The study finds that:

* Current Federated Learning approaches tend to focus on local performance, but neglect generalization to new data.
* The proposed FLIU approach can improve both local performance and generalization.
* The performance of machine learning models can vary greatly depending on the diversity of the data.

**Implications**

The study's findings have important implications for the development of machine learning models that can perform well in diverse data environments. By balancing local performance and generalization, machine learning models can become more robust and effective in real-world applications."
cs.CV,Fast and accurate neural reflectance transformation imaging through   knowledge distillation,"Reflectance Transformation Imaging (RTI) is very popular for its ability to visually analyze surfaces by enhancing surface details through interactive relighting, starting from only a few tens of photographs taken with a fixed camera and variable illumination. Traditional methods like Polynomial Texture Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle to accurately capture complex reflectance fields using few per-pixel coefficients and fixed bases, leading to artifacts, especially in highly reflective or shadowed areas. The NeuralRTI approach, which exploits a neural autoencoder to learn a compact function that better approximates the local reflectance as a function of light directions, has been shown to produce superior quality at comparable storage cost. However, as it performs interactive relighting with custom decoder networks with many parameters, the rendering step is computationally expensive and not feasible at full resolution for large images on limited hardware. Earlier attempts to reduce costs by directly training smaller networks have failed to produce valid results. For this reason, we propose to reduce its computational cost through a novel solution based on Knowledge Distillation (DisK-NeuralRTI). ...",http://arxiv.org/abs/2510.24486v1,2025-10-28T15:00:07Z,"Tinsae G. Dulecha, Leonardo Righetto, Ruggero Pintus, Enrico Gobbetti, Andrea Giachetti","Here's a summary of the research paper for a general audience:

**Advancing Reflectance Transformation Imaging: A Breakthrough in Surface Analysis**

Imagine taking a few dozen photos of an object or surface with a camera, but with different lighting conditions. This technique, called Reflectance Transformation Imaging (RTI), helps analyze the surface details by simulating different lighting effects. However, traditional methods have limitations, especially when dealing with complex surfaces that reflect light in various ways.

Researchers have proposed a new approach called NeuralRTI, which uses artificial intelligence (AI) to better capture the way light interacts with a surface. While NeuralRTI produces high-quality results, it requires significant computational power, making it impractical for large images or devices with limited hardware.

To overcome this challenge, the researchers introduced a novel solution called DisK-NeuralRTI, which leverages a technique called knowledge distillation. This approach enables the creation of a more efficient AI model that can produce high-quality results quickly and accurately, without requiring extensive computational resources.

In simple terms, DisK-NeuralRTI is a faster and more efficient way to analyze surfaces using RTI, enabling researchers and practitioners to gain deeper insights into the properties of materials and surfaces. This breakthrough has the potential to impact various fields, such as cultural heritage preservation, product design, and scientific research."
cs.CV,Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated   Sampling,"Denoising generative models, such as diffusion and flow-based models, produce high-quality samples but require many denoising steps due to discretization error. Flow maps, which estimate the average velocity between timesteps, mitigate this error and enable faster sampling. However, their training typically demands architectural changes that limit compatibility with pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding strategy that converts flow models into flow map models without architectural modifications. Our method conditions the final blocks of diffusion transformers on the subsequent timestep, allowing pretrained flow models to be directly repurposed as flow maps. Combined with enhanced training techniques, this design enables high-quality generation in as few as 1 to 4 steps. Notably, we find that training flow models and subsequently converting them is more efficient and effective than training flow maps from scratch. On ImageNet 256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12, respectively, surpassing prior art by a large margin. Furthermore, we achieve FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the performance of flow models while delivering over 100x faster inference.",http://arxiv.org/abs/2510.24474v1,2025-10-28T14:43:48Z,"Kyungmin Lee, Sihyun Yu, Jinwoo Shin","Here's a summary of the research paper for a general audience:

**Faster Image Generation with AI**

Researchers have developed a new method called Decoupled MeanFlow that speeds up the process of generating high-quality images using artificial intelligence (AI). Currently, AI models that generate images, such as those used for creating realistic pictures of faces or objects, require many steps to produce a good image. This can make the process slow.

The new method, Decoupled MeanFlow, allows these AI models to generate high-quality images much faster, in as few as 1 to 4 steps. This is achieved by converting existing AI models into a new type of model that estimates the average movement between steps, rather than relying on many individual steps.

The best part is that this method doesn't require building new AI models from scratch. Instead, it can be applied to existing models, making it a efficient and effective way to improve image generation. In fact, the researchers found that using Decoupled MeanFlow with existing models is more efficient and effective than training new models from scratch.

The results are impressive, with the new method achieving image quality scores that surpass previous state-of-the-art results. For example, on a dataset of images with a resolution of 256x256, the method achieved a score of 2.16 in just 1 step, and 1.51 in 4 steps. This is a significant improvement over previous methods, and it enables much faster image generation, with speeds that are over 100 times faster than before."
cs.CV,Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras,"Markerless multiview motion capture is often constrained by the need for precise camera calibration, limiting accessibility for non-experts and in-the-wild captures. Existing calibration-free approaches mitigate this requirement but suffer from high computational cost and reduced reconstruction accuracy.   We present Kineo, a fully automatic, calibration-free pipeline for markerless motion capture from videos captured by unsynchronized, uncalibrated, consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf detectors to simultaneously calibrate cameras, including Brown-Conrady distortion coefficients, and reconstruct 3D keypoints and dense scene point maps at metric scale. A confidence-driven spatio-temporal keypoint sampling strategy, combined with graph-based global optimization, ensures robust calibration at a fixed computational cost independent of sequence length. We further introduce a pairwise reprojection consensus score to quantify 3D reconstruction reliability for downstream tasks.   Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements over prior calibration-free methods. Compared to previous state-of-the-art approaches, Kineo reduces camera translation error by approximately 83-85%, camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by 83-91%.   Kineo is also efficient in real-world scenarios, processing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage). The full pipeline and evaluation code are openly released to promote reproducibility and practical adoption at https://liris-xr.github.io/kineo/.",http://arxiv.org/abs/2510.24464v1,2025-10-28T14:30:47Z,"Charles Javerliat, Pierre Raimbaud, Guillaume Lavoué","**Breakthrough in Motion Capture Technology: Kineo**

Imagine being able to capture human movements with precision, using just a few regular cameras, without needing specialized equipment or expertise. Researchers have developed a new technology called Kineo, which makes this possible. Kineo is a motion capture system that uses multiple RGB cameras to track human movements in 3D space, without requiring precise camera calibration.

**What does this mean?**

Traditional motion capture systems rely on precise camera calibration, which can be time-consuming and requires expertise. Kineo eliminates this need, making it more accessible and practical for various applications. The system uses artificial intelligence to detect key points on the human body, such as joints, and then reconstructs the 3D movements.

**How does it work?**

Kineo's innovative approach combines 2D keypoint detection with advanced optimization techniques to simultaneously calibrate the cameras and reconstruct 3D movements. This results in accurate and reliable motion capture, even with sparse camera views.

**Key benefits:**

* **Easy to use**: No need for precise camera calibration or specialized expertise.
* **Accurate**: Kineo achieves state-of-the-art performance, with significant improvements over existing calibration-free methods (83-92% reduction in errors).
* **Efficient**: Can process multi-view sequences faster than their duration (e.g., 36 minutes to process 1 hour and 20 minutes of footage).

**Potential applications:**

* Film and video production
* Sports analysis and training
* Healthcare and rehabilitation
* Virtual reality and gaming

The Kineo pipeline and evaluation code are openly available, promoting reproducibility and practical adoption. This breakthrough technology has the potential to democratize motion capture, making it more accessible and affordable for various industries and applications."
cs.CV,A Critical Study towards the Detection of Parkinsons Disease using ML   Technologies,"The proposed solution is Deep Learning Technique that will be able classify three types of tea leaves diseases from which two diseases are caused by the pests and one due to pathogens (infectious organisms) and environmental conditions and also show the area damaged by a disease in leaves. Namely Red Rust, Helopeltis and Red spider mite respectively. In this paper we have evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%. While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95 and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than SSD. Also used Mask R-CNN for Object Instance Segmentation where we have implemented our custom method to calculate the damaged diseased portion of leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.",http://arxiv.org/abs/2510.24456v1,2025-10-28T14:24:34Z,"Vivek Chetia, Abdul Taher Khan, Rahish Gogoi, David Kapsian Khual, Purnendu Bikash, Sajal Saha","I think there's been a mistake!

The research paper you provided doesn't seem to be related to Parkinson's Disease, but rather to detecting diseases in tea leaves using machine learning technologies. Here's a summary for a general audience:

**Original Research Focus:** This study aimed to develop a deep learning technique to detect diseases in tea leaves, specifically Red Rust, Helopeltis, and Red spider mite. The researchers tested two object detection models, SSD MobileNet V2 and Faster R-CNN ResNet50 V1, and found that Faster R-CNN performed better. They also used Mask R-CNN to segment and calculate the damaged area of the leaves.

**Not related to Parkinson's Disease:** Unfortunately, this study does not investigate Parkinson's Disease, a neurodegenerative disorder that affects movement and motor control. If you're interested in learning more about Parkinson's Disease research, I'd be happy to help summarize a relevant study!"
cs.CV,Rethinking Visual Intelligence: Insights from Video Pretraining,"Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.",http://arxiv.org/abs/2510.24448v1,2025-10-28T14:12:11Z,"Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro","**Unlocking Visual Intelligence: A Breakthrough in AI Research**

Imagine a computer system that can quickly learn to solve new problems, just like humans do. This has been achieved in language processing, but not in visual processing, where computers struggle to understand complex scenes and tasks. Researchers have been trying to bridge this gap, and a new study provides promising insights.

The study explores the potential of ""video pretraining,"" which involves training AI models on vast amounts of video data. This approach enables models to develop a deeper understanding of structure and dynamics, similar to how humans learn from watching and experiencing the world.

To test the effectiveness of video pretraining, researchers compared two types of AI models: one trained on language data and the other on video data. They equipped both models with simple ""adapters"" that allowed them to learn new tasks quickly. The results showed that the video-trained model outperformed the language-trained model in various visual tasks, such as understanding complex scenes, planning routes, and solving visual games.

These findings suggest that video pretraining can provide AI models with a strong foundation for visual intelligence, enabling them to adapt to new tasks more efficiently. This breakthrough has the potential to lead to more sophisticated AI systems that can understand and interact with the visual world in a more human-like way."
cs.CV,SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box   Adversarial Paraphrasing in Text Autoencoder Latent Space,"Multimodal large language models (MLLMs) have shown impressive capabilities in vision-language tasks such as reasoning segmentation, where models generate segmentation masks based on textual queries. While prior work has primarily focused on perturbing image inputs, semantically equivalent textual paraphrases-crucial in real-world applications where users express the same intent in varied ways-remain underexplored. To address this gap, we introduce a novel adversarial paraphrasing task: generating grammatically correct paraphrases that preserve the original query meaning while degrading segmentation performance. To evaluate the quality of adversarial paraphrases, we develop a comprehensive automatic evaluation protocol validated with human studies. Furthermore, we introduce SPARTA-a black-box, sentence-level optimization method that operates in the low-dimensional semantic latent space of a text autoencoder, guided by reinforcement learning. SPARTA achieves significantly higher success rates, outperforming prior methods by up to 2x on both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive baselines to assess the robustness of advanced reasoning segmentation models. We reveal that they remain vulnerable to adversarial paraphrasing-even under strict semantic and grammatical constraints. All code and data will be released publicly upon acceptance.",http://arxiv.org/abs/2510.24446v1,2025-10-28T14:09:05Z,"Viktoriia Zinkovich, Anton Antonov, Andrei Spiridonov, Denis Shepelev, Andrey Moskalenko, Daria Pugacheva, Elena Tutubalina, Andrey Kuznetsov, Vlad Shakhuro","Here's a summary of the research paper for a general audience:

**Can AI Models Handle Different Ways of Asking Questions?**

Artificial intelligence (AI) models have become really good at understanding images and text. For example, they can look at a picture and generate a mask to highlight specific parts of the image based on a text query. However, researchers have found that these models can be tricked by changing the wording of the text query, even if the meaning remains the same.

In this study, researchers developed a new method called SPARTA to test the robustness of these AI models. SPARTA generates alternative versions of a text query that have the same meaning but are worded differently. The goal is to see if the AI model can still accurately generate the correct segmentation mask.

The researchers found that even advanced AI models can be vulnerable to these alternative queries, and their performance can degrade significantly. This highlights the need for more robust AI models that can handle different ways of asking questions.

The researchers also developed a new evaluation protocol to assess the quality of these alternative queries and plan to release their code and data publicly. This work has important implications for developing more reliable and trustworthy AI models."
cs.AI,Does Object Binding Naturally Emerge in Large Pretrained Vision   Transformers?,"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.",http://arxiv.org/abs/2510.24709v1,2025-10-28T17:57:05Z,"Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording","**Unlocking Object Binding in AI: A Breakthrough in Vision Transformers**

Imagine you're looking at a picture of a cat wearing a hat. Your brain automatically groups the features of the cat (its fur, eyes, whiskers) and the hat (its shape, color, texture) into two separate objects. This ability, called object binding, is crucial for human cognition. But do artificial intelligence (AI) models, specifically Vision Transformers (ViTs), have this ability too?

Researchers investigated whether ViTs, a type of AI model that processes visual data, can naturally group features into objects without explicit programming. They found that ViTs pretrained using self-supervised methods (e.g., DINO, MAE, CLIP) can indeed represent object binding, accurately identifying which patches of an image belong to the same object. This ability emerges because object binding helps the model make better predictions and improve its performance.

The study used a technique called similarity probe to decode object binding information from the model's internal representations. They discovered that object binding is encoded in a low-dimensional subspace on top of object features and guides attention. In simpler terms, the model uses object binding to focus on specific parts of the image and make sense of the visual data.

The researchers also found that object binding is not just a trivial byproduct of the model's architecture, but rather an ability acquired through specific pretraining objectives. When they removed object binding information from the model's internal representations, the model's performance degraded, highlighting the importance of object binding for the model's performance.

These findings challenge the common view that ViTs lack object binding and demonstrate that symbolic knowledge of ""which parts belong together"" can emerge naturally in a connectionist system. In other words, AI models can learn to group features into objects without explicit programming, just like humans do. This breakthrough has significant implications for the development of more advanced AI models that can better understand and interpret visual data."
cs.AI,ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality   Games?,"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.",http://arxiv.org/abs/2510.24706v1,2025-10-28T17:55:42Z,"Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu","**Can AI Models Play Virtual Reality Games Like Humans?**

Imagine playing a virtual reality (VR) game where you need to use a controller to interact with virtual objects. Humans can easily do this, but can artificial intelligence (AI) models, specifically Large Language Models (LLMs), perform similarly?

Researchers created a benchmark called ComboBench to test LLMs' ability to translate simple actions into precise movements to play VR games. They evaluated seven LLMs across 262 scenarios from four popular VR games and compared their performance to human players.

The results showed that while top-performing LLMs demonstrated strong abilities, they still struggled with understanding spatial relationships and following procedures, unlike humans. The performance of LLMs varied greatly across games, suggesting that they are sensitive to the complexity of interactions.

However, providing LLMs with a few examples of correct actions significantly improved their performance. This suggests that targeted training can enhance LLMs' ability to interact with VR games.

The study's findings have implications for the development of more sophisticated AI models that can interact with virtual environments. The researchers have made all their materials publicly available, paving the way for further research in this area."
cs.AI,"Agent Data Protocol: Unifying Datasets for Diverse, Effective   Fine-tuning of LLM Agents","Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an ""interlingua"" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.",http://arxiv.org/abs/2510.24702v1,2025-10-28T17:53:13Z,"Yueqi Song, Ketan Ramaneti, Zaid Sheikh, Ziru Chen, Boyu Gou, Tianbao Xie, Yiheng Xu, Danyang Zhang, Apurva Gandhi, Fan Yang, Joseph Liu, Tianyue Ou, Zhihao Yuan, Frank Xu, Shuyan Zhou, Xingyao Wang, Xiang Yue, Tao Yu, Huan Sun, Yu Su, Graham Neubig","Here's a summary of the research paper for a general audience:

**Making AI Training Easier and More Effective**

Training artificial intelligence (AI) agents to perform complex tasks is a challenging task. One of the main hurdles is collecting and organizing the data needed to train these agents. Currently, data is scattered across different formats, tools, and interfaces, making it difficult to use.

To address this issue, researchers have developed a new protocol called the Agent Data Protocol (ADP). ADP is a simple and flexible language that allows different types of data to be unified and used for training AI agents. This protocol can capture a wide range of tasks, such as using software tools, browsing the internet, coding, and more.

In experiments, researchers used ADP to combine 13 existing datasets and train AI agents using this unified data. The results showed that the AI agents performed about 20% better than previous models, and achieved state-of-the-art or near-top performance on various benchmarks.

The good news is that the researchers have made all their code and data publicly available. This means that other researchers and developers can use ADP to train their own AI agents more easily and effectively. By standardizing the way AI agents are trained, ADP has the potential to lower the barrier to creating more advanced and capable AI systems."
cs.AI,Tongyi DeepResearch Technical Report,"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",http://arxiv.org/abs/2510.24701v1,2025-10-28T17:53:02Z,"Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang","Here's a summary of the research paper for a general audience:

**Introducing Tongyi DeepResearch: A Powerful AI Model for In-Depth Research**

Imagine having a super-smart research assistant that can dig deep into complex topics, find relevant information, and provide insightful answers. That's what Tongyi DeepResearch is - a cutting-edge AI model designed to perform in-depth research tasks.

**What makes Tongyi DeepResearch special?**

* It's a large language model with 30.5 billion parameters, making it capable of understanding and analyzing vast amounts of information.
* It's been trained using a unique framework that allows it to learn autonomously, without relying on human annotation.
* It can perform complex tasks, such as seeking information, reasoning, and problem-solving, across a range of subjects.

**What can Tongyi DeepResearch do?**

* Achieve state-of-the-art performance on various research benchmarks, such as:
	+ Answering complex questions
	+ Browsing and summarizing web content
	+ Solving problems that require deep understanding

**The best part?**

The researchers behind Tongyi DeepResearch are open-sourcing the model, framework, and solutions, making it available to the wider community to use and build upon. This could lead to exciting advancements in AI-powered research and applications."
cs.AI,Greedy Sampling Is Provably Efficient for RLHF,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.",http://arxiv.org/abs/2510.24700v1,2025-10-28T17:52:08Z,"Di Wu, Chengshuai Shi, Jing Yang, Cong Shen","**Improving AI Training with a Simple yet Powerful Technique**

Researchers have made a breakthrough in training large language models, a crucial component of many AI systems. The technique, called Reinforcement Learning from Human Feedback (RLHF), helps fine-tune these models to better align with human values and preferences. Despite its success, the underlying math behind RLHF was not well understood - until now.

The study focuses on a key challenge in RLHF: how to efficiently learn from human feedback, which is often given in the form of preferences (e.g., ""this response is better than that one""). The researchers discovered that a surprisingly simple approach, called ""greedy sampling,"" can be highly effective. This approach involves choosing the next action based on the most likely outcome, rather than trying to construct complex estimates.

The findings show that greedy sampling can achieve state-of-the-art performance in RLHF, outperforming existing methods. This is significant because it provides a more efficient and straightforward way to train AI models. The researchers also found that this approach works particularly well for a specific type of preference model, called the Bradley-Terry model.

In simple terms, the study shows that a straightforward and intuitive approach can be highly effective in training AI models to align with human values and preferences. This breakthrough has the potential to improve the performance and reliability of many AI systems."
cs.AI,ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking,"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.",http://arxiv.org/abs/2510.24698v1,2025-10-28T17:51:50Z,"Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang","Here's a summary of the research paper ""ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking"" for a general audience:

**Improving How Computers Search for Information**

Imagine you're trying to solve a complex problem, like planning a trip or understanding a new technology. You'd want to explore different possibilities and gather information from various sources to make an informed decision. Computers can do the same, but they often struggle with exploring many options efficiently and making sense of the information they gather.

Researchers have proposed a new approach called ParallelMuse, which helps computers think more effectively by exploring multiple possibilities in parallel. This approach addresses two main challenges: 

1. **Inefficiency**: Computers often have to start from scratch every time they explore a new possibility, which can be time-consuming.
2. **Information Overload**: Computers have limited capacity to store and process information, making it difficult to integrate and make sense of the information they gather.

ParallelMuse works in two stages:

1. **Efficient Exploration**: It breaks down the exploration process into smaller, manageable parts, allowing the computer to reuse and build upon previous explorations. This reduces the number of steps needed to gather information.
2. **Smart Information Integration**: It compresses the gathered information, removing redundant details and synthesizing a coherent final answer.

**Results**

The researchers tested ParallelMuse on multiple computer systems and benchmarks, and the results showed significant improvements:

* Up to 62% better performance in solving complex problems
* 10-30% reduction in the number of steps needed to gather information

Overall, ParallelMuse offers a promising approach to improve how computers search for information and solve complex problems."
cs.AI,AgentFold: Long-Horizon Web Agents with Proactive Context Management,"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.",http://arxiv.org/abs/2510.24699v1,2025-10-28T17:51:50Z,"Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang","Here's a summary of the research paper ""AgentFold: Long-Horizon Web Agents with Proactive Context Management"" for a general audience:

**Improving AI Web Agents**

Imagine you're trying to book a flight and hotel for a trip, but you need to navigate through multiple websites to find the best deals. AI web agents are designed to help with such tasks, but they often struggle with long-term goals. A new approach called AgentFold aims to change that.

**The Problem: Context Management**

Current AI agents have a hard time managing their ""memory"" of previous interactions. They either get overwhelmed by too much information or lose important details. AgentFold addresses this issue by introducing a proactive context management system, inspired by how humans process information.

**How AgentFold Works**

AgentFold treats its context like a dynamic workspace that can be actively managed. At each step, it decides how to condense or summarize its previous interactions, preserving important details while discarding unnecessary ones. This allows it to focus on the most relevant information and make better decisions.

**Impressive Results**

The results are impressive: AgentFold outperforms much larger AI models and even rivals proprietary agents like those from OpenAI. With simple fine-tuning, AgentFold achieves high success rates on two benchmark tasks. This breakthrough has the potential to significantly improve AI web agents and make them more effective at completing complex tasks."
cs.AI,Repurposing Synthetic Data for Fine-grained Search Agent Supervision,"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative ""near-miss"" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these ""near-misses"". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.",http://arxiv.org/abs/2510.24694v1,2025-10-28T17:50:40Z,"Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang","**Improving Search Agents with Better Training Data**

Imagine you're searching for information online, and a virtual assistant helps you find the answers. These assistants, called search agents, are getting better at solving complex tasks, but they still need to improve. Researchers have been training these agents using artificial data that mimics real-world information. However, the current training methods have a limitation: they only focus on whether the final answer is correct or not, ignoring valuable information about the entities (like people, places, or organizations) mentioned in the data.

A team of researchers has found a way to improve the training process by leveraging this entity information. They discovered that there's a strong correlation between the number of correct entities identified by the agent and the accuracy of its final answer. Building on this insight, they developed a new framework called Entity-aware Group Relative Policy Optimization (E-GRPO). This framework assigns partial rewards to incorrect answers based on how many entities they match, allowing the agent to learn from its mistakes.

**Key Findings:**

* The new framework, E-GRPO, outperforms the current state-of-the-art method (GRPO) in various question-answering and research benchmarks.
* E-GRPO achieves better accuracy and induces more efficient reasoning policies, requiring fewer tool calls.
* The approach is more effective and sample-efficient, making it a promising solution for improving search agents.

**What does this mean?**

This research has the potential to improve the performance of search agents, making them more accurate and efficient in solving complex tasks. By leveraging entity information, E-GRPO can help search agents learn from their mistakes and provide better results, ultimately enhancing the search experience for users."
cs.AI,Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework   for In-Context Planning,"We present a framework for uncovering and exploiting dependencies among tools and documents to enhance exemplar artifact generation. Our method begins by constructing a tool knowledge graph from tool schemas,including descriptions, arguments, and output payloads, using a DeepResearch-inspired analysis. In parallel, we derive a complementary knowledge graph from internal documents and SOPs, which is then fused with the tool graph. To generate exemplar plans, we adopt a deep-sparse integration strategy that aligns structural tool dependencies with procedural knowledge. Experiments demonstrate that this unified framework effectively models tool interactions and improves plan generation, underscoring the benefits of linking tool graphs with domain knowledge graphs for tool-augmented reasoning and planning.",http://arxiv.org/abs/2510.24690v1,2025-10-28T17:50:15Z,"Shengjie Liu, Li Dong, Zhenyu Zhang","**Unlocking the Power of Tools and Knowledge: A New Framework for Planning**

Imagine you're trying to build a complex project, but you have many different tools at your disposal, each with its own strengths and weaknesses. How do you choose the right tools and use them in the right order to get the job done? A new research paper presents a innovative solution to this problem.

The researchers have developed a framework that helps connect the dots between different tools and the knowledge contained in documents and standard operating procedures (SOPs). By creating a ""map"" of how tools work together and combining it with a ""map"" of domain-specific knowledge, they can generate effective plans for completing tasks.

This approach has been tested and shown to improve the generation of plans, highlighting the benefits of integrating tool knowledge with domain expertise. The framework has the potential to enhance decision-making and planning in a wide range of fields, from business and healthcare to engineering and finance.

In simple terms, this research provides a powerful new way to:

1. Understand how different tools work together
2. Tap into existing knowledge and expertise
3. Generate effective plans for complex tasks

By bridging the gap between tool dependencies and domain knowledge, this framework can help individuals and organizations make more informed decisions and achieve their goals more efficiently."
cs.AI,Fast algorithms enabling optimization and deep learning for   photoacoustic tomography in a circular detection geometry,"The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \times n)$ image, our algorithms compute these operators in $\mathcal{O}(n^2 \log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.",http://arxiv.org/abs/2510.24687v1,2025-10-28T17:49:31Z,"Andreas Hauptmann, Leonid Kunyansky, Jenni Poimala","**Breakthrough in Medical Imaging: Faster Algorithms for Photoacoustic Tomography**

Photoacoustic tomography is a medical imaging technique that uses light and sound to create detailed images of the body's internal structures. However, reconstructing these images can be a complex and time-consuming process. Researchers have developed new algorithms that significantly speed up this process, enabling faster and more accurate image reconstruction.

The new algorithms work by efficiently solving the ""inverse source problem"", which is a mathematical challenge in photoacoustic tomography. These algorithms can be used with traditional image reconstruction methods, such as non-negative least squares and total variation regularized least squares, as well as with newer deep learning techniques.

The key achievement of this research is that the new algorithms can compute the necessary mathematical operations in O(n^2 log n) floating point operations, which is a significant improvement over previous methods. This enables faster and more efficient image reconstruction, which can lead to better diagnosis and treatment of diseases.

The researchers have also made their algorithms publicly available, along with example code and computational examples, which can be used by others to accelerate their own research and applications in photoacoustic tomography. This development has the potential to improve medical imaging and accelerate advancements in the field."
cs.AI,Dissecting Role Cognition in Medical LLMs via Neuronal Ablation,"Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the   context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP),   instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied   professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This   study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct,   role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA   datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our   results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they   primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation   across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform   across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical   practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine   cognitive processes rather than linguistic imitation.We have released the related code in the following repository:https:   //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",http://arxiv.org/abs/2510.24677v1,2025-10-28T17:40:53Z,"Xun Liang, Huayi Lai, Hanyu Wang, Wentao Zhang, Linfeng Zhang, Yanfang Chen, Feiyu Xiong, Zhiyu Li","Here's a summary of the research paper for a general audience:

**The Limitations of Role-Playing in Medical AI**

Imagine asking a computer to pretend to be a doctor, a nurse, or a medical student to help with medical decisions. This approach, called role-playing, is becoming increasingly popular in medical artificial intelligence (AI). However, researchers have wondered whether this role-playing truly helps computers think like medical professionals or if it's just a superficial imitation.

To investigate this, researchers developed a new framework to evaluate how well large language models (LLMs) - a type of AI - can simulate different medical roles. They tested three medical question-answering datasets and used techniques like ""neuron ablation"" (essentially, ""disabling"" certain parts of the AI's brain) to see how the models' reasoning changed.

The surprising finding? Role-playing prompts don't actually improve the medical reasoning abilities of LLMs. Instead, they mainly affect the way the models respond, making them sound more like a doctor or a student, but not actually changing how they think. The researchers conclude that current role-playing methods are limited and don't truly replicate the complex thinking processes of real medical professionals.

This study highlights the need for more advanced AI models that can genuinely simulate human-like thinking and decision-making in medicine, rather than just mimicking language."
cs.AI,Learning to Drive Safely with Hybrid Options,"Out of the many deep reinforcement learning approaches for autonomous driving, only few make use of the options (or skills) framework. That is surprising, as this framework is naturally suited for hierarchical control applications in general, and autonomous driving tasks in specific. Therefore, in this work the options framework is applied and tailored to autonomous driving tasks on highways. More specifically, we define dedicated options for longitudinal and lateral manoeuvres with embedded safety and comfort constraints. This way, prior domain knowledge can be incorporated into the learning process and the learned driving behaviour can be constrained more easily. We propose several setups for hierarchical control with options and derive practical algorithms following state-of-the-art reinforcement learning techniques. By separately selecting actions for longitudinal and lateral control, the introduced policies over combined and hybrid options obtain the same expressiveness and flexibility that human drivers have, while being easier to interpret than classical policies over continuous actions. Of all the investigated approaches, these flexible policies over hybrid options perform the best under varying traffic conditions, outperforming the baseline policies over actions.",http://arxiv.org/abs/2510.24674v1,2025-10-28T17:40:04Z,"Bram De Cooman, Johan Suykens","**Learning to Drive Safely with Hybrid Options**

Imagine a self-driving car that can navigate through busy highways safely and smoothly. Researchers have been working on developing autonomous driving systems using a technique called deep reinforcement learning. In a recent study, they explored a new approach called the ""options framework"" to improve the safety and flexibility of autonomous driving.

The researchers applied this framework to highway driving tasks, defining specific ""options"" for controlling the car's speed (longitudinal) and steering (lateral) while ensuring safety and comfort. By breaking down driving tasks into smaller, manageable options, they created a more hierarchical and interpretable system.

The study found that using hybrid options, which combine longitudinal and lateral control, allows for more flexible and safe driving behaviors. This approach outperformed traditional methods, especially in varying traffic conditions. The results suggest that this new approach can lead to more reliable and efficient autonomous driving systems.

**In simple terms:** This research proposes a new way to teach self-driving cars to drive safely and smoothly on highways. By breaking down driving tasks into smaller, manageable parts, the system can learn to make decisions more like a human driver, while prioritizing safety and comfort."
cs.AI,Multi-Agent Scenario Generation in Roundabouts with a   Transformer-enhanced Conditional Variational Autoencoder,"With the increasing integration of intelligent driving functions into serial-produced vehicles, ensuring their functionality and robustness poses greater challenges. Compared to traditional road testing, scenario-based virtual testing offers significant advantages in terms of time and cost efficiency, reproducibility, and exploration of edge cases. We propose a Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for generating multi-agent traffic scenarios in roundabouts, which are characterized by high vehicle dynamics and complex layouts, yet remain relatively underexplored in current research. The results show that the proposed model can accurately reconstruct original scenarios and generate realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators (KPIs) are employed to evaluate the interactive behavior in the generated scenarios. Analysis of the latent space reveals partial disentanglement, with several latent dimensions exhibiting distinct and interpretable effects on scenario attributes such as vehicle entry timing, exit timing, and velocity profiles. The results demonstrate the model's capability to generate scenarios for the validation of intelligent driving functions involving multi-agent interactions, as well as to augment data for their development and iterative improvement.",http://arxiv.org/abs/2510.24671v1,2025-10-28T17:36:52Z,"Li Li, Tobias Brinkmann, Till Temmen, Markus Eisenbarth, Jakob Andert","**Advancing Virtual Testing for Self-Driving Cars: A New Approach**

As self-driving cars become more common, ensuring they work safely and reliably is crucial. One way to test these vehicles is through virtual scenarios, which can be more efficient and cost-effective than traditional road testing. Researchers have developed a new model, called a Transformer-enhanced Conditional Variational Autoencoder (CVAE-T), to generate realistic and diverse scenarios for testing self-driving cars in roundabouts.

Roundabouts are complex traffic situations that are often challenging for self-driving cars to navigate. The CVAE-T model can create virtual scenarios that mimic real-world traffic patterns, allowing researchers to test how self-driving cars interact with other vehicles in these situations. The model can also generate new scenarios that are similar to real-world ones, but with different variables, such as different traffic speeds or vehicle entry times.

The results show that the CVAE-T model is effective in generating realistic and diverse scenarios, and can help improve the development and testing of self-driving cars. This technology has the potential to make virtual testing more efficient and effective, ultimately contributing to the development of safer and more reliable self-driving cars."
cs.AI,InteractComp: Evaluating Search Agents With Ambiguous Queries,"Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.",http://arxiv.org/abs/2510.24668v1,2025-10-28T17:35:54Z,"Mingyi Deng, Lijun Huang, Yani Fan, Jiayi Zhang, Fashen Ren, Jinyi Bai, Fuzhen Yang, Dayi Miao, Zhaoyang Yu, Yifan Wu, Yanfei Zhang, Fengwei Teng, Yingjia Wan, Song Hu, Yude Li, Xin Jin, Conghao Hu, Haoyu Li, Qirui Fu, Tai Zhong, Xinyu Wang, Xiangru Tang, Nan Tang, Chenglin Wu, Yuyu Luo","**Improving Search Agents: A New Benchmark for Handling Ambiguous Queries**

Imagine you're searching for information online, but your initial query is unclear or incomplete. You'd likely want the search agent to ask for clarification or provide options to help refine your search. However, most search agents today assume your query is clear and complete, which isn't always the case.

To address this limitation, researchers have created a new benchmark called InteractComp. This benchmark evaluates the ability of search agents to recognize when a query is ambiguous and interact with the user to clarify it. The researchers tested 17 search models and found that they performed poorly, with the best model achieving only 13.73% accuracy.

The good news is that when the search agents were forced to interact with the user, their performance improved dramatically. This suggests that current search agents have the potential to handle ambiguous queries, but they're not using it.

The study also found that the ability of search agents to interact with users hasn't improved much over the past 15 months, despite significant advancements in search performance. This highlights a critical blind spot in current search agents.

The InteractComp benchmark provides a valuable resource for evaluating and training search agents to handle ambiguous queries. By improving this capability, search agents can become more effective and user-friendly, providing better results and a more satisfying search experience."
cs.AI,OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan   DAGs,"Agentic tool use has gained traction with the rise of agentic tool calling, yet most existing work overlooks the complexity of multi-turn tool interactions. We introduce OrchDAG, a synthetic data generation pipeline that models tool execution as directed acyclic graphs (DAGs) with controllable complexity. Using this dataset, we benchmark model performance and propose a graph-based reward to enhance RLVR training. Experiments show that the dataset presents a challenging but solvable benchmark, and the proposed reward is effective when combined with GRPO-style algorithms, highlighting the importance of leveraging topological structure and data complexity in multi-turn tool use.",http://arxiv.org/abs/2510.24663v1,2025-10-28T17:28:01Z,"Yifu Lu, Shengjie Liu, Li Dong","Here's a summary of the research paper for a general audience:

**Title:** OrchDAG: A New Way to Manage Complex Tool Interactions

**What it's about:** Imagine you're trying to accomplish a task that requires using multiple tools in a specific order. For example, you might need to use a map to find a location, then use a calculator to figure out the best route, and finally use a messaging app to share the route with a friend. This can be tricky, especially when there are many tools involved and the order matters.

**The problem:** Most current AI systems struggle with these kinds of multi-step tasks, especially when they involve many tools and complex interactions.

**The solution:** Researchers have developed a new system called OrchDAG, which generates synthetic data to help train AI models to handle these complex tool interactions. OrchDAG uses a graph-like structure to represent the tools and their interactions, allowing for more flexible and controllable complexity.

**The breakthrough:** The researchers used OrchDAG to create a benchmark dataset and test the performance of various AI models. They found that the dataset is challenging but solvable, and that a new graph-based reward system can help improve the models' performance. This work highlights the importance of considering the underlying structure of complex tool interactions when training AI models.

**Why it matters:** This research has implications for developing more sophisticated AI systems that can handle complex tasks, such as those involved in robotics, automation, and decision-making. By improving the ability of AI models to interact with multiple tools in a coordinated way, we can create more efficient and effective systems that can tackle real-world challenges."
cs.AI,"Advancing site-specific disease and pest management in precision   agriculture: From reasoning-driven foundation models to adaptive,   feedback-based learning","Site-specific disease management (SSDM) in crops has advanced rapidly through machine and deep learning (ML and DL) for real-time computer vision. Research evolved from handcrafted feature extraction to large-scale automated feature learning. With foundation models (FMs), crop disease datasets are now processed in fundamentally new ways. Unlike traditional neural networks, FMs integrate visual and textual data, interpret symptoms in text, reason about symptom-management relationships, and support interactive QA for growers and educators. Adaptive and imitation learning in robotics further enables field-based disease management. This review screened approx. 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and discussing their role in adaptive learning (AL), reinforcement learning (RL), and digital twin frameworks for targeted spraying. Key findings: (a) FMs are gaining traction with surging literature in 2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL and AL are still nascent for smart spraying; (d) digital twins with RL can simulate targeted spraying virtually; (e) addressing the sim-to-real gap is critical for real-world deployment; (f) human-robot collaboration remains limited, especially in human-in-the-loop approaches where robots detect early symptoms and humans validate uncertain cases; (g) multi-modal FMs with real-time feedback will drive next-gen SSDM. For updates, resources, and contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to submit papers, code, or datasets.",http://arxiv.org/abs/2510.24650v1,2025-10-28T17:16:47Z,"Nitin Rai, Daeun, Choi, Nathan S. Boyd, Arnold W. Schumann","**Advancements in Precision Agriculture: Using AI to Improve Crop Disease Management**

Precision agriculture is becoming increasingly important for efficient and sustainable crop management. A recent review of research papers highlights the growing use of artificial intelligence (AI) and machine learning (ML) to improve site-specific disease management (SSDM) in crops. Specifically, the study focuses on ""foundation models"" (FMs), which are a type of AI that can process both visual and textual data.

**Key Findings:**

* FMs are being used to analyze crop disease datasets in new ways, enabling better interpretation of symptoms and more effective management strategies.
* Vision-language models (VLMs) are leading the way in FM applications, with a significant increase in publications in recent years.
* Researchers are exploring the use of reinforcement learning (RL) and adaptive learning (AL) to improve targeted spraying and reduce waste.
* Digital twins, which are virtual simulations of real-world systems, can be used to test and refine RL and AL strategies before deploying them in the field.

**Challenges and Future Directions:**

* One of the main challenges is the ""sim-to-real gap,"" which refers to the difficulty of translating virtual simulations to real-world applications.
* Human-robot collaboration is still limited, but could be improved through the use of multi-modal FMs that provide real-time feedback.
* Future research is needed to address these challenges and develop more effective AI-powered SSDM systems.

**Implications:**

* The use of AI and ML in precision agriculture has the potential to improve crop yields, reduce waste, and promote more sustainable farming practices.
* By leveraging FMs and other AI technologies, farmers and agricultural professionals can make more informed decisions and take more targeted approaches to disease management.

Overall, this research highlights the exciting advancements being made in precision agriculture and the potential for AI and ML to drive more efficient and effective crop management practices."
cs.AI,FunReason-MT Technical Report: Overcoming the Complexity Barrier in   Multi-Turn Function Calling,"Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.",http://arxiv.org/abs/2510.24645v1,2025-10-28T17:15:26Z,"Zengzhuang Xu, Bingguang Hao, Zechuan Wang, Yuntao Wen, Maolin Wang, Yang Liu, Long Chen, Dong Wang, Yicheng Chen, Cunyin Peng, Chenyi Zhuang, Jinjie Gu, Leilei Gan, Xiangyu Zhao, Shi Gu","Here's a summary of the research paper for a general audience:

**Improving AI's Ability to Use Tools and Solve Complex Problems**

Researchers have developed a new framework called FunReason-MT to help artificial intelligence (AI) systems learn to use external tools and solve complex problems. This is a crucial capability for AI systems, as it enables them to interact with the world and make decisions based on real-world data.

The challenge is that current methods for training AI systems to use tools are limited, making it difficult to generate high-quality data that reflects real-world scenarios. The researchers identified three main obstacles: 

1. Training AI models to target specific tasks
2. Isolating the architecture of tools used by AI systems
3. Managing the logical dependencies between multiple steps in a task

To overcome these challenges, FunReason-MT uses a novel approach that involves:

1. Creating a graph of interactions between the environment and APIs (application programming interfaces) to gather diverse and high-quality data
2. Simplifying the construction of complex queries to tools
3. Generating step-by-step reasoning chains to help AI systems learn to use tools effectively

The results are impressive: an AI model trained with FunReason-MT-generated data achieved state-of-the-art performance on a benchmark leaderboard, outperforming many other models. Further testing confirmed that FunReason-MT provides a reliable and robust source of data for AI learning. This breakthrough has the potential to enable AI systems to solve complex problems and interact with the world in more effective and efficient ways."
cs.AI,The Cost of Robustness: Tighter Bounds on Parameter Complexity for   Robust Memorization in ReLU Nets,"We study the parameter complexity of robust memorization for $\mathrm{ReLU}$ networks: the number of parameters required to interpolate any given dataset with $\epsilon$-separation between differently labeled points, while ensuring predictions remain consistent within a $\mu$-ball around each training sample. We establish upper and lower bounds on the parameter count as a function of the robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain tighter upper and lower bounds that improve upon existing results. Our findings reveal that the parameter complexity of robust memorization matches that of non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.",http://arxiv.org/abs/2510.24643v1,2025-10-28T17:09:43Z,"Yujun Kim, Chaewon Moon, Chulhee Yun","**The Cost of Robustness in Artificial Intelligence**

Imagine you're training a computer model to recognize pictures of cats and dogs. You want the model to not only learn from the pictures you've shown it, but also to be robust to small changes in the images, like a slight rotation or a tiny noise. This is known as ""robust memorization"".

Researchers have been trying to understand how complex a model needs to be to achieve this kind of robust memorization. In a new study, they've made significant progress in answering this question for a type of model called ReLU networks.

The researchers found that the number of parameters (or ""brain cells"") required by the model to robustly memorize a set of images depends on how much robustness is desired. They measured this robustness using a ratio of two values: how much the model should tolerate small changes around each image ($\mu$), and how far apart images with different labels should be ($\epsilon$).

Their key finding is that when the model needs to be very robust (i.e., tolerate large changes), it requires more parameters. However, when the model only needs to be slightly robust, the number of parameters required is similar to that of a non-robust model.

This study provides new and tighter bounds on the number of parameters required for robust memorization, which can help guide the design of more efficient and effective AI models. The results have implications for a wide range of applications, from image and speech recognition to autonomous vehicles and medical diagnosis."
cs.AI,Causal Ordering for Structure Learning From Time Series,"Predicting causal structure from time series data is crucial for understanding complex phenomena in physiology, brain connectivity, climate dynamics, and socio-economic behaviour. Causal discovery in time series is hindered by the combinatorial complexity of identifying true causal relationships, especially as the number of variables and time points grow. A common approach to simplify the task is the so-called ordering-based methods. Traditional ordering methods inherently limit the representational capacity of the resulting model. In this work, we fix this issue by leveraging multiple valid causal orderings, instead of a single one as standard practice. We propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based causal discovery for temporal data. By integrating multiple orderings, DOTS effectively recovers the transitive closure of the underlying directed acyclic graph, mitigating spurious artifacts inherent in single-ordering approaches. We formalise the problem under standard assumptions such as stationarity and the additive noise model, and leverage score matching with diffusion processes to enable efficient Hessian estimation. Extensive experiments validate the approach. Empirical evaluations on synthetic and real-world datasets demonstrate that DOTS outperforms state-of-the-art baselines, offering a scalable and robust approach to temporal causal discovery. On synthetic benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the best on individual datasets, DOTS attains the highest average summary-graph $F1$ while halving runtime relative to graph-optimisation methods. These results establish DOTS as a scalable and accurate solution for temporal causal discovery.",http://arxiv.org/abs/2510.24639v1,2025-10-28T17:06:15Z,"Pedro P. Sanchez, Damian Machlanski, Steven McDonagh, Sotirios A. Tsaftaris","**Unlocking Hidden Relationships in Time Series Data**

Understanding how different factors influence each other over time is crucial in various fields, such as physiology, climate dynamics, and socio-economic behavior. Researchers have developed methods to discover causal relationships from time series data, but the complexity of the data can make it challenging to identify true relationships.

A new approach, called DOTS (Diffusion Ordered Temporal Structure), aims to improve the accuracy of causal discovery in time series data. Traditional methods rely on a single ordering of variables, which can lead to incomplete or inaccurate representations of the underlying relationships. DOTS addresses this limitation by leveraging multiple valid causal orderings.

**Key Breakthroughs:**

* DOTS uses a diffusion-based approach to integrate multiple orderings, resulting in a more comprehensive and accurate representation of causal relationships.
* The method outperforms state-of-the-art baselines on synthetic and real-world datasets, demonstrating its scalability and robustness.
* DOTS achieves higher accuracy and faster runtime compared to existing methods, making it a valuable tool for temporal causal discovery.

**Implications:**

* DOTS has the potential to unlock new insights in various fields, such as understanding the complex interactions between physiological variables, identifying causal relationships in climate dynamics, and analyzing socio-economic behavior.
* The method's scalability and accuracy make it a promising solution for large-scale time series data analysis.

Overall, DOTS represents a significant advancement in causal discovery from time series data, offering a more accurate and efficient approach to understanding complex phenomena."
cs.AI,All in one timestep: Enhancing Sparsity and Energy efficiency in   Multi-level Spiking Neural Networks,"Spiking Neural Networks (SNNs) are one of the most promising bio-inspired neural networks models and have drawn increasing attention in recent years. The event-driven communication mechanism of SNNs allows for sparse and theoretically low-power operations on dedicated neuromorphic hardware. However, the binary nature of instantaneous spikes also leads to considerable information loss in SNNs, resulting in accuracy degradation. To address this issue, we propose a multi-level spiking neuron model able to provide both low-quantization error and minimal inference latency while approaching the performance of full precision Artificial Neural Networks (ANNs). Experimental results with popular network architectures and datasets, show that multi-level spiking neurons provide better information compression, allowing therefore a reduction in latency without performance loss. When compared to binary SNNs on image classification scenarios, multi-level SNNs indeed allow reducing by 2 to 3 times the energy consumption depending on the number of quantization intervals. On neuromorphic data, our approach allows us to drastically reduce the inference latency to 1 timestep, which corresponds to a compression factor of 10 compared to previously published results. At the architectural level, we propose a new residual architecture that we call Sparse-ResNet. Through a careful analysis of the spikes propagation in residual connections we highlight a spike avalanche effect, that affects most spiking residual architectures. Using our Sparse-ResNet architecture, we can provide state-of-the-art accuracy results in image classification while reducing by more than 20% the network activity compared to the previous spiking ResNets.",http://arxiv.org/abs/2510.24637v1,2025-10-28T17:03:33Z,"Andrea Castagnetti, Alain Pegatoquet, Benoît Miramond","**Breakthrough in Energy-Efficient Artificial Intelligence**

Researchers have made a significant advancement in developing more efficient and accurate artificial intelligence (AI) models inspired by the human brain. They propose a new type of neural network called multi-level Spiking Neural Networks (SNNs), which can process information more efficiently and accurately than traditional models.

**What are Spiking Neural Networks?**

SNNs mimic the way brain cells communicate with each other through electrical spikes. This allows them to operate with very low power consumption, making them suitable for use in mobile devices and other applications where energy efficiency is crucial.

**The Problem with Traditional SNNs**

Traditional SNNs use simple binary spikes, which can lead to information loss and reduced accuracy. To address this issue, the researchers developed a new multi-level SNN model that can represent more complex signals, reducing information loss and improving accuracy.

**Key Benefits**

The new multi-level SNN model offers several key benefits:

* **Improved Accuracy**: The model achieves similar performance to traditional AI models, but with much lower energy consumption.
* **Reduced Energy Consumption**: The model can reduce energy consumption by 2-3 times compared to traditional SNNs, making it more suitable for mobile devices and other applications where energy efficiency is crucial.
* **Faster Processing**: The model can process information in just one timestep, which is 10 times faster than previous models.

**New Architecture**

The researchers also proposed a new architecture called Sparse-ResNet, which can reduce network activity by over 20% while maintaining state-of-the-art accuracy in image classification tasks.

**Implications**

This breakthrough has significant implications for the development of more efficient and accurate AI models, enabling applications such as:

* **Edge AI**: AI models that can run on mobile devices and other edge devices, without relying on cloud computing.
* **Neuromorphic Computing**: AI models that mimic the human brain, enabling more efficient and adaptive processing of complex data.

Overall, this research has the potential to enable more efficient and accurate AI models, with significant implications for a wide range of applications."
cs.CL,MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25   Evaluation Shared Task,"In this paper, we present our submissions to the unified WMT25 Translation Evaluation Shared Task. For the Quality Score Prediction subtask, we create a new generation of MetricX with improvements in the input format and the training protocol, while for the Error Span Detection subtask we develop a new model, GemSpanEval, trained to predict error spans along with their severities and categories. Both systems are based on the state-of-the-art multilingual open-weights model Gemma 3, fine-tuned on publicly available WMT data. We demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture with a regression head on top, can be trained to effectively predict both MQM and ESA quality scores, and significantly outperforms its predecessor. Our decoder-only GemSpanEval model, on the other hand, we show to be competitive in error span detection with xCOMET, a strong encoder-only sequence-tagging baseline. With error span detection formulated as a generative task, we instruct the model to also output the context for each predicted error span, thus ensuring that error spans are identified unambiguously.",http://arxiv.org/abs/2510.24707v1,2025-10-28T17:56:20Z,"Juraj Juraska, Tobias Domhan, Mara Finkelstein, Tetsuji Nakagawa, Geza Kovacs, Daniel Deutsch, Pidong Wang, Markus Freitag","**Improving Machine Translation Evaluation**

Researchers at Google have made significant advancements in evaluating machine translation systems, like Google Translate. They developed two new tools to assess the quality of translations:

1. **MetricX-25**: A system that predicts the quality of translations, measuring how accurate and fluent they are. This tool has been improved to better understand the nuances of language and can accurately predict quality scores.
2. **GemSpanEval**: A model that identifies specific errors in translations, such as incorrect words or phrases, and categorizes their severity. This tool can also provide context for each error, making it easier to understand and correct.

Both tools are based on a state-of-the-art language model called Gemma 3 and were trained on publicly available data. The researchers demonstrated that their tools outperform previous systems, marking an important step forward in machine translation evaluation. These advancements can lead to even more accurate and reliable translations, benefiting users of machine translation systems worldwide."
cs.CL,ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality   Games?,"Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against annotated ground truth and human performance. Our results reveal that while top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition capabilities, they still struggle with procedural reasoning and spatial understanding compared to humans. Performance varies significantly across games, suggesting sensitivity to interaction complexity. Few-shot examples substantially improve performance, indicating potential for targeted enhancement of LLMs' VR manipulation capabilities. We release all materials at https://sites.google.com/view/combobench.",http://arxiv.org/abs/2510.24706v1,2025-10-28T17:55:42Z,"Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu","**Can AI Models Play Virtual Reality Games Like Humans?**

Imagine playing a virtual reality (VR) game where you need to use a controller to interact with virtual objects. Humans can easily do this, but can artificial intelligence (AI) models, specifically Large Language Models (LLMs), perform just as well? A recent study introduced a benchmark called ComboBench to test LLMs' ability to translate simple actions into precise movements to play VR games.

The researchers evaluated seven LLMs, including popular models like GPT-3.5 and GPT-4, on 262 scenarios from four popular VR games. They found that while top-performing LLMs showed strong abilities in breaking down tasks, they still struggled with understanding procedures and spatial awareness compared to humans. The performance of LLMs varied greatly across games, suggesting that they are sensitive to the complexity of interactions.

However, the study also found that providing LLMs with a few examples of correct actions significantly improved their performance. This suggests that targeted training can enhance LLMs' ability to interact with VR games.

The study's findings have implications for the development of more advanced AI models that can interact with virtual environments. The researchers have made all their materials publicly available, which can help advance research in this area."
cs.CL,"Agent Data Protocol: Unifying Datasets for Diverse, Effective   Fine-tuning of LLM Agents","Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an ""interlingua"" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.",http://arxiv.org/abs/2510.24702v1,2025-10-28T17:53:13Z,"Yueqi Song, Ketan Ramaneti, Zaid Sheikh, Ziru Chen, Boyu Gou, Tianbao Xie, Yiheng Xu, Danyang Zhang, Apurva Gandhi, Fan Yang, Joseph Liu, Tianyue Ou, Zhihao Yuan, Frank Xu, Shuyan Zhou, Xingyao Wang, Xiang Yue, Tao Yu, Huan Sun, Yu Su, Graham Neubig","Here's a summary of the research paper for a general audience:

**Making AI Training Easier and More Effective**

Training artificial intelligence (AI) agents to perform complex tasks is a challenging task. One of the main hurdles is collecting and organizing the data needed to train these agents. Currently, data is scattered across different formats, tools, and interfaces, making it difficult to use.

To address this issue, researchers have introduced a new protocol called the Agent Data Protocol (ADP). ADP is a simple and flexible language that allows different types of data to be unified and used to train AI agents. This protocol can handle a wide range of tasks, such as using software tools, browsing the internet, coding, and more.

In experiments, researchers used ADP to combine 13 existing datasets and train AI agents using this unified data. The results showed that the AI agents performed about 20% better than previous models, and achieved state-of-the-art or near-top performance on various benchmarks.

The good news is that the researchers have made all their code and data publicly available. This means that other researchers and developers can use ADP to train their own AI agents more easily and effectively. By standardizing the way AI agents are trained, ADP has the potential to lower the barrier to creating more advanced and capable AI systems."
cs.CL,Tongyi DeepResearch Technical Report,"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",http://arxiv.org/abs/2510.24701v1,2025-10-28T17:53:02Z,"Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang","Here's a summary of the research paper for a general audience:

**Introducing Tongyi DeepResearch: A Powerful AI Model for In-Depth Research**

Imagine having a super-smart research assistant that can dig deep into complex topics, find relevant information, and provide insightful answers. That's what Tongyi DeepResearch is - a cutting-edge AI model designed to perform in-depth research tasks.

**What makes Tongyi DeepResearch special?**

* It's a large language model with 30.5 billion parameters, making it capable of understanding and processing vast amounts of information.
* It was trained using a unique framework that allows it to learn autonomously, without relying on human annotation.
* It can perform complex tasks, such as seeking information, reasoning, and problem-solving, across various domains.

**What can Tongyi DeepResearch do?**

* Achieve state-of-the-art performance on a range of research benchmarks, including exams, question-answering tasks, and information-seeking challenges.
* Provide accurate and insightful answers to complex research questions.

**What's the impact?**

* The creators of Tongyi DeepResearch are open-sourcing the model, framework, and solutions, making it accessible to the wider research community.
* This can empower researchers, scientists, and developers to build upon this technology and advance the field of AI research.

Overall, Tongyi DeepResearch represents a significant breakthrough in AI research, with the potential to revolutionize the way we conduct in-depth research and seek information."
cs.CL,ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking,"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.",http://arxiv.org/abs/2510.24698v1,2025-10-28T17:51:50Z,"Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang","Here's a summary of the research paper ""ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking"" for a general audience:

**Improving Information Search with Parallel Thinking**

Imagine you're trying to solve a complex problem, like finding the best solution to a puzzle. You might try searching for information online, but it's easy to get stuck in a narrow line of thinking. Researchers have proposed a new approach called ""parallel thinking"" that can help explore multiple ideas at the same time, leading to better problem-solving.

However, traditional parallel thinking methods have limitations. They can be inefficient and struggle to combine different lines of thought into a coherent answer. To address these challenges, a team of researchers has developed a new system called ParallelMuse.

**How ParallelMuse Works**

ParallelMuse uses a two-stage approach to improve information search. First, it breaks down the search process into smaller, more manageable parts, and reuses and builds upon previous explorations to reduce waste and improve efficiency. Second, it compresses the information gathered into a more digestible form, making it easier to synthesize a final answer.

**The Results**

The researchers tested ParallelMuse on several open-source information search agents and benchmarks, and the results were impressive. ParallelMuse improved performance by up to 62% while reducing the number of ""exploratory tokens"" (or searches) needed by 10-30%. This means that ParallelMuse can help information search agents find better solutions to complex problems more efficiently."
cs.CL,AgentFold: Long-Horizon Web Agents with Proactive Context Management,"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.",http://arxiv.org/abs/2510.24699v1,2025-10-28T17:51:50Z,"Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, Yong Jiang","**Breakthrough in Web Agents: AgentFold Revolutionizes Long-Horizon Tasks**

Imagine having a personal assistant that can help you navigate complex tasks on the web, like booking a trip or researching a topic. Researchers have made a significant step towards creating such assistants, called web agents, which are powered by large language models (LLMs). However, these agents struggle with long-term tasks because they have to manage a lot of information, or ""context,"" over time.

The problem is that current agents either get overwhelmed by too much information or lose important details. To address this, a team of researchers introduced AgentFold, a new type of web agent that proactively manages its context, inspired by how humans process information. AgentFold uses a unique ""folding"" operation to condense or abstract away information at different scales, preserving important details while avoiding information overload.

The results are impressive: AgentFold outperformed much larger and more complex models, including proprietary ones from leading companies like OpenAI. With simple fine-tuning, AgentFold achieved state-of-the-art performance on two benchmark tasks, demonstrating its potential to revolutionize long-horizon web tasks. This innovation brings us closer to having reliable and efficient web assistants that can help us with complex tasks."
cs.CL,WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling   Info-Rich Seeking,"Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.",http://arxiv.org/abs/2510.24697v1,2025-10-28T17:51:42Z,"Zhengwei Tao, Haiyang Shen, Baixuan Li, Wenbiao Yin, Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, Xinyu Wang, Pengjun Xie, Jingren Zhou, Yong Jiang","**Improving Web Search Efficiency with WebLeaper**

Imagine having a super-smart assistant that can search the web for you and find exactly what you need. This is made possible by Large Language Model (LLM)-based agents, which are designed to solve complex problems on their own. However, these agents often struggle with finding information efficiently, which can limit their overall performance.

A team of researchers has developed a new framework called WebLeaper, which aims to improve the efficiency and effectiveness of these agents when searching for information on the web. WebLeaper works by creating a more comprehensive set of training tasks that help agents learn to search more efficiently. It does this by:

1. **Formulating search as a tree-structured reasoning problem**: This allows agents to explore a larger set of possible solutions within a limited context.
2. **Using curated Wikipedia tables**: The researchers used these tables to create three different methods for generating search tasks, which helped to increase both search efficiency and accuracy.
3. **Optimizing training data**: The team carefully selected training data that was both accurate and efficient, ensuring that the model learned to balance correctness with search performance.

The results were impressive: WebLeaper consistently outperformed existing methods in both effectiveness and efficiency across five different benchmarks. This breakthrough has the potential to enable more efficient and accurate web search capabilities, making it easier for users to find what they need online."
cs.CL,AgentFrontier: Expanding the Capability Frontier of LLM Agents with   ZPD-Guided Data Synthesis,"Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.",http://arxiv.org/abs/2510.24695v1,2025-10-28T17:50:47Z,"Xuanzhong Chen, Zile Qiao, Guoxin Chen, Liangcai Su, Zhen Zhang, Xinyu Wang, Pengjun Xie, Fei Huang, Jingren Zhou, Yong Jiang","**Unlocking the Potential of Large Language Models**

Researchers have made a breakthrough in developing more advanced language models, like those used in chatbots and virtual assistants. They've created a new approach called AgentFrontier, which helps train these models on complex tasks that are just beyond their current capabilities.

The idea is based on the educational concept of the ""Zone of Proximal Development"" (ZPD), which suggests that people (and machines) learn best when challenged with tasks that are slightly too difficult, but can be mastered with guidance. The researchers developed a tool called the AgentFrontier Engine, which automatically generates high-quality training data that falls within this ZPD.

Using this engine, they trained a large language model, called AgentFrontier-30B-A3B, on a wide range of complex tasks. The results were impressive: the model achieved state-of-the-art results on challenging benchmarks, even outperforming some of the best proprietary models.

This work shows that a ZPD-guided approach to training data can be a powerful way to build more capable language models. This could lead to significant improvements in areas like natural language processing, reasoning, and decision-making, with potential applications in fields like customer service, education, and healthcare."
cs.CL,Repurposing Synthetic Data for Fine-grained Search Agent Supervision,"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative ""near-miss"" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these ""near-misses"". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.",http://arxiv.org/abs/2510.24694v1,2025-10-28T17:50:40Z,"Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang","**Improving AI-Powered Search Agents with a New Training Method**

Researchers have developed a new approach to train AI search agents, which are used to answer complex questions and find information. These agents are typically trained on artificial data that mimics real-world information, but current training methods don't make the most of this data. A new method, called Entity-aware Group Relative Policy Optimization (E-GRPO), uses the detailed information in the artificial data to improve the agents' performance.

The researchers found that when AI agents correctly identify more entities (e.g., people, places, organizations) related to a question, they are more likely to provide accurate answers. E-GRPO uses this insight to give agents partial credit for correct entities, even if their final answer is incorrect. This approach allows agents to learn from ""near-miss"" responses, which are answers that are close but not quite correct.

In experiments, E-GRPO outperformed existing training methods, achieving higher accuracy and more efficient reasoning. This new approach has the potential to improve AI-powered search agents, making them more effective and efficient in finding information and answering complex questions."
cs.CL,STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D   Intelligence,"Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.",http://arxiv.org/abs/2510.24693v1,2025-10-28T17:50:34Z,"Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang","**Unlocking the Secrets of Sound: Introducing STAR-Bench**

Imagine being able to hear a complex sound, like a symphony, and understanding not just what instruments are playing, but also where they are in space, how they move over time, and how they interact with each other. This is the concept of ""audio 4D intelligence,"" which involves reasoning about sound dynamics in time and 3D space.

Researchers have created a new benchmark, called STAR-Bench, to test the ability of artificial intelligence (AI) models to understand sound in this way. STAR-Bench evaluates two key aspects: 

1. **Foundational Acoustic Perception**: This involves recognizing basic audio attributes, such as pitch, volume, and location. 
2. **Holistic Spatio-Temporal Reasoning**: This involves understanding more complex aspects of sound, such as how different sounds interact with each other in space and time.

The results are surprising: even the best AI models struggle with STAR-Bench, showing significant gaps in their ability to understand sound compared to humans. For example, when tested on tasks that require understanding the timing and location of sounds, AI models performed 31.5% and 35.2% worse, respectively, than they did on tasks that only required text-based answers.

The researchers tested 19 AI models and found that closed-source models (proprietary models not publicly available) are limited by their ability to perceive fine-grained details in sound, while open-source models (publicly available models) lag behind in multiple areas, including perception, knowledge, and reasoning.

The STAR-Bench benchmark provides a new way to evaluate and improve AI models' understanding of sound and the physical world. By highlighting the strengths and weaknesses of current models, STAR-Bench paves the way for the development of more advanced AI systems that can better understand and interact with the world around us."
cs.CL,SPICE: Self-Play In Corpus Environments Improves Reasoning,"Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.",http://arxiv.org/abs/2510.24684v1,2025-10-28T17:46:16Z,"Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, Jason Weston","**Improving Reasoning with Self-Play in Large Datasets**

Researchers have developed a new framework called SPICE (Self-Play In Corpus Environments) that enables artificial intelligence (AI) systems to improve their reasoning abilities through self-play. In SPICE, a single AI model plays two roles: a ""Challenger"" that generates diverse reasoning tasks by mining documents from a large dataset, and a ""Reasoner"" that solves these tasks.

The key innovation of SPICE is that it uses a large corpus of text (such as books, articles, and websites) to create a continuous stream of challenging tasks for the Reasoner to solve. This approach, called ""corpus grounding,"" allows the AI system to learn and improve in a sustained way, unlike previous self-play methods that can become stagnant.

The researchers tested SPICE on various AI models and found that it consistently improved their reasoning abilities, with gains of up to 8.9% in mathematical reasoning and 9.8% in general reasoning. The study suggests that SPICE has the potential to enable AI systems to continuously learn and improve their reasoning abilities, leading to more intelligent and capable machines."
cs.CL,Dissecting Role Cognition in Medical LLMs via Neuronal Ablation,"Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the   context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP),   instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied   professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This   study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct,   role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA   datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our   results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they   primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation   across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform   across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical   practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine   cognitive processes rather than linguistic imitation.We have released the related code in the following repository:https:   //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",http://arxiv.org/abs/2510.24677v1,2025-10-28T17:40:53Z,"Xun Liang, Huayi Lai, Hanyu Wang, Wentao Zhang, Linfeng Zhang, Yanfang Chen, Feiyu Xiong, Zhiyu Li","Here's a summary of the research paper for a general audience:

**The Limitations of Role-Playing in Medical AI**

Imagine you're chatting with a computer program that's supposed to act like a doctor. You ask it a question about a medical condition, and it responds in a way that's supposed to sound like a doctor. But is it really thinking like a doctor, or is it just mimicking the way a doctor talks?

Researchers investigated this question by testing large language models (LLMs) that are designed to simulate different roles, such as a medical student or an experienced doctor. They found that these models don't actually change their reasoning processes or thinking patterns when they're told to adopt a different role. Instead, they just change the way they phrase their answers to sound more like the role they're playing.

The researchers used a technique called ""neuron ablation"" to analyze how the models process information and make decisions. They found that the models' decision-making mechanisms remain the same, regardless of the role they're playing. This means that current role-playing methods in medical AI are not effective in replicating the complex thinking processes that real doctors use.

The study's findings highlight the limitations of role-playing in medical AI and the need for more advanced models that can simulate genuine cognitive processes, rather than just imitating the way humans talk. This has important implications for the development of AI systems that are designed to support medical decision-making."
cs.CL,InteractComp: Evaluating Search Agents With Ambiguous Queries,"Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.",http://arxiv.org/abs/2510.24668v1,2025-10-28T17:35:54Z,"Mingyi Deng, Lijun Huang, Yani Fan, Jiayi Zhang, Fashen Ren, Jinyi Bai, Fuzhen Yang, Dayi Miao, Zhaoyang Yu, Yifan Wu, Yanfei Zhang, Fengwei Teng, Yingjia Wan, Song Hu, Yude Li, Xin Jin, Conghao Hu, Haoyu Li, Qirui Fu, Tai Zhong, Xinyu Wang, Xiangru Tang, Nan Tang, Chenglin Wu, Yuyu Luo","**Improving Search Agents: A New Benchmark for Interactive Search**

Imagine asking a search engine a question, but it doesn't quite understand what you mean. You'd want to be able to clarify your question, right? Most search agents, like virtual assistants or chatbots, assume users ask clear and complete questions. However, in reality, users often start with vague or unclear queries that need clarification.

To address this issue, researchers have created a new benchmark called InteractComp. This benchmark evaluates whether search agents can recognize when a user's query is unclear and interact with the user to clarify it. The researchers tested 17 search models and found that they performed poorly, with the best model achieving only 13.73% accuracy. However, when forced to interact with users, the models showed significant improvement.

The study reveals a surprising finding: while search agents have improved dramatically over time, their ability to interact with users has not. In fact, interaction capabilities have stagnated over the past 15 months. This highlights a critical blind spot in current search agents.

The InteractComp benchmark provides a valuable resource for evaluating and training search agents to interact with users more effectively. By developing this capability, search agents can better understand user queries and provide more accurate results. The code for InteractComp is now available, allowing researchers to build upon this work and create more interactive and effective search agents."
cs.CL,MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine   Translation,"Human evaluation of machine translation is in an arms race with translation model quality: as our models get better, our evaluation methods need to be improved to ensure that quality gains are not lost in evaluation noise. To this end, we experiment with a two-stage version of the current state-of-the-art translation evaluation paradigm (MQM), which we call MQM re-annotation. In this setup, an MQM annotator reviews and edits a set of pre-existing MQM annotations, that may have come from themselves, another human annotator, or an automatic MQM annotation system. We demonstrate that rater behavior in re-annotation aligns with our goals, and that re-annotation results in higher-quality annotations, mostly due to finding errors that were missed during the first pass.",http://arxiv.org/abs/2510.24664v1,2025-10-28T17:29:59Z,"Parker Riley, Daniel Deutsch, Mara Finkelstein, Colten DiIanni, Juraj Juraska, Markus Freitag","Here's a summary of the research paper for a general audience:

**Improving the Evaluation of Machine Translation**

As machines get better at translating languages, we need to make sure we're evaluating their performance accurately. Researchers have developed a method called MQM to assess the quality of machine translations, but it's not perfect. To improve it, they've introduced a new technique called MQM re-annotation.

In MQM re-annotation, a human evaluator reviews and edits existing annotations (or ratings) of machine translations. This process helps to catch errors that might have been missed the first time around. The study found that this two-stage approach leads to higher-quality evaluations, mainly because it reveals mistakes that were overlooked initially.

By refining the evaluation process, researchers can get a more accurate picture of how well machine translation systems are performing. This, in turn, can help to drive further improvements in machine translation technology."
cs.CL,Evolving Diagnostic Agents in a Virtual Clinical Environment,"In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone.",http://arxiv.org/abs/2510.24654v1,2025-10-28T17:19:47Z,"Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie","**Breakthrough in AI-Powered Medical Diagnosis**

Imagine a future where artificial intelligence (AI) can help doctors diagnose patients more accurately and efficiently. A recent research paper presents a significant step towards making this a reality. The authors have developed a new framework that trains large language models (LLMs) to act as diagnostic agents, mimicking the process of a doctor diagnosing a patient.

The researchers created a virtual clinical environment, called DiagGym, which simulates real-world patient cases and allows the AI to learn from interactive exploration and feedback. They trained their AI model, called DiagAgent, using reinforcement learning, which enables it to adaptively select examinations, manage multi-turn diagnostic processes, and commit to final diagnoses.

The results are impressive: DiagAgent outperformed 10 state-of-the-art LLMs, including popular models like GPT-4o, in various diagnostic settings. It achieved significant improvements in diagnostic accuracy (up to 15.12%) and examination recommendation hit ratio (up to 44.03%). These findings suggest that learning policies in interactive clinical environments can confer dynamic and clinically meaningful diagnostic management abilities that are not possible with traditional training methods.

This research has the potential to revolutionize medical diagnosis by providing doctors with AI-powered tools that can help them make more accurate diagnoses and develop more effective treatment plans. The authors' work paves the way for further development of AI-assisted diagnostic systems that can improve patient outcomes and healthcare efficiency."
cs.CL,Optimizing Retrieval for RAG via Reinforced Contrastive Learning,"As retrieval-augmented generation (RAG) becomes increasingly widespread, the role of information retrieval (IR) is shifting from retrieving information for human users to retrieving contextual knowledge for artificial intelligence (AI) systems, where relevance becomes difficult to define or annotate beforehand. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through trialand-feedback Reinforced contrastive learning. Unlike prior approaches that rely on annotated or synthetic data for supervised fine-tuning, R3 enables the retriever to dynamically explore and optimize relevance within the RAG environment. During training, the retrieved results interact with the environment to produce contrastive signals that automatically guide the retriever's self-improvement. Extensive experiments across diverse tasks demonstrate that R3 improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day.",http://arxiv.org/abs/2510.24652v1,2025-10-28T17:18:30Z,"Jiawei Zhou, Lei Chen","Here's a summary of the research paper for a general audience:

**Improving AI's Ability to Find Relevant Information**

Imagine you're asking a virtual assistant a question, and it needs to find relevant information from a huge database to answer you accurately. This is a challenging task, especially when the assistant needs to learn what ""relevant"" information means on its own. Researchers have proposed a new framework called R3 to help AI systems improve their ability to retrieve relevant information.

**The Problem: Defining Relevance**

Traditional methods for training AI retrievers rely on human-annotated data, which can be time-consuming and expensive to create. Moreover, defining what makes information ""relevant"" can be tricky, especially when the context is complex.

**The Solution: R3**

R3 uses a technique called reinforced contrastive learning to help the AI retriever learn what makes information relevant. During training, the retriever interacts with the environment, and the results are used to automatically guide its self-improvement. This approach allows the retriever to dynamically explore and optimize relevance within the AI system.

**The Results: Significant Improvements**

The researchers tested R3 on various tasks and found that it improves the performance of retrieval-augmented generation (RAG) systems by 5.2% compared to the original retriever. It also outperforms state-of-the-art retrievers by 4.9% and achieves comparable results to more complex systems that use large language models.

**The Benefits: Efficiency and Practicality**

The best part? R3 is efficient and practical. It only requires 4 GPUs and can complete training within a single day, making it a promising solution for improving AI's ability to find relevant information."
cs.CL,"Quantifying the Effects of Word Length, Frequency, and Predictability on   Dyslexia","We ask where, and under what conditions, dyslexic reading costs arise in a large-scale naturalistic reading dataset. Using eye-tracking aligned to word-level features (word length, frequency, and predictability), we model how each feature influences dyslexic time costs. We find that all three features robustly change reading times in both typical and dyslexic readers, and that dyslexic readers show stronger sensitivities to each, especially predictability. Counterfactual manipulations of these features substantially narrow the dyslexic-control gap by about one third, with predictability showing the strongest effect, followed by length and frequency. These patterns align with dyslexia theories that posit heightened demands on linguistic working memory and phonological encoding, and they motivate further work on lexical complexity and parafoveal preview benefits to explain the remaining gap. In short, we quantify when extra dyslexic costs arise, how large they are, and offer actionable guidance for interventions and computational models for dyslexics.",http://arxiv.org/abs/2510.24647v1,2025-10-28T17:15:31Z,"Hugo Rydel-Johnston, Alex Kafkas","**Understanding Dyslexia: New Insights into Reading Challenges**

Dyslexia, a learning disorder that affects reading abilities, has long been a subject of research. A recent study aimed to better understand the specific challenges faced by individuals with dyslexia when reading. By analyzing eye-tracking data from a large group of readers, researchers investigated how three key factors - word length, frequency, and predictability - impact reading times.

The study found that individuals with dyslexia are more sensitive to these factors than typical readers. For example, when encountering longer or less common words, or words that are harder to predict from context, individuals with dyslexia take more time to read. Notably, the study revealed that predictability has the greatest impact on reading times for individuals with dyslexia.

The good news is that the study suggests that targeted interventions could help alleviate some of these reading challenges. By manipulating these factors, such as providing more context to help predict words or using simpler vocabulary, researchers estimate that the reading gap between individuals with dyslexia and typical readers could be reduced by about one third.

These findings have important implications for the development of more effective reading interventions and computational models to support individuals with dyslexia. By understanding the specific challenges faced by individuals with dyslexia, researchers and educators can work together to create more tailored and supportive learning environments."
cs.CL,OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning,"Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the model's internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation.",http://arxiv.org/abs/2510.24636v2,2025-10-28T17:02:46Z,"Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren","Here's a summary of the research paper for a general audience:

**Improving AI Evaluation: A New Approach to Assessing Long-Form Tasks**

Large language models (LLMs) are AI systems that can process and generate human-like text. To ensure these models produce accurate and helpful responses, researchers use ""reward models"" to evaluate their performance. However, existing reward models struggle with complex, long-form tasks that require external knowledge and evidence to assess correctness.

To address this limitation, researchers have developed a new tool called OpenReward. OpenReward uses external tools to gather relevant evidence and evaluate open-ended responses. This approach enables the model to make more accurate judgments, especially when external evidence is necessary.

The researchers trained OpenReward using a novel method called Group Relative Policy Optimization (GRPO) on a large dataset of synthesized examples. They found that OpenReward significantly outperforms existing reward modeling approaches on several datasets.

The study also demonstrated that integrating OpenReward into AI systems can lead to improved performance on downstream tasks. This suggests that tool-augmented reward models like OpenReward have the potential to scale reliable evaluation of long-form tasks, enabling more accurate and helpful AI responses.

**In simple terms:** Researchers have created a new AI tool called OpenReward that helps evaluate the performance of large language models on complex tasks. OpenReward uses external evidence to make more accurate judgments, leading to improved AI performance and more reliable evaluation."
cs.CL,"""Mm, Wat?"" Detecting Other-initiated Repair Requests in Dialogue","Maintaining mutual understanding is a key component in human-human conversation to avoid conversation breakdowns, in which repair, particularly Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the other to resolve), plays a vital role. However, Conversational Agents (CAs) still fail to recognize user repair initiation, leading to breakdowns or disengagement. This work proposes a multimodal model to automatically detect repair initiation in Dutch dialogues by integrating linguistic and prosodic features grounded in Conversation Analysis. The results show that prosodic cues complement linguistic features and significantly improve the results of pretrained text and audio embeddings, offering insights into how different features interact. Future directions include incorporating visual cues, exploring multilingual and cross-context corpora to assess the robustness and generalizability.",http://arxiv.org/abs/2510.24628v1,2025-10-28T16:58:26Z,"Anh Ngo, Nicolas Rollet, Catherine Pelachaud, Chloe Clavel","**Improving Conversational AI: Detecting Repair Requests**

Have you ever been in a conversation where you realized you didn't quite understand what the other person was saying? Or maybe you said something that was misinterpreted? To avoid these kinds of breakdowns, humans use a process called ""repair"" to clarify and correct each other. But current conversational AI systems, like chatbots and virtual assistants, often struggle to recognize when a user is trying to initiate repair.

Researchers have developed a new model that can detect when a user is trying to signal that they didn't understand something or need clarification. This model uses a combination of language and sound cues, like tone of voice and pitch, to identify ""repair requests"" in conversations. The results show that by incorporating both language and sound features, the model can more accurately detect repair requests.

This research has important implications for improving conversational AI systems, making them more effective and user-friendly. The next steps include exploring the use of visual cues, like facial expressions and body language, and testing the model in different languages and contexts to ensure it works universally. By advancing our understanding of how humans communicate and repair misunderstandings, we can create more sophisticated and helpful conversational AI systems."
cs.CL,Relative Scaling Laws for LLMs,"Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introduce relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson.",http://arxiv.org/abs/2510.24626v1,2025-10-28T16:55:22Z,"William Held, David Hall, Percy Liang, Diyi Yang","**The Power and Limitations of Scaling Up Language Models**

Imagine being able to predict how well a computer language model will perform as it gets bigger and smarter. Researchers have developed ""scaling laws"" to help forecast this, but these laws have limitations. They typically measure performance on a broad test set, which can mask differences in how well the model performs on specific groups or tasks.

A new study introduces ""relative scaling laws,"" which track how performance gaps between different test groups change as the model gets bigger. The researchers trained 255 language models with varying sizes and computational power, and tested them on a range of tasks.

The study found that as language models get bigger, they don't always improve equally across all tasks and groups. For example:

* Academic domains, such as science and history, tend to become more accurate and equal in performance.
* Regional dialects of English may improve, but the amount of improvement depends on the size of the population that speaks that dialect.
* Certain types of risks, such as capability and influence-related risks, may increase as the model gets bigger, while others, like adversarial risks, do not.

These findings suggest that simply scaling up a language model is not enough to ensure it performs well across all tasks and groups. The researchers have made their models and results publicly available, so that others can study and build on their work. This can help prioritize efforts to improve the robustness and fairness of language models."
stat.ML,Machine-Learning-Assisted Comparison of Regression Functions,"We revisit the classical problem of comparing regression functions, a fundamental question in statistical inference with broad relevance to modern applications such as data integration, transfer learning, and causal inference. Existing approaches typically rely on smoothing techniques and are thus hindered by the curse of dimensionality. We propose a generalized notion of kernel-based conditional mean dependence that provides a new characterization of the null hypothesis of equal regression functions. Building on this reformulation, we develop two novel tests that leverage modern machine learning methods for flexible estimation. We establish the asymptotic properties of the test statistics, which hold under both fixed- and high-dimensional regimes. Unlike existing methods that often require restrictive distributional assumptions, our framework only imposes mild moment conditions. The efficacy of the proposed tests is demonstrated through extensive numerical studies.",http://arxiv.org/abs/2510.24714v1,2025-10-28T17:59:15Z,"Jian Yan, Zhuoxi Li, Yang Ning, Yong Chen","**Comparing Regression Functions Made Easier with Machine Learning**

Imagine you're trying to understand how different factors affect an outcome, like how much rainfall affects crop yields in different regions. A key question is whether the relationship between these factors is the same across different groups. This is known as comparing regression functions, a fundamental problem in statistics.

Traditional methods for comparing regression functions have limitations, especially when dealing with many variables (high dimensionality). They often rely on smoothing techniques, which can be inaccurate in such cases.

In this study, researchers propose a new approach that uses machine learning to compare regression functions more effectively. They develop two novel tests that can handle high-dimensional data and don't require strict assumptions about the data distribution. The tests are flexible, reliable, and work well even when there are many variables involved.

The researchers tested their approach through simulations and found it to be effective. Their work has implications for various applications, including data integration, transfer learning, and causal inference, making it a valuable contribution to the field of statistics and machine learning."
stat.ML,A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel   Optimization,"We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.",http://arxiv.org/abs/2510.24710v1,2025-10-28T17:58:17Z,"Wei Shen, Jiawei Zhang, Minhui Huang, Cong Shen","**Breakthrough in Optimizing Complex Systems**

Imagine you're trying to optimize a complex system with two interconnected parts, where one part has its own constraints. This is known as a bilevel optimization problem. Researchers have made a significant advancement in solving such problems, particularly when the lower-level part is strongly convex and has linear constraints.

The team developed a new algorithm, called SFLCB, which simplifies the problem by transforming it into a single-level one using penalty and augmented Lagrangian methods. This approach overcomes the challenges of non-smoothness and computational complexity associated with traditional methods.

The SFLCB algorithm is efficient, single-loop, and first-order, meaning it requires less computational power and is faster than previous double-loop algorithms. The researchers proved that SFLCB converges quickly, with a rate of $O(\epsilon^{-3})$, which is an improvement over previous algorithms.

The study's findings were validated through experiments, which demonstrated the practical efficiency of the SFLCB algorithm. The simulation code is openly available, allowing others to build upon this research.

**In simpler terms:** This research presents a new, efficient algorithm for optimizing complex systems with interconnected parts. The algorithm is faster and more powerful than previous ones, and its effectiveness was confirmed through experiments. This breakthrough has the potential to improve various applications, such as machine learning, economics, and engineering."
stat.ML,Greedy Sampling Is Provably Efficient for RLHF,"Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post-training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use the empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.",http://arxiv.org/abs/2510.24700v1,2025-10-28T17:52:08Z,"Di Wu, Chengshuai Shi, Jing Yang, Cong Shen","**Improving AI Training with a Simple yet Powerful Method**

Researchers have made a breakthrough in training large language models, a crucial component of many AI systems. The technique, called Reinforcement Learning from Human Feedback (RLHF), helps fine-tune these models to better align with human preferences. Despite its success in practice, the theoretical foundations of RLHF were not well understood - until now.

The study focuses on a key challenge in RLHF: how to efficiently learn from human feedback, which is often provided in the form of preferences (e.g., ""this response is better than that one""). The researchers developed a new approach that uses a surprisingly simple method called ""greedy sampling."" This method involves choosing the next action based on the current best estimate, without trying to be overly optimistic or pessimistic.

The exciting finding is that this straightforward approach is not only effective but also provably efficient, outperforming existing methods by a significant margin. The researchers showed that greedy sampling can achieve better results with less data, making it a promising technique for training large language models.

This breakthrough has important implications for the development of more accurate and helpful AI systems. By providing a solid theoretical foundation for RLHF, the study paves the way for further improvements in AI training and applications."
stat.ML,Eigenfunction Extraction for Ordered Representation Learning,"Recent advances in representation learning reveal that widely used objectives, such as contrastive and non-contrastive, implicitly perform spectral decomposition of a contextual kernel, induced by the relationship between inputs and their contexts. Yet, these methods recover only the linear span of top eigenfunctions of the kernel, whereas exact spectral decomposition is essential for understanding feature ordering and importance. In this work, we propose a general framework to extract ordered and identifiable eigenfunctions, based on modular building blocks designed to satisfy key desiderata, including compatibility with the contextual kernel and scalability to modern settings. We then show how two main methodological paradigms, low-rank approximation and Rayleigh quotient optimization, align with this framework for eigenfunction extraction. Finally, we validate our approach on synthetic kernels and demonstrate on real-world image datasets that the recovered eigenvalues act as effective importance scores for feature selection, enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional representations.",http://arxiv.org/abs/2510.24672v1,2025-10-28T17:37:12Z,"Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar","**Unlocking the Secrets of Data Representation**

Imagine you're trying to understand a complex picture. Your brain breaks it down into simpler features, like colors, shapes, and textures. Computers do the same thing with data, using a technique called representation learning. This helps them make sense of the data and make predictions.

But how do computers decide which features are most important? Researchers have made progress in understanding how popular methods work, but there's still a challenge: these methods only give a rough idea of the top features, without explaining their order or importance.

A new study proposes a framework to extract and order these features, called eigenfunctions, in a way that's compatible with the data and scalable to large datasets. The researchers show that two common mathematical approaches can be used to achieve this goal.

The study tested this approach on artificial data and real-world images. The results demonstrate that the extracted features can be used to identify the most important ones, allowing for efficient trade-offs between accuracy and computational resources. This breakthrough has the potential to improve how computers understand and process complex data."
stat.ML,Bridging Simulators with Conditional Optimal Transport,"We propose a new field-level emulator that bridges two simulators using unpaired simulation datasets. Our method leverages a flow-based approach to learn the likelihood transport from one simulator to the other. Since multiple transport maps exist, we employ Conditional Optimal Transport Flow Matching (COT-FM) to ensure that the transformation minimally distorts the underlying structure of the data. We demonstrate the effectiveness of this approach by bridging weak lensing simulators: a Lagrangian Perturbation Theory (LPT) to a N-body Particle-Mesh (PM). We demonstrate that our emulator captures the full correction between the simulators by showing that it enables full-field inference to accurately recover the true posterior, validating its accuracy beyond traditional summary statistics.",http://arxiv.org/abs/2510.24631v1,2025-10-28T16:59:42Z,"Justine Zeghal, Benjamin Remy, Yashar Hezaveh, Francois Lanusse, Laurence Perreault Levasseur","**Unlocking New Insights in Astrophysics with Advanced Simulation Techniques**

Imagine trying to compare two different computer simulations that model the same astrophysical phenomenon, but use different methods to do so. This can be challenging, especially when the simulations don't produce identical results. Researchers have now developed a new method, called Conditional Optimal Transport Flow Matching (COT-FM), to ""bridge"" these simulations and enable more accurate comparisons.

In a recent study, scientists applied COT-FM to two different simulations of weak lensing, a phenomenon in which the light from distant galaxies is bent by the gravitational pull of intervening matter. The simulations used different approaches: one was based on a mathematical theory (Lagrangian Perturbation Theory), while the other used a more detailed, particle-based simulation (N-body Particle-Mesh).

The researchers showed that their new method can effectively ""translate"" the output of one simulation into a format that matches the other, without distorting the underlying data. This allows for more accurate comparisons and inferences to be made between the two simulations. The study demonstrates the potential of COT-FM to improve our understanding of complex astrophysical phenomena, and could have far-reaching implications for fields such as cosmology and astrophysics."
stat.ML,Coreset for Robust Geometric Median: Eliminating Size Dependency on   Outliers,"We study the robust geometric median problem in Euclidean space $\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact summary of a dataset $P$ of size $n$ that approximates the robust cost for all centers $c$ within a multiplicative error $\varepsilon$. Given an outlier count $m$, we construct a coreset of size $\tilde{O}(\varepsilon^{-2} \cdot \min\{\varepsilon^{-2}, d\})$ when $n \geq 4m$, eliminating the $O(m)$ dependency present in prior work [Huang et al., 2022 & 2023]. For the special case of $d = 1$, we achieve an optimal coreset size of $\tilde{\Theta}(\varepsilon^{-1/2} + \frac{m}{n} \varepsilon^{-1})$, revealing a clear separation from the vanilla case studied in [Huang et al., 2023; Afshani and Chris, 2024]. Our results further extend to robust $(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence under mild data assumptions. The key technical contribution is a novel non-component-wise error analysis, enabling substantial reduction of outlier influence, unlike prior methods that retain them.Empirically, our algorithms consistently outperform existing baselines in terms of size-accuracy tradeoffs and runtime, even when data assumptions are violated across a wide range of datasets.",http://arxiv.org/abs/2510.24621v1,2025-10-28T16:49:03Z,"Ziyi Fang, Lingxiao Huang, Runkai Yang","**Breakthrough in Data Analysis: Robust Geometric Median with Coresets**

Imagine trying to find the middle point of a set of data points, but some of those points are outliers that can throw off your calculations. Researchers have developed a new method to solve this problem more efficiently and accurately.

The method, called coreset construction, creates a smaller, representative summary of the data that still captures its essential characteristics. This allows for faster and more reliable calculations, even when there are many outliers.

The researchers made significant improvements to existing methods, reducing the size of the coreset needed to achieve accurate results, especially when there are many outliers. In fact, their approach eliminates the dependency on the number of outliers, making it much more efficient.

The new method works not only for simple data sets but also for more complex ones, such as clustering data points into groups. The researchers tested their approach on various data sets and found that it consistently outperformed existing methods in terms of accuracy and speed.

This breakthrough has the potential to improve data analysis in many fields, from computer science to statistics and machine learning, by providing a more robust and efficient way to find the geometric median and perform clustering tasks."
stat.ML,Statistical physics of deep learning: Optimal learning of a multi-layer   perceptron near interpolation,"For three decades statistical physics has been providing a framework to analyse neural networks. A long-standing question remained on its capacity to tackle deep learning models capturing rich feature learning effects, thus going beyond the narrow networks or kernel methods analysed until now. We positively answer through the study of the supervised learning of a multi-layer perceptron. Importantly, (i) its width scales as the input dimension, making it more prone to feature learning than ultra wide networks, and more expressive than narrow ones or with fixed embedding layers; and (ii) we focus on the challenging interpolation regime where the number of trainable parameters and data are comparable, which forces the model to adapt to the task. We consider the matched teacher-student setting. It provides the fundamental limits of learning random deep neural network targets and helps in identifying the sufficient statistics describing what is learnt by an optimally trained network as the data budget increases. A rich phenomenology emerges with various learning transitions. With enough data optimal performance is attained through model's ""specialisation"" towards the target, but it can be hard to reach for training algorithms which get attracted by sub-optimal solutions predicted by the theory. Specialisation occurs inhomogeneously across layers, propagating from shallow towards deep ones, but also across neurons in each layer. Furthermore, deeper targets are harder to learn. Despite its simplicity, the Bayesian-optimal setting provides insights on how the depth, non-linearity and finite (proportional) width influence neural networks in the feature learning regime that are potentially relevant way beyond it.",http://arxiv.org/abs/2510.24616v2,2025-10-28T16:44:34Z,"Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk","**Unlocking the Secrets of Deep Learning: A Breakthrough in Statistical Physics**

For decades, researchers have been using statistical physics to understand how neural networks learn and make predictions. However, until now, this approach has been limited to simple neural networks. A new study has successfully applied statistical physics to deep learning models, which are a type of artificial intelligence that mimics the human brain.

The researchers studied a type of deep learning model called a multi-layer perceptron, which is like a layered network of interconnected nodes that process information. They found that as the model learns from data, it goes through different phases, similar to how a physical system changes state (e.g., from solid to liquid).

The study revealed that when the model has enough data, it can learn to specialize in the task at hand, but this can be challenging for training algorithms to achieve. The researchers also discovered that deeper layers of the network learn more slowly than shallower ones, and that some neurons learn faster than others.

This breakthrough provides new insights into how deep learning models work, and could lead to improvements in AI systems. The study's findings have implications for the design of neural networks, and could help researchers develop more efficient and effective training methods.

**Key Takeaways:**

* Statistical physics can be used to understand deep learning models
* Deep learning models go through different phases as they learn from data
* Models can learn to specialize in a task, but this can be challenging to achieve
* Deeper layers of the network learn more slowly than shallower ones

This research has the potential to advance our understanding of AI and improve the performance of deep learning models."
stat.ML,Comparison of generalised additive models and neural networks in   applications: A systematic review,"Neural networks have become a popular tool in predictive modelling, more commonly associated with machine learning and artificial intelligence than with statistics. Generalised Additive Models (GAMs) are flexible non-linear statistical models that retain interpretability. Both are state-of-the-art in their own right, with their respective advantages and disadvantages. This paper analyses how these two model classes have performed on real-world tabular data. Following PRISMA guidelines, we conducted a systematic review of papers that performed empirical comparisons of GAMs and neural networks. Eligible papers were identified, yielding 143 papers, with 430 datasets. Key attributes at both paper and dataset levels were extracted and reported. Beyond summarising comparisons, we analyse reported performance metrics using mixed-effects modelling to investigate potential characteristics that can explain and quantify observed differences, including application area, study year, sample size, number of predictors, and neural network complexity. Across datasets, no consistent evidence of superiority was found for either GAMs or neural networks when considering the most frequently reported metrics (RMSE, $R^2$, and AUC). Neural networks tended to outperform in larger datasets and in those with more predictors, but this advantage narrowed over time. Conversely, GAMs remained competitive, particularly in smaller data settings, while retaining interpretability. Reporting of dataset characteristics and neural network complexity was incomplete in much of the literature, limiting transparency and reproducibility. This review highlights that GAMs and neural networks should be viewed as complementary approaches rather than competitors. For many tabular applications, the performance trade-off is modest, and interpretability may favour GAMs.",http://arxiv.org/abs/2510.24601v1,2025-10-28T16:28:42Z,"Jessica Doohan, Lucas Kook, Kevin Burke","**The Battle Between Neural Networks and Statistical Models: A Systematic Review**

In the world of data analysis, two powerful tools have emerged: neural networks and Generalized Additive Models (GAMs). Neural networks are a type of machine learning algorithm inspired by the human brain, while GAMs are a type of statistical model that can handle complex relationships between variables. Both have their strengths and weaknesses, but which one is better?

A recent systematic review of 143 research papers and 430 datasets compared the performance of neural networks and GAMs on real-world data. The surprising result: neither tool consistently outperformed the other. When looking at common metrics such as accuracy, precision, and recall, both neural networks and GAMs performed similarly.

However, the review did find some differences. Neural networks tended to do better with larger datasets and more complex problems, but this advantage decreased over time. GAMs, on the other hand, remained competitive, especially with smaller datasets, and had the added benefit of being more interpretable, meaning it's easier to understand how they arrived at their conclusions.

The review also highlighted a major limitation of the existing research: many studies didn't provide enough information about the data and neural network complexity, making it hard to reproduce and build upon their results.

**The Takeaway:** Neural networks and GAMs are not competing tools, but rather complementary approaches. Depending on the specific problem and dataset, one may be more suitable than the other. For many applications, the performance difference between the two is modest, and GAMs may be preferred due to their interpretability. Ultimately, the choice between neural networks and GAMs depends on the specific needs of the project and the goals of the analysis."
stat.ML,Nearest Neighbor Matching as Least Squares Density Ratio Estimation and   Riesz Regression,"This study proves that Nearest Neighbor (NN) matching can be interpreted as an instance of Riesz regression for automatic debiased machine learning. Lin et al. (2023) shows that NN matching is an instance of density-ratio estimation with their new density-ratio estimator. Chernozhukov et al. (2024) develops Riesz regression for automatic debiased machine learning, which directly estimates the Riesz representer (or equivalently, the bias-correction term) by minimizing the mean squared error. In this study, we first prove that the density-ratio estimation method proposed in Lin et al. (2023) is essentially equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz regression using the LSIF framework. Based on these results, we derive NN matching from Riesz regression. This study is based on our work Kato (2025a) and Kato (2025b).",http://arxiv.org/abs/2510.24433v1,2025-10-28T14:01:51Z,Masahiro Kato,"Here's a summary of the research paper for a general audience:

**Understanding Nearest Neighbor Matching in a New Way**

Imagine you're trying to match similar things, like people or objects, based on certain characteristics. A common method used for this is called Nearest Neighbor (NN) matching. Researchers have been working to better understand how NN matching works and how it can be improved.

This study shows that NN matching can be viewed in a new light. It can be seen as a way of estimating the ratio of two probability distributions (called density-ratio estimation) and then using that information to make more accurate matches. The researchers also connected NN matching to a statistical method called Riesz regression, which helps to reduce bias in machine learning models.

The study's findings are important because they provide a deeper understanding of how NN matching works and how it can be used in a variety of applications, such as data analysis and machine learning. By understanding NN matching in this new way, researchers can develop new and improved methods for matching similar things, which can lead to more accurate and reliable results."
stat.ML,Perception Learning: A Formal Separation of Sensory Representation   Learning from Decision Learning,"We introduce Perception Learning (PeL), a paradigm that optimizes an agent's sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic signals, decoupled from downstream decision learning $g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free perceptual properties, such as stability to nuisances, informativeness without collapse, and controlled geometry, assessed via objective representation-invariant metrics. We formalize the separation of perception and decision, define perceptual properties independent of objectives or reparameterizations, and prove that PeL updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients. Additionally, we provide a suite of task-agnostic evaluation metrics to certify perceptual quality.",http://arxiv.org/abs/2510.24356v1,2025-10-28T12:19:49Z,Suman Sanyal,"Here's a summary of the research paper for a general audience:

**Improving How Machines Perceive the World**

Imagine you're trying to recognize a cat in a picture. Your brain doesn't just look at the picture as a whole; it breaks it down into smaller parts, like the shape of the ears or the color of the fur. This process is called perception. Researchers have made significant progress in teaching machines to make decisions, but how they perceive the world is still a challenge.

A new approach called Perception Learning (PeL) aims to improve how machines perceive the world by separating it into two stages: perception and decision-making. In the perception stage, the machine learns to extract useful information from sensory data, like images or sounds, without worrying about making decisions. This is like teaching a child to recognize different shapes and colors before asking them to identify a specific object.

The key innovation of PeL is that it uses task-agnostic signals, meaning it doesn't rely on specific goals or objectives. Instead, it focuses on learning perceptual properties that are useful in general, such as stability to changes in lighting or viewpoint. This approach allows machines to develop a more robust and informative representation of the world.

The researchers also developed new metrics to evaluate the quality of a machine's perception, which can help ensure that the machine is perceiving the world accurately and effectively. By decoupling perception from decision-making, PeL has the potential to improve the performance of machines in a wide range of applications, from computer vision to robotics.

**In simple terms:** Perception Learning is a new approach that helps machines better understand the world by separating perception from decision-making. It teaches machines to extract useful information from sensory data without worrying about specific goals, leading to more robust and accurate perception."
stat.ML,Problem-Parameter-Free Decentralized Bilevel Optimization,"Decentralized bilevel optimization has garnered significant attention due to its critical role in solving large-scale machine learning problems. However, existing methods often rely on prior knowledge of problem parameters-such as smoothness, convexity, or communication network topologies-to determine appropriate stepsizes. In practice, these problem parameters are typically unavailable, leading to substantial manual effort for hyperparameter tuning. In this paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for decentralized bilevel optimization with a single-loop structure. AdaSDBO leverages adaptive stepsizes based on cumulative gradient norms to update all variables simultaneously, dynamically adjusting its progress and eliminating the need for problem-specific hyperparameter tuning. Through rigorous theoretical analysis, we establish that AdaSDBO achieves a convergence rate of $\widetilde{\mathcal{O}}\left(\frac{1}{T}\right)$, matching the performance of well-tuned state-of-the-art methods up to polylogarithmic factors. Extensive numerical experiments demonstrate that AdaSDBO delivers competitive performance compared to existing decentralized bilevel optimization methods while exhibiting remarkable robustness across diverse stepsize configurations.",http://arxiv.org/abs/2510.24288v1,2025-10-28T10:50:04Z,"Zhiwei Zhai, Wenjing Yan, Ying-Jun Angela Zhang","**Breakthrough in Machine Learning: A New Algorithm for Large-Scale Optimization**

Imagine you're trying to optimize a complex system, like a traffic network or a recommendation engine. You need to make decisions on two levels: one for the overall system and another for individual components. This is known as bilevel optimization. A new algorithm, called AdaSDBO, has been developed to tackle this challenge in a decentralized way, meaning it can handle large-scale problems with many interconnected components.

The exciting part about AdaSDBO is that it doesn't require prior knowledge of specific problem details, such as how smooth or complex the problem is. This makes it much easier to use in practice, as users no longer need to spend a lot of time fine-tuning the algorithm. Instead, AdaSDBO adapts automatically, adjusting its progress and eliminating the need for manual hyperparameter tuning.

Theoretical analysis and numerical experiments have shown that AdaSDBO performs competitively with state-of-the-art methods, while being more robust and easier to use. This breakthrough has the potential to accelerate progress in various fields, including machine learning, artificial intelligence, and data science. With AdaSDBO, researchers and practitioners can now tackle complex optimization problems with greater ease and efficiency."
stat.ML,Self-Concordant Perturbations for Linear Bandits,"We study the adversarial linear bandits problem and present a unified algorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and Follow-the-Perturbed-Leader (FTPL) methods, extending the known connection between them from the full-information setting. Within this framework, we introduce self-concordant perturbations, a family of probability distributions that mirror the role of self-concordant barriers previously employed in the FTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based algorithm that combines self-concordant regularization with efficient stochastic exploration. Our approach achieves a regret of $O(d\sqrt{n \ln n})$ on both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean ball, this matches the rate attained by existing self-concordant FTRL methods. For the hypercube, this represents a $\sqrt{d}$ improvement over these methods and matches the optimal bound up to logarithmic factors.",http://arxiv.org/abs/2510.24187v1,2025-10-28T08:47:15Z,"Lucas Lévy, Jean-Lou Valeau, Arya Akhavan, Patrick Rebeschini","**Improving Decision-Making in Uncertain Environments**

Imagine you're trying to make a series of decisions, but you're not sure what the outcomes will be. This is a common problem in many fields, from finance to healthcare. Researchers have developed a new approach to help make better decisions in these uncertain situations.

The approach combines two popular methods, Follow-the-Regularized-Leader (FTRL) and Follow-the-Perturbed-Leader (FTPL), into a single framework. This framework uses a special type of ""perturbation"" - a random disturbance - to help guide the decision-making process.

The researchers introduced a new type of perturbation called ""self-concordant perturbations"", which helps to balance exploration and exploitation. This means that the algorithm can try new things, while also making informed decisions based on what it already knows.

The results show that this new approach can make better decisions than existing methods, especially in situations where there are many possible choices. Specifically, it achieved a regret of $O(d\sqrt{n \ln n})$ on both the $d$-dimensional hypercube and the Euclidean ball. This is a significant improvement over existing methods, with a $\sqrt{d}$ improvement over previous methods on the hypercube.

In simple terms, this research provides a new tool for making better decisions in uncertain environments, with potential applications in many fields. By combining the strengths of FTRL and FTPL methods, this approach can help improve decision-making and lead to better outcomes."
stat.ML,Copula-Stein Discrepancy: A Generator-Based Stein Operator for   Archimedean Dependence,"Kernel Stein discrepancies (KSDs) have become a principal tool for goodness-of-fit testing, but standard KSDs are often insensitive to higher-order dependency structures, such as tail dependence, which are critical in many scientific and financial domains. We address this gap by introducing the Copula-Stein Discrepancy (CSD), a novel class of discrepancies tailored to the geometry of statistical dependence. By defining a Stein operator directly on the copula density, CSD leverages the generative structure of dependence, rather than relying on the joint density's score function. For the broad class of Archimedean copulas, this approach yields a closed-form Stein kernel derived from the scalar generator function. We provide a comprehensive theoretical analysis, proving that CSD (i) metrizes weak convergence of copula distributions, ensuring it detects any mismatch in dependence; (ii) has an empirical estimator that converges at the minimax optimal rate of $O_P(n^{-1/2})$; and (iii) is provably sensitive to differences in tail dependence coefficients. The framework is extended to general non-Archimedean copulas, including elliptical and vine copulas. Computationally, the exact CSD kernel evaluation scales linearly in dimension, while a novel random feature approximation reduces the $n$-dependence from quadratic $O(n^2)$ to near-linear $\tilde{O}(n)$, making CSD a practical and theoretically principled tool for dependence-aware inference.",http://arxiv.org/abs/2510.24056v1,2025-10-28T04:33:57Z,"Agnideep Aich, Ashit Baran Aich","**Understanding Dependence in Data: A New Statistical Tool**

Imagine you're analyzing data from finance, climate science, or medicine. You want to know not just how individual variables behave, but also how they relate to each other. This is called ""dependence"" or ""correlation."" A new statistical tool, called Copula-Stein Discrepancy (CSD), can help you better understand these relationships.

**The Problem with Current Methods**

Current methods for testing how well a model fits data, called goodness-of-fit testing, often miss important details about how variables depend on each other, especially in extreme situations (like financial crises or natural disasters). This is a problem because understanding these dependencies is crucial in many fields.

**What is Copula-Stein Discrepancy (CSD)?**

CSD is a novel statistical tool that specifically targets the geometry of dependence between variables. It's designed to detect differences in how variables relate to each other, including in extreme situations. CSD works by analyzing the ""copula"" of the data, which is a mathematical representation of the dependence structure.

**Key Benefits of CSD**

1. **Sensitive to Tail Dependence**: CSD can detect differences in how variables behave in extreme situations, which is critical in many fields.
2. **Fast and Scalable**: CSD can be computed quickly, even for large datasets.
3. **Theoretically Principled**: CSD has a strong theoretical foundation, ensuring it provides accurate and reliable results.

**Implications and Applications**

CSD has the potential to improve statistical analysis in various fields, including finance, climate science, and medicine. By providing a more nuanced understanding of dependence, CSD can help researchers and practitioners make more informed decisions and predictions.

**In Simple Terms**

Think of CSD like a pair of glasses that helps you see the relationships between variables more clearly. It's a powerful tool that can help you understand how variables depend on each other, even in complex and extreme situations. With CSD, you can make more accurate predictions and informed decisions in a wide range of fields."
stat.ML,Optimal Arm Elimination Algorithms for Combinatorial Bandits,"Combinatorial bandits extend the classical bandit framework to settings where the learner selects multiple arms in each round, motivated by applications such as online recommendation and assortment optimization. While extensions of upper confidence bound (UCB) algorithms arise naturally in this context, adapting arm elimination methods has proved more challenging. We introduce a novel elimination scheme that partitions arms into three categories (confirmed, active, and eliminated), and incorporates explicit exploration to update these sets. We demonstrate the efficacy of our algorithm in two settings: the combinatorial multi-armed bandit with general graph feedback, and the combinatorial linear contextual bandit. In both cases, our approach achieves near-optimal regret, whereas UCB-based methods can provably fail due to insufficient explicit exploration. Matching lower bounds are also provided.",http://arxiv.org/abs/2510.23992v1,2025-10-28T01:50:24Z,"Yuxiao Wen, Yanjun Han, Zhengyuan Zhou","**New Algorithm Improves Decision-Making in Complex Online Systems**

Imagine you're a website owner trying to decide which products to recommend to your users. You want to maximize sales, but you also want to learn which products are most popular and effective over time. This is a classic problem in online systems, known as the ""bandit problem."" A team of researchers has now developed a new algorithm that helps solve this problem in complex situations where multiple products are recommended at once.

The algorithm works by categorizing products into three groups: those that are confirmed to be good, those that are still being tested, and those that are eliminated because they're unlikely to be effective. The algorithm then uses a strategy of explicit exploration to learn more about the products and update these categories.

The researchers tested their algorithm in two scenarios: one where the system receives feedback on which products worked well, and another where the system has to make decisions based on user characteristics, such as age and location. In both cases, their algorithm performed nearly optimally, meaning it made good decisions and learned quickly. This is an improvement over existing algorithms that can get stuck in certain situations. The researchers also showed that their algorithm is close to the best possible performance, which is a strong guarantee of its effectiveness."
stat.ML,Score-based constrained generative modeling via Langevin diffusions with   boundary conditions,"Score-based generative models based on stochastic differential equations (SDEs) achieve impressive performance in sampling from unknown distributions, but often fail to satisfy underlying constraints. We propose a constrained generative model using kinetic (underdamped) Langevin dynamics with specular reflection of velocity on the boundary defining constraints. This results in piecewise continuously differentiable noising and denoising process where the latter is characterized by a time-reversed dynamics restricted to a domain with boundary due to specular boundary condition. In addition, we also contribute to existing reflected SDEs based constrained generative models, where the stochastic dynamics is restricted through an abstract local time term. By presenting efficient numerical samplers which converge with optimal rate in terms of discretizations step, we provide a comprehensive comparison of models based on confined (specularly reflected kinetic) Langevin diffusion with models based on reflected diffusion with local time.",http://arxiv.org/abs/2510.23985v1,2025-10-28T01:36:54Z,"Adam Nordenhög, Akash Sharma","Here's a summary of the research paper for a general audience:

**Generating Realistic Data while Following Rules**

Imagine you want to generate new images or data that look realistic, but also need to follow certain rules, such as having a specific shape or staying within a certain boundary. This is a challenge for current artificial intelligence (AI) models, which can generate impressive data but often struggle to adhere to constraints.

**A New Approach**

Researchers have proposed a new method to tackle this problem by using a type of mathematical equation called a stochastic differential equation (SDE). Their approach uses a process called Langevin dynamics, which simulates the motion of particles. By adding a ""specular reflection"" mechanism, the particles bounce off the boundary when they hit it, ensuring that the generated data stays within the desired limits.

**Key Contributions**

The researchers made two main contributions:

1. They developed a new generative model that uses kinetic Langevin dynamics with specular reflection to ensure that the generated data follows the desired constraints.
2. They compared their approach to existing methods that use reflected diffusion with local time, and showed that their method is more efficient and converges faster.

**Impact**

This research has the potential to improve the generation of realistic data that follows specific rules, which could have applications in areas such as computer vision, robotics, and data augmentation. The new method provides a more efficient and effective way to generate data that meets certain constraints, which could lead to breakthroughs in various fields."
stat.ML,Machine learning approaches for interpretable antibody property   prediction using structural data,"Understanding the relationship between antibody sequence, structure and function is essential for the design of antibody-based therapeutics and research tools. Recently, machine learning (ML) models mostly based on the application of large language models to sequence information have been developed to predict antibody properties. Yet there are open directions to incorporate structural information, not only to enhance prediction but also to offer insights into the underlying molecular mechanisms. This chapter provides an overview of these approaches and describes two ML frameworks that integrate structural data (via graph representations) with neural networks to predict properties of antibodies: ANTIPASTI predicts binding affinity (a global property) whereas INFUSSE predicts residue flexibility (a local property). We survey the principles underpinning these models; the ways in which they encode structural knowledge; and the strategies that can be used to extract biologically relevant statistical signals that can help discover and disentangle molecular determinants of the properties of interest.",http://arxiv.org/abs/2510.23975v1,2025-10-28T01:13:09Z,"Kevin Michalewicz, Mauricio Barahona, Barbara Bravi","Here's a summary of the research paper for a general audience:

**Unlocking the Secrets of Antibodies with Machine Learning**

Antibodies are proteins that play a crucial role in our immune system, and scientists are working to design new antibody-based treatments for various diseases. To do this, they need to understand how the structure of an antibody relates to its function. Recently, researchers have been using machine learning (a type of artificial intelligence) to predict the properties of antibodies, such as how well they bind to specific targets.

In this study, the researchers developed two new machine learning models that incorporate structural data about antibodies, such as their 3D shape, to make more accurate predictions. These models, called ANTIPASTI and INFUSSE, can predict properties like binding affinity (how strongly an antibody binds to a target) and residue flexibility (how flexible certain parts of the antibody are).

The researchers found that by using structural data, their models can not only make more accurate predictions but also provide insights into the underlying molecular mechanisms that govern antibody behavior. This can help scientists identify the key factors that determine an antibody's properties and design new, more effective treatments.

Overall, this study demonstrates the power of machine learning in understanding the complex relationships between antibody structure and function, and has the potential to accelerate the development of new antibody-based therapies."
stat.ML,The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity,"Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences. Fitting a na\""ive probabilistic model to pairwise comparison data (say over prompt-completion pairs) yields an inconsistent estimate of the population-average utility -a canonical measure of social welfare. We propose a new method, dubbed the sign estimator, that provides a simple, provably consistent, and efficient estimator by replacing cross-entropy with binary classification loss in the aggregation step. This simple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finite-sample error bounds in this setting. In realistic simulations of LLM alignment using digital twins, the sign estimator substantially reduces preference distortion over a panel of simulated personas, cutting (angular) estimation error by nearly 35% and decreasing disagreement with true population preferences from 12% to 8% compared to standard RLHF. Our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individual-level preference data-all while maintaining the implementation simplicity of existing LLM alignment pipelines.",http://arxiv.org/abs/2510.23965v2,2025-10-28T00:42:38Z,"Ali Aouad, Aymane El Gadarri, Vivek F. Farias","Here's a summary of the research paper for a general audience:

**Improving AI Alignment with Human Preferences**

Large Language Models (LLMs) are AI systems that can understand and generate human-like text. To make them useful and safe, researchers need to ""align"" them with human preferences, so they produce responses that people find helpful and acceptable. However, people have different opinions and preferences, which can make it challenging to align LLMs.

**The Problem with Current Methods**

Current methods for aligning LLMs assume that all people have the same preferences, which is not true. This can lead to biased and inaccurate results. Researchers have tried to address this issue by using complex models that account for individual differences, but these models are often difficult to implement and require a lot of data.

**A New Solution: The Sign Estimator**

A team of researchers has developed a new method called the ""sign estimator"" that provides a simple and effective way to align LLMs with human preferences, even when people have different opinions. This method uses a different type of mathematical function to aggregate human preferences, which leads to more accurate and consistent results.

**Benefits of the Sign Estimator**

In simulations, the sign estimator outperformed traditional methods, reducing errors by 35% and disagreements with true population preferences by 4%. This method is also simpler to implement than other approaches that try to account for individual differences, making it a promising solution for aligning LLMs with human preferences.

Overall, the sign estimator offers a promising new approach to improving AI alignment with human preferences, which could lead to more accurate and helpful AI systems."
stat.ML,Understanding Fairness and Prediction Error through Subspace   Decomposition and Influence Analysis,"Machine learning models have achieved widespread success but often inherit and amplify historical biases, resulting in unfair outcomes. Traditional fairness methods typically impose constraints at the prediction level, without addressing underlying biases in data representations. In this work, we propose a principled framework that adjusts data representations to balance predictive utility and fairness. Using sufficient dimension reduction, we decompose the feature space into target-relevant, sensitive, and shared components, and control the fairness-utility trade-off by selectively removing sensitive information. We provide a theoretical analysis of how prediction error and fairness gaps evolve as shared subspaces are added, and employ influence functions to quantify their effects on the asymptotic behavior of parameter estimates. Experiments on both synthetic and real-world datasets validate our theoretical insights and show that the proposed method effectively improves fairness while preserving predictive performance.",http://arxiv.org/abs/2510.23935v1,2025-10-27T23:38:00Z,"Enze Shi, Pankaj Bhagwat, Zhixian Yang, Linglong Kong, Bei Jiang","**Making Machine Learning Fairer: A New Approach**

Machine learning models are increasingly used in our daily lives, but they can perpetuate and amplify existing biases, leading to unfair outcomes. For instance, a model used to screen job applicants may inadvertently favor candidates from certain backgrounds. To address this issue, researchers have proposed various ""fairness"" methods, but most focus on tweaking the model's predictions rather than tackling the root causes of bias.

A new study takes a different approach. The researchers propose a framework that adjusts the way data is represented to balance the model's accuracy and fairness. They break down the data into three components: information relevant to the prediction, sensitive information (e.g., age, sex, or ethnicity), and shared information that overlaps between the two.

By selectively removing sensitive information, the researchers can control the trade-off between fairness and accuracy. They also provide a theoretical analysis of how prediction errors and fairness gaps change as they add or remove shared information.

The good news is that experiments on both synthetic and real-world datasets show that this approach can effectively improve fairness while preserving the model's predictive performance. This work offers a promising new direction for developing more equitable machine learning models."
stat.ML,Testing-driven Variable Selection in Bayesian Modal Regression,"We propose a Bayesian variable selection method in the framework of modal regression for heavy-tailed responses. An efficient expectation-maximization algorithm is employed to expedite parameter estimation. A test statistic is constructed to exploit the shape of the model error distribution to effectively separate informative covariates from unimportant ones. Through simulations, we demonstrate and evaluate the efficacy of the proposed method in identifying important covariates in the presence of non-Gaussian model errors. Finally, we apply the proposed method to analyze two datasets arising in genetic and epigenetic studies.",http://arxiv.org/abs/2510.23831v1,2025-10-27T20:17:34Z,"Jiasong Duan, Hongmei Zhang, Xianzheng Huang","Here's a summary of the research paper for a general audience:

**Improving Data Analysis with a New Statistical Method**

Researchers have developed a new statistical method to help identify the most important factors that affect a particular outcome, even when the data is messy or doesn't follow a perfect pattern. This method, called Bayesian modal regression, is particularly useful when working with data that has extreme values or outliers.

The researchers tested their method using computer simulations and found that it was effective in picking out the most relevant factors from a large set of data. They also applied their method to two real-world datasets related to genetics and epigenetics, which study how genes are expressed and interact with the environment.

The innovation of this method lies in its ability to handle ""heavy-tailed"" data, which is common in many fields, and to distinguish between important and unimportant factors. This can lead to more accurate and reliable conclusions in data analysis, which can have significant implications in fields such as medicine, biology, and social sciences."
stat.ML,A Physics-informed Multi-resolution Neural Operator,"The predictive accuracy of operator learning frameworks depends on the quality and quantity of available training data (input-output function pairs), often requiring substantial amounts of high-fidelity data, which can be challenging to obtain in some real-world engineering applications. These datasets may be unevenly discretized from one realization to another, with the grid resolution varying across samples. In this study, we introduce a physics-informed operator learning approach by extending the Resolution Independent Neural Operator (RINO) framework to a fully data-free setup, addressing both challenges simultaneously. Here, the arbitrarily (but sufficiently finely) discretized input functions are projected onto a latent embedding space (i.e., a vector space of finite dimensions), using pre-trained basis functions. The operator associated with the underlying partial differential equations (PDEs) is then approximated by a simple multi-layer perceptron (MLP), which takes as input a latent code along with spatiotemporal coordinates to produce the solution in the physical space. The PDEs are enforced via a finite difference solver in the physical space. The validation and performance of the proposed method are benchmarked on several numerical examples with multi-resolution data, where input functions are sampled at varying resolutions, including both coarse and fine discretizations.",http://arxiv.org/abs/2510.23810v1,2025-10-27T19:50:02Z,"Sumanta Roy, Bahador Bahmani, Ioannis G. Kevrekidis, Michael D. Shields","**Breakthrough in AI-Powered Predictions for Complex Systems**

Imagine being able to accurately predict the behavior of complex systems, like weather patterns or fluid flows, using artificial intelligence (AI). However, this often requires a huge amount of high-quality data, which can be difficult to obtain. A team of researchers has made a significant advancement in addressing this challenge.

They've developed a new AI framework that combines physics and machine learning to make predictions without needing large amounts of data. This approach, called a physics-informed multi-resolution neural operator, uses a clever trick to work with data that's been collected at different levels of detail.

The researchers tested their method on several examples and found that it works well, even when the data is unevenly detailed. This breakthrough has the potential to improve predictions in various fields, such as engineering, climate modeling, and more. By leveraging the power of physics and AI, scientists can make more accurate predictions, even with limited data."
