category,title,summary,link,published,authors,summary_short
cs.LG,Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference   from weak lensing and galaxy clustering maps with deep learning. I. Analysis   design,"Data-driven approaches using deep learning are emerging as powerful techniques to extract non-Gaussian information from cosmological large-scale structure. This work presents the first simulation-based inference (SBI) pipeline that combines weak lensing and galaxy clustering maps in a realistic Dark Energy Survey Year 3 (DES Y3) configuration and serves as preparation for a forthcoming analysis of the survey data. We develop a scalable forward model based on the CosmoGridV1 suite of N-body simulations to generate over one million self-consistent mock realizations of DES Y3 at the map level. Leveraging this large dataset, we train deep graph convolutional neural networks on the full survey footprint in spherical geometry to learn low-dimensional features that approximately maximize mutual information with target parameters. These learned compressions enable neural density estimation of the implicit likelihood via normalizing flows in a ten-dimensional parameter space spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy bias parameters, while marginalizing over baryonic, photometric redshift, and shear bias nuisances. To ensure robustness, we extensively validate our inference pipeline using synthetic observations derived from both systematic contaminations in our forward model and independent Buzzard galaxy catalogs. Our forecasts yield significant improvements in cosmological parameter constraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m - S_8$ plane relative to our implementation of baseline two-point statistics and effectively breaking parameter degeneracies through probe combination. These results demonstrate the potential of SBI analyses powered by deep learning for upcoming Stage-IV wide-field imaging surveys.",http://arxiv.org/abs/2511.04681v1,2025-11-06T18:59:59Z,"A. Thomsen, J. Bucko, T. Kacprzak, V. Ajani, J. Fluri, A. Refregier, D. Anbajagane, F. J. Castander, A. Ferté, M. Gatti, N. Jeffrey, A. Alarcon, A. Amon, K. Bechtol, M. R. Becker, G. M. Bernstein, A. Campos, A. Carnero Rosell, C. Chang, R. Chen, A. Choi, M. Crocce, C. Davis, J. DeRose, S. Dodelson, C. Doux, K. Eckert, J. Elvin-Poole, S. Everett, P. Fosalba, D. Gruen, I. Harrison, K. Herner, E. M. Huff, M. Jarvis, N. Kuropatkin, P. -F. Leget, N. MacCrann, J. McCullough, J. Myles, A. Navarro-Alsina, S. Pandey, A. Porredon, J. Prat, M. Raveri, M. Rodriguez-Monroy, R. P. Rollins, A. Roodman, E. S. Rykoff, C. Sánchez, L. F. Secco, E. Sheldon, T. Shin, M. A. Troxel, I. Tutusaus, T. N. Varga, N. Weaverdyck, R. H. Wechsler, B. Yanny, B. Yin, Y. Zhang, J. Zuntz, S. Allam, F. Andrade-Oliveira, D. Bacon, J. Blazek, D. Brooks, R. Camilleri, J. Carretero, R. Cawthon, L. N. da Costa, M. E. da Silva Pereira, T. M. Davis, J. De Vicente, S. Desai, P. Doel, J. García-Bellido, G. Gutierrez, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, K. Kuehn, O. Lahav, S. Lee, J. L. Marshall, J. Mena-Fernández, F. Menanteau, R. Miquel, J. Muir, R. L. C. Ogando, A. A. Plazas Malagón, E. Sanchez, D. Sanchez Cid, I. Sevilla-Noarbe, M. Smith, E. Suchyta, M. E. C. Swanson, D. Thomas, C. To, D. L. Tucker","**Unlocking the Secrets of the Universe with Artificial Intelligence**

A team of researchers has made a groundbreaking advancement in understanding the universe using a combination of artificial intelligence (AI) and advanced simulations. Their work focuses on a mysterious phenomenon known as dark energy, which is thought to be driving the accelerating expansion of the universe.

The researchers developed a sophisticated AI pipeline that analyzes large-scale structures in the universe, such as galaxy distributions and weak lensing effects (the bending of light around massive objects). By training deep learning models on over a million simulated universes, they were able to extract valuable information from these structures.

The AI pipeline uses a technique called simulation-based inference, which allows researchers to make accurate predictions about the universe's properties, such as the amount of dark matter and dark energy. The team validated their approach using synthetic data and found that it yields significantly improved constraints on cosmological parameters, achieving 2-3 times higher precision than traditional methods.

This innovative approach has the potential to revolutionize our understanding of the universe, particularly with the advent of upcoming large-scale surveys that will provide vast amounts of new data. By harnessing the power of AI and simulations, researchers can unlock new insights into the fundamental nature of the universe."
cs.LG,"Multi-Method Analysis of Mathematics Placement Assessments: Classical,   Machine Learning, and Clustering Approaches","This study evaluates a 40-item mathematics placement examination administered to 198 students using a multi-method framework combining Classical Test Theory, machine learning, and unsupervised clustering. Classical Test Theory analysis reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$) while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement. Question 6 (Graph Interpretation) emerges as the examination's most powerful discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA F-statistic ($F = 4609.1$), and maximum Random Forest feature importance (0.206), accounting for 20.6\% of predictive power. Machine learning algorithms demonstrate exceptional performance, with Random Forest and Gradient Boosting achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering identifies a natural binary competency structure with a boundary at 42.5\%, diverging from the institutional threshold of 55\% and suggesting potential overclassification into remedial categories. The two-cluster solution exhibits exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster purity. Convergent evidence across methods supports specific refinements: replace poorly discriminating items, implement a two-stage assessment, and integrate Random Forest predictions with transparency mechanisms. These findings demonstrate that multi-method integration provides a robust empirical foundation for evidence-based mathematics placement optimization.",http://arxiv.org/abs/2511.04667v1,2025-11-06T18:53:07Z,"Julian D. Allagan, Dasia A. Singleton, Shanae N. Perry, Gabrielle C. Morgan, Essence A. Morgan","Here's a summary of the research paper for a general audience:

**Improving Math Placement Tests with Data Analysis**

Researchers analyzed a math placement test taken by 198 students to see how well it worked and how it could be improved. They used a combination of traditional statistical methods, machine learning algorithms, and clustering techniques to evaluate the test.

**Key Findings:**

* The test has some questions that don't do a good job of distinguishing between students who know the material and those who don't. About 30% of the questions need to be replaced.
* One question, about interpreting graphs, was particularly good at distinguishing between students.
* The researchers used machine learning algorithms to predict how well students would do on the test, and these predictions were very accurate (97.5% and 96% accuracy).
* The analysis also showed that students can be grouped into two categories: those who are competent in math and those who need extra help. This grouping was found to be very stable and accurate.

**Recommendations:**

* Replace poorly performing questions with new ones.
* Consider using a two-stage assessment process to get a more accurate picture of students' math abilities.
* Use machine learning predictions to help make placement decisions, but also provide transparency into how these predictions are made.

Overall, the study demonstrates the value of using multiple methods to analyze and improve math placement tests. By using a combination of traditional statistics, machine learning, and clustering techniques, educators can create more effective and accurate assessments that help students succeed."
cs.LG,Forgetting is Everywhere,"A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.",http://arxiv.org/abs/2511.04666v1,2025-11-06T18:52:57Z,"Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey","**The Science of Forgetting: A New Perspective on Learning**

Imagine you're trying to learn a new skill, but every time you acquire new information, you start to forget what you learned before. This phenomenon, known as ""forgetting,"" is a major challenge in developing artificial intelligence and machine learning algorithms. Researchers have been studying forgetting for decades, but a unified understanding of the concept has been elusive.

A recent study proposes a new theory that characterizes forgetting as a lack of consistency in a learner's predictions about future experiences. According to this theory, forgetting occurs when a learner's ability to make accurate predictions about future events deteriorates over time. This theory provides a general measure of an algorithm's tendency to forget and has been validated through a series of experiments across various learning settings, including classification, regression, generative modeling, and reinforcement learning.

The study's findings have significant implications for the development of more efficient and effective learning algorithms. By understanding the dynamics of forgetting, researchers can design algorithms that retain information better and learn more efficiently. This research lays the foundation for improving the information retention capabilities of general learning algorithms, which could lead to breakthroughs in areas such as artificial intelligence, robotics, and data analysis. Ultimately, a better understanding of forgetting could help us develop more intelligent and adaptable machines that can learn and improve over time."
cs.LG,Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation   of Soft-Body Interactions,"Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",http://arxiv.org/abs/2511.04665v1,2025-11-06T18:52:08Z,"Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li","**Advancing Robot Learning: A New Way to Test Robot Policies**

Imagine teaching a robot to pick up a soft toy or manipulate a rope. Currently, testing these robot policies in the real world is expensive, time-consuming, and hard to repeat. Researchers have found a solution: a simulation framework that mimics real-world interactions with soft objects, like toys and ropes.

This new approach, called ""Real-to-Sim,"" uses videos of real-world interactions to create digital replicas of objects and environments. It then uses a technique called 3D Gaussian Splatting to render these digital twins with incredible visual detail. This allows researchers to test robot policies in a simulated environment that closely resembles the real world.

In tests, the simulated results matched the real-world performance of robots learning to pack plush toys, route ropes, and push blocks. This suggests that the Real-to-Sim framework can accurately and reliably evaluate robot policies, making it easier to develop and improve robotic manipulation skills. This breakthrough could lead to faster progress in robotics and more efficient development of robots that can interact with soft objects."
cs.LG,Nowcast3D: Reliable precipitation nowcasting via gray-box learning,"Extreme precipitation nowcasting demands high spatiotemporal fidelity and extended lead times, yet existing approaches remain limited. Numerical Weather Prediction (NWP) and its deep-learning emulations are too slow and coarse for rapidly evolving convection, while extrapolation and purely data-driven models suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce a gray-box, fully three-dimensional nowcasting framework that directly processes volumetric radar reflectivity and couples physically constrained neural operators with datadriven learning. The model learns vertically varying 3D advection fields under a conservative advection operator, parameterizes spatially varying diffusion, and introduces a Brownian-motion--inspired stochastic term to represent unresolved motions. A residual branch captures small-scale convective initiation and microphysical variability, while a diffusion-based stochastic module estimates uncertainty. The framework achieves more accurate forecasts up to three-hour lead time across precipitation regimes and ranked first in 57\% of cases in a blind evaluation by 160 meteorologists. By restoring full 3D dynamics with physical consistency, it offers a scalable and robust pathway for skillful and reliable nowcasting of extreme precipitation.",http://arxiv.org/abs/2511.04659v1,2025-11-06T18:44:35Z,"Huaguan Chen, Wei Han, Haofei Sun, Ning Lin, Xingtao Song, Yunfan Yang, Jie Tian, Yang Liu, Ji-Rong Wen, Xiaoye Zhang, Xueshun Shen, Hao Sun","**Accurate Rainfall Forecasting Just Got a Boost**

Predicting heavy rainfall is crucial for preventing floods and landslides, but current methods have limitations. Researchers have developed a new approach called Nowcast3D, which uses a combination of data and physical laws to forecast rainfall more accurately.

Traditional methods rely on numerical weather prediction models or simple extrapolation of current weather conditions. However, these approaches are often too slow or too simplistic to capture the complex dynamics of rapidly evolving storms.

Nowcast3D takes a different approach by directly processing 3D radar data and combining it with physical constraints and machine learning algorithms. This allows the model to capture the vertical structure of storms and accurately predict rainfall patterns.

The results are impressive: Nowcast3D outperformed existing methods in a blind evaluation by 160 meteorologists, ranking first in 57% of cases. The model can forecast rainfall up to three hours in advance with greater accuracy, across various precipitation regimes.

This breakthrough offers a more reliable and scalable way to predict extreme precipitation events, which can help save lives and prevent damage to infrastructure. By restoring the full 3D dynamics of storms with physical consistency, Nowcast3D paves the way for more accurate and reliable rainfall forecasting."
cs.LG,TT-Prune: Joint Model Pruning and Resource Allocation for   Communication-efficient Time-triggered Federated Learning,"Federated learning (FL) offers new opportunities in machine learning, particularly in addressing data privacy concerns. In contrast to conventional event-based federated learning, time-triggered federated learning (TT-Fed), as a general form of both asynchronous and synchronous FL, clusters users into different tiers based on fixed time intervals. However, the FL network consists of a growing number of user devices with limited wireless bandwidth, consequently magnifying issues such as stragglers and communication overhead. In this paper, we introduce adaptive model pruning to wireless TT-Fed systems and study the problem of jointly optimizing the pruning ratio and bandwidth allocation to minimize the training loss while ensuring minimal learning latency. To answer this question, we perform convergence analysis on the gradient l_2 norm of the TT-Fed model based on model pruning. Based on the obtained convergence upper bound, a joint optimization problem of pruning ratio and wireless bandwidth is formulated to minimize the model training loss under a given delay threshold. Then, we derive closed-form solutions for wireless bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The simulation results show that model pruning could reduce the communication cost by 40% while maintaining the model performance at the same level.",http://arxiv.org/abs/2511.04653v1,2025-11-06T18:43:03Z,"Xinlu Zhang, Yansha Deng, Toktam Mahmoodi","Here's a summary of the research paper for a general audience:

**Improving Efficiency in Machine Learning**

Machine learning has become a crucial part of our lives, but it often requires large amounts of data and computing power. Federated learning (FL) is a technique that allows multiple devices to learn from each other without sharing their data, which helps with data privacy. However, as more devices join the network, communication between them becomes a bottleneck, slowing down the learning process.

Researchers have proposed a new approach called TT-Prune, which combines two techniques to improve the efficiency of federated learning. The first technique, called model pruning, reduces the size of the machine learning model, making it easier to communicate between devices. The second technique optimizes how bandwidth is allocated to devices, ensuring that the most important information is transmitted quickly.

The study found that by using TT-Prune, the communication cost can be reduced by 40% without sacrificing the performance of the machine learning model. This is a significant improvement, as it enables faster and more efficient learning on a large scale. The results have the potential to make machine learning more accessible and efficient, especially in applications where data privacy is a concern."
cs.LG,Optimal Inference Schedules for Masked Diffusion Models,"A major bottleneck of standard auto-regressive large language models is that their inference process is inherently sequential, resulting in very long and costly inference times. To circumvent this, practitioners proposed a class of language models called diffusion language models, of which the masked diffusion model (MDM) is the most successful. The MDM is able to sample tokens out-of-order and, ostensibly, many tokens at once and in parallel. However, there is very limited rigorous understanding of how much parallel sampling these models can perform without noticeable degradation in their sampling performance. Prior work of Li and Cai obtained some preliminary bounds, but these are not tight for many natural classes of distributions. In this work, we give a new, exact characterization of the expected divergence between the true distribution and the sampled distribution, for any distribution and any unmasking schedule for the sampler, showing an elegant connection to the theory of univariate function approximation.   By leveraging this connection, we then attain a number of novel lower and upper bounds for this problem. While the connection to function approximation in principle gives the optimal unmasking schedule for any distribution, we show that it is in general impossible to compete with it without strong a priori knowledge of the distribution, even in seemingly benign settings. However, we also demonstrate new upper bounds and new sampling schedules in terms of well-studied information-theoretic properties of the base distribution, namely, its total correlation and dual total correlation, which show that in some natural settings, one can sample in $O(log n)$ steps without any visible loss in performance, where $n$ is the total sequence length.",http://arxiv.org/abs/2511.04647v1,2025-11-06T18:38:24Z,"Sitan Chen, Kevin Cong, Jerry Li","**Breakthrough in AI Model Efficiency: Optimal Inference Schedules for Masked Diffusion Models**

Researchers have made a significant advancement in improving the efficiency of large language models, a crucial component of many AI systems. The traditional auto-regressive models have a major drawback: they process information sequentially, leading to slow and costly inference times. To overcome this limitation, a new class of models called diffusion language models, specifically the Masked Diffusion Model (MDM), has been developed. The MDM can sample tokens in parallel, potentially speeding up the inference process.

The study provides a deeper understanding of how much parallel sampling these models can perform without compromising their performance. The researchers have developed a new framework that characterizes the trade-off between parallel sampling and performance. They have also discovered that, in some natural settings, it is possible to sample in $O(log n)$ steps without any noticeable loss in performance, where $n$ is the total sequence length. This is a significant improvement over traditional sequential models.

The findings have important implications for the development of more efficient AI systems. By optimizing the inference schedule, researchers can create models that are not only faster but also more accurate. This breakthrough has the potential to accelerate progress in natural language processing and other areas of AI research."
cs.LG,DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for   Embodied LLM-Based Multi-Agent Collaboration,"Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",http://arxiv.org/abs/2511.04646v1,2025-11-06T18:37:18Z,"Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong","Here's a summary of the research paper for a general audience:

**Title:** DR. WELL: A New Framework for Teamwork between Artificial Agents

**Imagine a Team Working Together:** When people work together on a task, they often need to make decisions quickly and communicate with each other to avoid conflicts. This can be tricky, especially when they're working together on a complex task. Researchers have developed a new framework called DR. WELL, which helps artificial agents (like robots or virtual assistants) work together more effectively.

**How DR. WELL Works:** DR. WELL uses a combination of logical planning and learning to help agents work together. The framework involves a two-step process: first, agents propose roles and negotiate with each other to come to an agreement. Then, each agent creates a plan to achieve its role without sharing detailed steps with the others. A shared ""world model"" keeps track of the current state of the environment and helps agents adjust their plans as needed.

**The Benefits:** By using this approach, DR. WELL enables agents to work together more efficiently and effectively. The framework allows agents to adapt to changing situations and learn from experience, which improves their performance over time. In tests, agents using DR. WELL were able to complete tasks more quickly and accurately, even in complex scenarios.

**The Big Picture:** This research has implications for a wide range of applications, from robotics and autonomous vehicles to smart homes and cities. By enabling artificial agents to work together more effectively, we can create more efficient, safe, and productive systems that benefit society as a whole."
cs.LG,Efficient probabilistic surrogate modeling techniques for   partially-observed large-scale dynamical systems,"This paper is concerned with probabilistic techniques for forecasting dynamical systems described by partial differential equations (such as, for example, the Navier-Stokes equations). In particular, it is investigating and comparing various extensions to the flow matching paradigm that reduce the number of sampling steps. In this regard, it compares direct distillation, progressive distillation, adversarial diffusion distillation, Wasserstein GANs and rectified flows. Moreover, experiments are conducted on a set of challenging systems. In particular, we also address the challenge of directly predicting 2D slices of large-scale 3D simulations, paving the way for efficient inflow generation for solvers.",http://arxiv.org/abs/2511.04641v1,2025-11-06T18:35:01Z,"Hans Harder, Abhijeet Vishwasrao, Luca Guastoni, Ricardo Vinuesa, Sebastian Peitz","Here's a summary of the research paper for a general audience:

**Predicting Complex Systems with Less Computing Power**

Imagine being able to accurately predict the behavior of complex systems, like ocean currents or weather patterns, without needing massive amounts of computing power. Researchers have made progress in developing efficient techniques for forecasting these systems, which are crucial for fields like climate modeling, engineering, and physics.

The study focuses on a type of mathematical equation called partial differential equations, which describe how systems change over time. The researchers tested various methods for speeding up the prediction process, which involves making educated guesses about the system's behavior. They compared different approaches, including some that use artificial intelligence and machine learning.

The good news is that these techniques can significantly reduce the number of calculations needed to make accurate predictions. This is especially important for large-scale systems, like 3D simulations, where computing power is a major limitation. The researchers demonstrated their methods on challenging systems, including predicting 2D slices of 3D simulations. This breakthrough could lead to more efficient and accurate forecasting tools for a wide range of applications."
cs.LG,Addressing divergent representations from causal interventions on neural   networks,"A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two classes of such divergences: `harmless' divergences that occur in the null-space of the weights and from covariance within behavioral decision boundaries, and `pernicious' divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we modify the Counterfactual Latent (CL) loss from Grant (2025) that regularizes interventions to remain closer to the natural distributions, reducing the likelihood of harmful divergences while preserving the interpretive power of interventions. Together, these results highlight a path towards more reliable interpretability methods.",http://arxiv.org/abs/2511.04638v1,2025-11-06T18:32:34Z,"Satchel Grant, Simon Jerome Han, Alexa Tartaglini, Christopher Potts","**Unlocking the Secrets of AI: A New Approach to Understanding Neural Networks**

Imagine trying to understand how a complex machine works by poking and prodding its inner parts. That's basically what researchers do when they try to interpret how artificial intelligence (AI) systems, like neural networks, make decisions. One common approach is to intervene on specific parts of the network to see how it affects the outcome. However, a new study asks: what if these interventions create artificial or ""divergent"" representations that don't accurately reflect how the AI system normally works?

The researchers found that, indeed, common intervention techniques can create these divergent representations, which raises concerns about the accuracy of the insights gained. They identified two types of divergences: ""harmless"" ones that don't affect the AI's behavior and ""pernicious"" ones that can activate hidden pathways and cause unexpected changes.

To address this issue, the researchers modified a technique called the Counterfactual Latent (CL) loss, which helps interventions stay closer to the AI system's natural behavior. This approach reduces the likelihood of pernicious divergences while preserving the interpretive power of interventions.

In simple terms, this study highlights the importance of being careful when intervening on AI systems to understand how they work. By developing more reliable methods, researchers can gain a deeper understanding of AI decision-making and improve the development of more transparent and trustworthy AI systems."
cs.LG,ODE approximation for the Adam algorithm: General and overparametrized   setting,"The Adam optimizer is currently presumably the most popular optimization method in deep learning. In this article we develop an ODE based method to study the Adam optimizer in a fast-slow scaling regime. For fixed momentum parameters and vanishing step-sizes, we show that the Adam algorithm is an asymptotic pseudo-trajectory of the flow of a particular vector field, which is referred to as the Adam vector field. Leveraging properties of asymptotic pseudo-trajectories, we establish convergence results for the Adam algorithm. In particular, in a very general setting we show that if the Adam algorithm converges, then the limit must be a zero of the Adam vector field, rather than a local minimizer or critical point of the objective function.   In contrast, in the overparametrized empirical risk minimization setting, the Adam algorithm is able to locally find the set of minima. Specifically, we show that in a neighborhood of the global minima, the objective function serves as a Lyapunov function for the flow induced by the Adam vector field. As a consequence, if the Adam algorithm enters a neighborhood of the global minima infinitely often, it converges to the set of global minima.",http://arxiv.org/abs/2511.04622v1,2025-11-06T18:15:41Z,"Steffen Dereich, Arnulf Jentzen, Sebastian Kassing","**Unlocking the Secrets of the Adam Optimizer: A Breakthrough in Deep Learning**

The Adam optimizer is a widely used algorithm in deep learning, a field of artificial intelligence that enables computers to learn from data. Researchers have made a significant breakthrough in understanding how the Adam optimizer works. By using a mathematical technique called ordinary differential equations (ODEs), they have developed a new way to study the behavior of the Adam optimizer.

**What did the researchers discover?**

The researchers found that the Adam optimizer can be approximated by a continuous mathematical process, which helps to understand its behavior over time. They showed that if the Adam optimizer converges, it will converge to a point where the objective function is zero, rather than necessarily finding the optimal solution. However, in a specific setting where the model is overparametrized (i.e., has more parameters than needed), the Adam optimizer can locally find the set of optimal solutions.

**What does this mean?**

In simple terms, the researchers have gained a deeper understanding of how the Adam optimizer works and under what conditions it can find the optimal solution. This is important because it can help improve the performance of deep learning models, which are used in a wide range of applications, from image and speech recognition to natural language processing.

**Key takeaways:**

* The Adam optimizer can be approximated by a continuous mathematical process.
* The optimizer converges to a point where the objective function is zero, rather than necessarily finding the optimal solution.
* In overparametrized settings, the Adam optimizer can locally find the set of optimal solutions.

This research provides new insights into the behavior of the Adam optimizer and has the potential to improve the performance of deep learning models."
cs.LG,Dynamic causal discovery in Alzheimer's disease through latent   pseudotime modelling,"The application of causal discovery to diseases like Alzheimer's (AD) is limited by the static graph assumptions of most methods; such models cannot account for an evolving pathophysiology, modulated by a latent disease pseudotime. We propose to apply an existing latent variable model to real-world AD data, inferring a pseudotime that orders patients along a data-driven disease trajectory independent of chronological age, then learning how causal relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC 0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge substantially improved graph accuracy and orientation. Our framework reveals dynamic interactions between novel (NfL, GFAP) and established AD markers, enabling practical causal discovery despite violated assumptions.",http://arxiv.org/abs/2511.04619v1,2025-11-06T18:12:09Z,"Natalia Glazman, Jyoti Mangal, Pedro Borges, Sebastien Ourselin, M. Jorge Cardoso","**Unlocking the Progression of Alzheimer's Disease**

Researchers have made a breakthrough in understanding Alzheimer's disease (AD) by developing a new approach to analyze the complex changes that occur in the brain over time. Current methods for studying AD assume that the relationships between different factors remain constant, but this is not the case. The disease progresses dynamically, and its underlying biology changes as it advances.

The researchers applied a novel method that infers a ""pseudotime"" - a data-driven measure of disease progression that is independent of a person's age. This pseudotime was able to predict AD diagnosis more accurately than age alone.

By incorporating minimal background knowledge, the researchers were able to create a more accurate map of the causal relationships between different factors involved in AD. This map reveals how interactions between established and novel markers of AD, such as NfL and GFAP, change as the disease progresses.

This study provides a new framework for understanding the dynamic progression of Alzheimer's disease, which could lead to the development of more effective treatments and diagnostic tools. The findings have the potential to improve our understanding of AD and ultimately lead to better patient outcomes."
cs.LG,evomap: A Toolbox for Dynamic Mapping in Python,"This paper presents evomap, a Python package for dynamic mapping. Mapping methods are widely used across disciplines to visualize relationships among objects as spatial representations, or maps. However, most existing statistical software supports only static mapping, which captures objects' relationships at a single point in time and lacks tools to analyze how these relationships evolve. evomap fills this gap by implementing the dynamic mapping framework EvoMap, originally proposed by Matthe, Ringel, and Skiera (2023), which adapts traditional static mapping methods for dynamic analyses. The package supports multiple mapping techniques, including variants of Multidimensional Scaling (MDS), Sammon Mapping, and t-distributed Stochastic Neighbor Embedding (t-SNE). It also includes utilities for data preprocessing, exploration, and result evaluation, offering a comprehensive toolkit for dynamic mapping applications. This paper outlines the foundations of static and dynamic mapping, describes the architecture and functionality of evomap, and illustrates its application through an extensive usage example.",http://arxiv.org/abs/2511.04611v1,2025-11-06T18:02:58Z,Maximilian Matthe,"**Introducing evomap: A New Tool for Dynamic Mapping**

Imagine being able to visualize how relationships between objects change over time. This is now possible with evomap, a new Python package that allows researchers and analysts to create dynamic maps. Maps are a great way to understand complex relationships between objects, but most current tools only provide a snapshot of these relationships at a single point in time.

evomap fills this gap by enabling dynamic mapping, which shows how relationships evolve over time. The package supports various mapping techniques, such as Multidimensional Scaling, Sammon Mapping, and t-SNE, and includes tools for data preparation, exploration, and evaluation.

With evomap, users can analyze how relationships between objects change over time, which can be useful in various fields, such as social network analysis, market research, and bioinformatics. The package provides a comprehensive toolkit for dynamic mapping applications, making it easier to gain insights into complex data.

The developers of evomap have demonstrated its capabilities through an extensive example, showcasing its potential to become a valuable resource for researchers and analysts working with dynamic data."
cs.LG,"Environment Agnostic Goal-Conditioning, A Study of Reward-Free   Autonomous Learning","In this paper we study how transforming regular reinforcement learning environments into goal-conditioned environments can let agents learn to solve tasks autonomously and reward-free. We show that an agent can learn to solve tasks by selecting its own goals in an environment-agnostic way, at training times comparable to externally guided reinforcement learning. Our method is independent of the underlying off-policy learning algorithm. Since our method is environment-agnostic, the agent does not value any goals higher than others, leading to instability in performance for individual goals. However, in our experiments, we show that the average goal success rate improves and stabilizes. An agent trained with this method can be instructed to seek any observations made in the environment, enabling generic training of agents prior to specific use cases.",http://arxiv.org/abs/2511.04598v1,2025-11-06T17:51:11Z,"Hampus Åström, Elin Anna Topp, Jacek Malec","Here's a summary of the research paper for a general audience:

**Teaching AI to Learn on Its Own**

Imagine teaching a robot to perform tasks without giving it specific instructions or rewards. Researchers have made progress in achieving this goal by developing a new approach to training AI agents. The method, called environment-agnostic goal-conditioning, allows agents to learn tasks autonomously and without external guidance.

In traditional reinforcement learning, an AI agent learns by receiving rewards or punishments for its actions. However, this approach can be limited and requires a lot of human input. The new approach enables agents to select their own goals and learn to achieve them without any external rewards.

The researchers found that agents trained with this method can learn to solve tasks just as well as those trained with traditional methods. Moreover, these agents can be instructed to pursue any goal they encounter in their environment, making them highly versatile.

This breakthrough has significant implications for the development of autonomous systems, such as robots and self-driving cars. It could enable these systems to adapt to new situations and learn new tasks on their own, without requiring extensive retraining."
cs.LG,Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest   Path Problems,"Multi-agent systems (MAS) are central to applications such as swarm robotics and traffic routing, where agents must coordinate in a decentralized manner to achieve a common objective. Stochastic Shortest Path (SSP) problems provide a natural framework for modeling decentralized control in such settings. While the problem of learning in SSP has been extensively studied in single-agent settings, the decentralized multi-agent variant remains largely unexplored. In this work, we take a step towards addressing that gap. We study decentralized multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the transition dynamics and costs are represented using linear models. Applying novel symmetry-based arguments, we identify the structure of optimal policies. Our main contribution is the first regret lower bound for this setting based on the construction of hard-to-learn instances for any number of agents, $n$. Our regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the inherent learning difficulty in Dec-MASSPs. These insights clarify the learning complexity of decentralized control and can further guide the design of efficient learning algorithms in multi-agent systems.",http://arxiv.org/abs/2511.04594v1,2025-11-06T17:49:33Z,"Utkarsh U. Chavan, Prashant Trivedi, Nandyala Hemachandra","**Decentralized Decision-Making in Multi-Agent Systems: A New Understanding**

Imagine a swarm of robots working together to navigate through a complex environment or a network of self-driving cars coordinating to optimize traffic flow. These multi-agent systems require decentralized decision-making, where individual agents make choices based on their local information to achieve a common goal. Researchers have been studying how to optimize decision-making in such systems, but a key challenge remains: how to learn effective strategies when multiple agents are involved.

A new study focuses on a specific type of problem known as the Stochastic Shortest Path (SSP) problem, which models situations where agents must navigate through uncertain environments to reach a goal. The researchers investigated decentralized multi-agent SSPs, where multiple agents work together to find the best path.

The study's main contribution is a new understanding of the fundamental limits of learning in decentralized multi-agent systems. The researchers found that, in the worst-case scenario, any learning algorithm will require a certain minimum number of trials (or ""episodes"") to learn an effective strategy. Specifically, they established a regret lower bound of $\Omega(\sqrt{K})$ over $K$ episodes, which highlights the inherent learning difficulty in decentralized multi-agent systems.

This result has important implications for the design of efficient learning algorithms in multi-agent systems. It suggests that researchers should focus on developing algorithms that can learn quickly and effectively in complex, decentralized environments. The study's findings can help guide the development of more efficient and effective solutions for decentralized decision-making in multi-agent systems."
cs.LG,Complexity as Advantage: A Regret-Based Perspective on Emergent   Structure,"We introduce Complexity as Advantage (CAA), a framework that defines the complexity of a system relative to a family of observers. Instead of measuring complexity as an intrinsic property, we evaluate how much predictive regret a system induces for different observers attempting to model it. A system is complex when it is easy for some observers and hard for others, creating an information advantage. We show that this formulation unifies several notions of emergent behavior, including multiscale entropy, predictive information, and observer-dependent structure. The framework suggests that ""interesting"" systems are those positioned to create differentiated regret across observers, providing a quantitative grounding for why complexity can be functionally valuable. We demonstrate the idea through simple dynamical models and discuss implications for learning, evolution, and artificial agents.",http://arxiv.org/abs/2511.04590v1,2025-11-06T17:46:53Z,Oshri Naparstek,"**Unlocking the Power of Complexity**

Imagine trying to understand a complex system, like a flock of birds or a stock market. Different people might have vastly different levels of difficulty in predicting what the system will do next. That's the core idea behind a new framework called Complexity as Advantage (CAA).

Researchers have developed CAA to measure the complexity of a system not by its inherent properties, but by how much it surprises or frustrates different observers trying to understand it. A system is considered complex if it's easy for some people to predict, but hard for others. This creates an ""information advantage"" where some observers have more knowledge or insight than others.

This new perspective on complexity helps explain why some systems are more interesting or valuable than others. It suggests that complex systems are those that can create a range of reactions and surprises, making them more useful for learning, evolution, and even artificial intelligence.

The researchers behind CAA have tested their ideas using simple computer models and found that they can indeed create different levels of surprise and insight for different observers. Their work has implications for fields such as biology, economics, and computer science, and could lead to a better understanding of how complex systems work and how to harness their power."
cs.LG,Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration   from a Baseline Paper,"Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",http://arxiv.org/abs/2511.04583v1,2025-11-06T17:37:49Z,"Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa","**Breakthrough in AI-Driven Scientific Research: Jr. AI Scientist**

Imagine a computer system that can analyze scientific research, identify areas for improvement, and even write its own papers. This is now a reality with Jr. AI Scientist, a cutting-edge artificial intelligence (AI) system designed to mimic the workflow of a junior researcher. Given a baseline research paper, Jr. AI Scientist can analyze its limitations, come up with new ideas to improve it, test these ideas through experiments, and write a paper with the results.

**What makes Jr. AI Scientist special?**

Unlike previous AI systems, Jr. AI Scientist follows a well-defined research workflow and can handle complex coding tasks. This leads to scientifically valuable contributions that can potentially advance various fields. In evaluations, Jr. AI Scientist generated papers that received higher review scores than existing fully automated systems.

**But what are the risks?**

While Jr. AI Scientist shows promise, the researchers behind it also identified important limitations and potential risks. For instance, the system may not always produce accurate or reliable results, and there are concerns about its ability to operate independently without human oversight. The researchers also highlighted the need for careful evaluation and testing to ensure that AI Scientist systems like Jr. AI Scientist are used responsibly.

**What's next?**

The development of Jr. AI Scientist provides valuable insights into the current progress and risks of AI-driven scientific research. As AI continues to play a larger role in scientific discovery, it's essential to understand both the benefits and challenges of these systems. The researchers hope that their findings will inform future research and help ensure that AI-driven scientific progress is trustworthy, sustainable, and serves the greater good."
cs.LG,Physics-Informed Neural Networks and Neural Operators for Parametric   PDEs: A Human-AI Collaborative Analysis,"PDEs arise ubiquitously in science and engineering, where solutions depend on parameters (physical properties, boundary conditions, geometry). Traditional numerical methods require re-solving the PDE for each parameter, making parameter space exploration prohibitively expensive. Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. We critically analyze two main paradigms: (1) PINNs, which embed physical laws as soft constraints and excel at inverse problems with sparse data, and (2) neural operators (e.g., DeepONet, Fourier Neural Operator), which learn mappings between infinite-dimensional function spaces and achieve unprecedented generalization. Through comparisons across fluid dynamics, solid mechanics, heat transfer, and electromagnetics, we show neural operators can achieve computational speedups of $10^3$ to $10^5$ times faster than traditional solvers for multi-query scenarios, while maintaining comparable accuracy. We provide practical guidance for method selection, discuss theoretical foundations (universal approximation, convergence), and identify critical open challenges: high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive, incrementally updated resource for this rapidly evolving field",http://arxiv.org/abs/2511.04576v2,2025-11-06T17:31:59Z,"Zhuo Zhang, Xiong Xiong, Sen Zhang, Yuan Zhao, Xi Yang","**Breakthrough in Solving Complex Mathematical Equations**

Researchers have made a significant advancement in solving complex mathematical equations, known as partial differential equations (PDEs), which are crucial in various fields such as physics, engineering, and environmental science. These equations describe how physical systems change over time and space, but solving them can be computationally expensive, especially when many variables are involved.

**The Problem with Traditional Methods**

Traditional numerical methods require re-solving the PDE for each set of parameters, which can be prohibitively expensive. For example, if you want to study how a bridge behaves under different weather conditions or with different materials, traditional methods would require re-running the simulation for each scenario.

**The Solution: Physics-Informed Neural Networks and Neural Operators**

Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. Researchers have developed two main approaches:

1. **Physics-Informed Neural Networks (PINNs)**: These embed physical laws into neural networks, making them particularly useful for solving inverse problems with limited data.
2. **Neural Operators**: These learn mappings between infinite-dimensional function spaces, allowing for rapid solutions across a wide range of parameters.

**Key Findings**

* Neural operators can solve PDEs 1,000 to 100,000 times faster than traditional methods, while maintaining comparable accuracy.
* These methods have been successfully applied to various fields, including fluid dynamics, solid mechanics, heat transfer, and electromagnetics.
* Researchers have established a unified framework for understanding parametric PDE solvers via operator learning.

**Future Directions**

While significant progress has been made, challenges remain, such as:

* Handling high-dimensional parameters
* Dealing with complex geometries
* Generalizing to new, unseen situations

This research provides a comprehensive resource for understanding and applying these innovative methods, paving the way for further breakthroughs in scientific computing and engineering applications."
cs.LG,ARETE: an R package for Automated REtrieval from TExt with large   language models,"1. A hard stop for the implementation of rigorous conservation initiatives is our lack of key species data, especially occurrence data. Furthermore, researchers have to contend with an accelerated speed at which new information must be collected and processed due to anthropogenic activity. Publications ranging from scientific papers to gray literature contain this crucial information but their data are often not machine-readable, requiring extensive human work to be retrieved. 2. We present the ARETE R package, an open-source software aiming to automate data extraction of species occurrences powered by large language models, namely using the chatGPT Application Programming Interface. This R package integrates all steps of the data extraction and validation process, from Optical Character Recognition to detection of outliers and output in tabular format. Furthermore, we validate ARETE through systematic comparison between what is modelled and the work of human annotators. 3. We demonstrate the usefulness of the approach by comparing range maps produced using GBIF data and with those automatically extracted for 100 species of spiders. Newly extracted data allowed to expand the known Extent of Occurrence by a mean three orders of magnitude, revealing new areas where the species were found in the past, which mayhave important implications for spatial conservation planning and extinction risk assessments. 4. ARETE allows faster access to hitherto untapped occurrence data, a potential game changer in projects requiring such data. Researchers will be able to better prioritize resources, manually verifying selected species while maintaining automated extraction for the majority. This workflow also allows predicting available bibliographic data during project planning.",http://arxiv.org/abs/2511.04573v1,2025-11-06T17:26:48Z,"Vasco V. Branco, Jandó Benedek, Lidia Pivovarova, Luís Correia, Pedro Cardoso","Here's a summary of the research paper for a general audience:

**Unlocking Hidden Data to Protect Endangered Species**

Conservation efforts are hindered by a lack of data on species' whereabouts. Researchers spend a lot of time manually extracting information from scientific papers, reports, and other documents, which can be a slow and tedious process. A new tool called ARETE aims to change that.

ARETE is a software package that uses artificial intelligence (AI) to automatically extract data on species occurrences from text. It uses large language models, like the technology behind chatbots, to read and understand text, and then extract relevant information. This can save researchers a significant amount of time and effort.

In a test, ARETE was used to extract data on 100 species of spiders. The results were remarkable: the extracted data revealed new areas where the species were found, expanding their known ranges by a huge amount. This has important implications for conservation planning and assessing the risk of extinction.

The ARETE tool has the potential to be a game-changer for conservation efforts. By automating the data extraction process, researchers can focus on higher-level tasks, such as analyzing and interpreting the data. This can help prioritize resources, predict where species are likely to be found, and ultimately inform conservation decisions."
cs.LG,Riesz Regression As Direct Density Ratio Estimation,"Riesz regression has garnered attention as a tool in debiased machine learning for causal and structural parameter estimation (Chernozhukov et al., 2021). This study shows that Riesz regression is closely related to direct density-ratio estimation (DRE) in important cases, including average treat- ment effect (ATE) estimation. Specifically, the idea and objective in Riesz regression coincide with the one in least-squares importance fitting (LSIF, Kanamori et al., 2009) in direct density-ratio estimation. While Riesz regression is general in the sense that it can be applied to Riesz representer estimation in a wide class of problems, the equivalence with DRE allows us to directly import exist- ing results in specific cases, including convergence-rate analyses, the selection of loss functions via Bregman-divergence minimization, and regularization techniques for flexible models, such as neural networks. Conversely, insights about the Riesz representer in debiased machine learning broaden the applications of direct density-ratio estimation methods. This paper consolidates our prior results in Kato (2025a) and Kato (2025b).",http://arxiv.org/abs/2511.04568v1,2025-11-06T17:25:05Z,Masahiro Kato,"**Unlocking New Connections in Machine Learning**

Researchers have discovered a surprising link between two powerful machine learning tools: Riesz regression and direct density-ratio estimation (DRE). Riesz regression is used to improve the accuracy of machine learning models by reducing bias in complex data analysis, particularly in estimating cause-and-effect relationships. Direct density-ratio estimation, on the other hand, is a technique used to compare the probability distributions of different groups.

The study reveals that Riesz regression and a specific type of DRE called least-squares importance fitting (LSIF) are essentially equivalent. This connection allows researchers to borrow techniques and results from one field and apply them to the other, leading to improved analysis and prediction capabilities.

The findings have significant implications:

* **Faster and more accurate analysis**: By leveraging existing results from DRE, researchers can improve the speed and accuracy of Riesz regression in certain cases.
* **New applications**: The connection also opens up new possibilities for applying DRE methods to a broader range of problems, including those involving complex data and causal relationships.

Overall, this research has the potential to advance the field of machine learning by providing new tools and techniques for analyzing complex data and making more accurate predictions."
cs.CV,Carousel: A High-Resolution Dataset for Multi-Target Automatic Image   Cropping,"Automatic image cropping is a method for maximizing the human-perceived quality of cropped regions in photographs. Although several works have proposed techniques for producing singular crops, little work has addressed the problem of producing multiple, distinct crops with aesthetic appeal. In this paper, we motivate the problem with a discussion on modern social media applications, introduce a dataset of 277 relevant images and human labels, and evaluate the efficacy of several single-crop models with an image partitioning algorithm as a pre-processing step. The dataset is available at https://github.com/RafeLoya/carousel.",http://arxiv.org/abs/2511.04680v1,2025-11-06T18:59:52Z,"Rafe Loya, Andrew Hamara, Benjamin Estell, Benjamin Kilpatrick, Andrew C. Freeman","Here's a summary of the research paper for a general audience:

**Improving Image Cropping for Social Media**

When sharing photos on social media, cropping them to focus on the best parts can make a big difference. However, current image cropping tools usually only produce one crop per photo. Researchers have now created a new dataset, called Carousel, which contains 277 images with multiple, high-quality crops. This dataset aims to help develop tools that can automatically produce multiple, visually appealing crops from a single photo.

The researchers behind Carousel hope that their work will improve how images are presented on social media platforms, making it easier to share engaging and well-composed photos. The dataset is now publicly available, allowing other researchers to build upon this work and develop more advanced image cropping tools."
cs.CV,GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human   and Object Interaction,"Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.",http://arxiv.org/abs/2511.04679v1,2025-11-06T18:59:33Z,"Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu","**Breakthrough in Humanoid Robot Interactions: GentleHumanoid Framework**

Imagine a future where humanoid robots can safely and naturally interact with humans and objects in their environment. A recent research paper introduces the GentleHumanoid framework, which enables humanoid robots to achieve upper-body compliance, allowing them to interact gently and smoothly with humans and objects.

**The Problem with Current Robots**

Current reinforcement learning policies for humanoid robots focus on precise movements and resist external forces, which can lead to stiff and unnatural interactions. Existing approaches to improve robot compliance have limitations, as they often focus on specific parts of the robot, such as the end effector, and prioritize resisting extreme forces over enabling smooth interactions.

**The GentleHumanoid Solution**

The GentleHumanoid framework integrates impedance control into a whole-body motion tracking policy, enabling the robot to adapt to different interaction scenarios. This approach uses a unified spring-based formulation to model both resistive and guiding contacts, ensuring consistent forces across the robot's upper body. The framework also includes adjustable force thresholds to ensure safety.

**Real-World Applications and Results**

The researchers tested the GentleHumanoid framework in simulation and on a Unitree G1 humanoid robot, evaluating its performance in tasks that require different levels of compliance, such as:

* Gentle hugging: The robot was able to hug a person gently, without applying too much pressure.
* Sit-to-stand assistance: The robot assisted a person in standing up from a seated position, providing smooth and natural support.
* Safe object manipulation: The robot handled objects with precision and care, avoiding sudden or jerky movements.

The results show that the GentleHumanoid framework consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. This breakthrough has significant implications for the development of humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.

**Implications and Future Directions**

The GentleHumanoid framework represents a significant step towards creating robots that can interact safely and naturally with humans. Future research directions may include:

* Expanding the framework to other parts of the robot's body
* Improving the robot's ability to adapt to different environments and scenarios
* Integrating the framework with other robotics applications, such as healthcare and manufacturing

Overall, the GentleHumanoid framework has the potential to transform the way humanoid robots interact with humans and objects, enabling more natural and safe collaboration in a variety of settings."
cs.CV,Tracking and Understanding Object Transformations,"Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at https://tubelet-graph.github.io.",http://arxiv.org/abs/2511.04678v1,2025-11-06T18:59:30Z,"Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan","**Understanding Object Changes Over Time**

Imagine watching an apple being sliced into pieces or a butterfly emerging from its cocoon. Tracking objects through these changes is crucial for understanding the world around us. However, current technology often struggles to keep track of objects after they undergo significant transformations.

Researchers have introduced a new task called ""Track Any State,"" which aims to track objects through changes while detecting and describing these changes. To tackle this problem, they've developed a system called TubeletGraph, which can recover missing objects after transformation and map out how object states evolve over time.

TubeletGraph works by identifying potentially overlooked tracks and determining whether they should be integrated based on their semantic meaning and proximity. It then generates a state graph that describes each observed transformation.

The results are promising, with TubeletGraph achieving top-notch tracking performance under transformations and demonstrating a deeper understanding of object changes. This technology has potential applications in areas such as temporal grounding and semantic reasoning for complex object transformations.

The researchers have also created a new benchmark dataset, VOST-TAS, and made their code and results publicly available, paving the way for further advancements in this field."
cs.CV,InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual   Generation,"We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.",http://arxiv.org/abs/2511.04675v1,2025-11-06T18:58:03Z,"Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan","Here's a summary of the research paper ""InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation"" for a general audience:

**Breakthrough in Video Generation**

Researchers have developed a new AI framework called InfinityStar, which can generate high-quality images and videos using a unified approach. This framework can create a wide range of visual content, including:

* Images from text descriptions
* Videos from text descriptions
* Videos from images
* Long, interactive videos

**What makes InfinityStar special?**

InfinityStar uses a novel approach that combines spatial and temporal dependencies in a single architecture. This allows it to generate high-quality videos quickly and efficiently. In fact, it can generate a 5-second, 720p video about 10 times faster than leading methods.

**How good is InfinityStar?**

InfinityStar has achieved impressive results, scoring 83.74 on a benchmark test (VBench), outperforming other autoregressive models and even some diffusion-based competitors. To put this into perspective, 720p is a high-definition video resolution commonly used in online video platforms.

**Why is this important?**

The development of InfinityStar marks a significant advancement in video generation technology. Its ability to generate high-quality videos quickly and efficiently has the potential to revolutionize various applications, such as:

* Video production for entertainment, advertising, and education
* Virtual reality and gaming
* Video editing and post-production

The researchers have made their code and models publicly available, which will facilitate further research and development in this area."
cs.CV,X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human   Demonstrations,"Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.",http://arxiv.org/abs/2511.04671v1,2025-11-06T18:56:30Z,"Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia","**Teaching Robots to Learn from Human Videos**

Imagine being able to teach a robot to perform tasks like picking up objects or assembling parts, simply by showing it a video of a human doing the task. This is a promising approach, as human videos can be recorded quickly and in large quantities. However, there's a catch: humans and robots have different physical capabilities, making it challenging to translate human movements directly into robot actions.

To overcome this challenge, researchers have developed a new method called X-Diffusion. This approach uses a technique called ""diffusion"" to gradually add noise to human actions, effectively ""erasing"" the differences between human and robot movements. By doing so, the robot can learn from human videos without trying to mimic the exact movements, which may not be physically possible for the robot.

The X-Diffusion method works by first training a classifier to distinguish between human and robot actions. Then, it adds noise to human actions until the classifier can't tell whether they're human or robot. The robot then learns from these noisy actions, which provide a general guide on how to perform the task.

The results are impressive: X-Diffusion improved the robot's performance on five manipulation tasks, achieving a 16% higher success rate compared to other methods. This breakthrough has the potential to enable robots to learn from large collections of human videos, making it easier to teach them new tasks and improve their performance."
cs.CV,Cambrian-S: Towards Spatial Supersensing in Video,"We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.",http://arxiv.org/abs/2511.04670v1,2025-11-06T18:55:17Z,"Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie","**Unlocking the Power of Spatial Supersensing: A New Frontier in Artificial Intelligence**

Imagine a world where computers can understand and interpret their surroundings like humans do. Researchers are working towards this goal by developing a concept called ""spatial supersensing,"" which enables machines to perceive and comprehend their environment in a more human-like way. In a recent study, the authors propose a new framework for spatial supersensing, which involves four key stages:

1. **Identifying objects**: recognizing what is being seen
2. **Tracking events**: keeping track of what happens over time
3. **Understanding 3D space**: inferring the layout of the environment
4. **Predicting what's next**: creating internal models to anticipate and make sense of the world

The researchers have created a new benchmark, VSI-SUPER, to test the capabilities of artificial intelligence (AI) models in spatial supersensing. They found that current models struggle with tasks that require long-term memory and understanding of 3D space. To address this, they developed Cambrian-S, a new AI model that achieves significant improvements in spatial supersensing.

However, the researchers also found that simply increasing the amount of data used to train the model is not enough to achieve true spatial supersensing. Instead, they propose a new approach called ""predictive sensing,"" which enables models to anticipate and make predictions about their environment. This approach has shown promising results, outperforming existing models and paving the way for further advancements in spatial supersensing.

The development of spatial supersensing has the potential to revolutionize various applications, such as robotics, autonomous vehicles, and healthcare. For example, robots equipped with spatial supersensing capabilities could navigate complex environments with ease, while autonomous vehicles could better anticipate and respond to changing road conditions. In healthcare, spatial supersensing could enable more accurate diagnosis and treatment of diseases.

In summary, the study highlights the importance of spatial supersensing in developing more intelligent and human-like machines. By creating new benchmarks and approaches, researchers can drive progress in this field and unlock new possibilities for AI applications."
cs.CV,SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding,"Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.",http://arxiv.org/abs/2511.04668v1,2025-11-06T18:53:31Z,"Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie","**Advancing Video Understanding with Simulated Training Data**

Researchers have made a breakthrough in improving the spatial reasoning abilities of artificial intelligence (AI) models that understand videos. These models, known as multimodal language models, are great at grasping the overall meaning of videos but struggle with understanding spatial relationships and changes over time.

The main challenge is that training these models requires a large amount of video data with precise annotations, which is difficult and expensive to obtain. To overcome this, the researchers developed a framework called SIMS-V, which uses 3D simulators to generate synthetic video data with precise spatial annotations.

By analyzing the generated data, the researchers identified a small set of key questions that, when used to train the AI model, enabled it to develop strong spatial reasoning abilities. These questions focused on:

1. Measuring distances and sizes
2. Understanding perspectives and viewpoints
3. Tracking objects over time

Surprisingly, using just these three question types proved more effective than using a broader range of questions. The researchers used this approach to fine-tune a large AI model with 7 billion parameters on just 25,000 simulated examples. The results were impressive:

* The model outperformed a much larger model with 72 billion parameters
* It achieved competitive performance with proprietary models on challenging real-world spatial reasoning benchmarks

The SIMS-V framework offers a efficient and effective way to improve the spatial reasoning abilities of AI models, with potential applications in areas like robotics, autonomous vehicles, and more."
cs.CV,Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation   of Soft-Body Interactions,"Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",http://arxiv.org/abs/2511.04665v1,2025-11-06T18:52:08Z,"Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li","**Advancing Robot Learning: A New Way to Test Robot Policies in a Virtual World**

Imagine a world where robots can learn and improve their skills in a virtual environment before being tested in the real world. This is now a step closer to reality, thanks to a new research paper on ""Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions"".

The researchers have developed a framework that allows them to create a digital twin of a real-world scene, including soft and flexible objects like plush toys, ropes, and blocks. This digital twin is created from videos of the real world and uses advanced rendering techniques to simulate how objects interact with each other.

The goal of this research is to make it easier and more efficient to test and evaluate robot policies, which are the instructions that tell a robot what to do. Currently, testing robot policies in the real world can be time-consuming, expensive, and difficult to reproduce. By using a virtual environment, researchers can test policies in a more controlled and repeatable way.

The researchers tested their framework on several tasks, including packing a plush toy, routing a rope through a series of obstacles, and pushing a block. They found that the results from the virtual environment closely matched the results from the real world, suggesting that their approach is accurate and reliable.

This breakthrough has the potential to accelerate the development of robots that can interact with and manipulate soft and flexible objects, which is a challenging task. The researchers' approach could enable robots to learn and improve their skills more quickly and efficiently, which could lead to advancements in areas like manufacturing, healthcare, and logistics.

**Key Takeaways:**

* A new framework for testing robot policies in a virtual environment
* Enables the creation of digital twins of real-world scenes with soft and flexible objects
* Allows for more efficient and reproducible testing of robot policies
* Could accelerate the development of robots that can interact with and manipulate soft and flexible objects

**Learn More:** Visit the project's website at https://real2sim-eval.github.io/ to learn more about this research and its potential applications."
cs.CV,"Benchmark Designers Should ""Train on the Test Set"" to Expose Exploitable   Non-Visual Shortcuts","Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.",http://arxiv.org/abs/2511.04655v1,2025-11-06T18:43:21Z,"Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie","Here's a summary of the research paper for a general audience:

**The Problem: AI Models Can Cheat on Tests**

Imagine you're taking a test, but instead of actually understanding the material, you just memorize the answers or look for easy patterns to get by. That's what's happening with some artificial intelligence (AI) models, specifically those that combine language and vision, like Multimodal Large Language Models (MLLMs). These models are being evaluated on benchmarks, or tests, that are meant to assess their ability to understand visual information. However, researchers have found that many models can ace these tests without actually understanding what they're seeing.

**The Solution: Make the Tests Tougher**

To fix this problem, the researchers propose that benchmark designers should ""train on the test set"" - essentially, try to game their own tests - to identify and remove biases and patterns that can be exploited by AI models. This involves using diagnostic tools to analyze the test set and identify areas where models can cheat, and then debiasing the test set to make it more challenging.

**The Researchers' Approach**

The researchers developed two tools to help with this process:

1. **Test-set Stress-Test (TsT)**: This involves training a powerful language model on only the textual inputs of the test set to see how well it can perform without actually looking at the visual information.
2. **Iterative Bias Pruning (IBP)**: This involves filtering out high-bias samples from the test set to make it more challenging for AI models.

**The Results**

The researchers applied their framework to four benchmarks and found that they were all susceptible to non-visual biases. They then created a debiased version of one of the benchmarks, VSI-Bench-Debiased, which was more challenging for AI models and required a better understanding of visual information.

**The Takeaway**

The study highlights the importance of designing robust benchmarks that truly test the abilities of AI models. By making the tests tougher and more challenging, researchers can ensure that AI models are actually learning and understanding what they're seeing, rather than just exploiting easy patterns."
cs.CV,Polarization-resolved imaging improves eye tracking,"Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization--filter--array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10--16\% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light--tissue polarization effects to practical gains in human--computer interaction and position PET as a simple, robust sensing modality for future wearable devices.",http://arxiv.org/abs/2511.04652v1,2025-11-06T18:42:09Z,"Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi, Fernando Alamos, Gauresh Pandit, Christopher Mei, Ali Behrooz, Ramin Mirjalili, Dave Stronks, Alexander Fix, Dmitri Model","**Advancing Eye Tracking Technology: A Breakthrough in Human-Computer Interaction**

Researchers have made a significant improvement to eye tracking technology, a crucial component in human-computer interaction. By using a special type of camera and near-infrared light, they have developed a system called Polarization-Enabled Eye Tracking (PET). This system captures not only the intensity of light reflected from the eye but also its polarization state, which provides additional useful information.

The study, conducted with 346 participants, showed that PET significantly outperforms traditional eye tracking methods. Specifically, PET reduced errors in gaze tracking by 10-16% under various conditions, including when eyelids partially covered the eyes, when the distance between the eye and the device changed, and when the pupil size varied.

This breakthrough has important implications for the development of wearable devices, such as smart glasses or virtual reality headsets, which rely on accurate eye tracking to function effectively. The PET system offers a simple, robust, and reliable way to track eye movements, enabling more seamless and intuitive interactions between humans and computers."
cs.CV,NovisVQ: A Streaming Convolutional Neural Network for No-Reference   Opinion-Unaware Frame Quality Assessment,"Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.",http://arxiv.org/abs/2511.04628v1,2025-11-06T18:23:55Z,"Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano","**Improving Video Quality Assessment with AI**

Assessing the quality of videos is crucial for various computer vision tasks, but existing methods have limitations. Some require a perfect reference video, while others rely on expensive human feedback. A new approach, called NovisVQ, overcomes these challenges by using a streaming convolutional neural network that evaluates video quality without needing a reference video or human opinion.

NovisVQ uses a synthetic dataset to train a model that predicts video quality metrics, such as clarity and similarity, directly from the degraded video. The model takes into account the temporal context of the video, which is essential for object detection and other computer vision tasks.

The results show that NovisVQ outperforms existing image-based methods and correlates well with full-reference metrics, which are considered more accurate. This approach has the potential to improve video quality assessment in real-world vision systems, enabling more efficient and accurate evaluation of video quality."
cs.CV,Building Trust in Virtual Immunohistochemistry: Automated Assessment of   Image Quality,"Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.",http://arxiv.org/abs/2511.04615v1,2025-11-06T18:09:09Z,"Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen","**Building Trust in Virtual Immunohistochemistry: A New Approach to Image Quality Assessment**

Imagine a world where doctors can quickly and accurately diagnose diseases like cancer using virtual images of tissue samples. This is the promise of virtual immunohistochemistry (IHC), a technology that uses artificial intelligence to generate virtual images of tissue samples stained with special dyes. However, one major challenge remains: ensuring that these virtual images are accurate and reliable.

Currently, researchers use metrics that evaluate the similarity between virtual and real images, but these metrics don't necessarily measure the accuracy of the virtual IHC staining. To address this issue, a team of researchers has developed a new framework that automatically assesses the quality of virtual IHC images.

**The Problem with Current Metrics**

The researchers found that conventional metrics, such as Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), don't accurately reflect the quality of virtual IHC images. These metrics focus on the overall similarity between images, but they don't account for the accuracy of the IHC staining.

**The New Framework**

The researchers' framework uses a technique called color deconvolution to identify pixels that are stained brown (indicating a positive IHC result) in both real and virtual images. They then compare the accuracy of the virtual IHC images to the real images using metrics such as Dice, IoU, and Hausdorff distance. These metrics measure the accuracy of the virtual IHC staining at the pixel level, without requiring expert manual annotations.

**Key Findings**

The study found that:

* Conventional image fidelity metrics correlate poorly with stain accuracy and pathologist assessment.
* Paired models, such as PyramidPix2Pix and AdaptiveNCE, achieve higher stain accuracy than unpaired diffusion- and GAN-based models.
* Evaluating virtual IHC images at the whole-slide level (rather than just patches) reveals performance declines that may not be visible in patch-based evaluations.

**Implications**

The development of this framework is a critical step towards translating virtual IHC into routine use by pathologists. By providing a reproducible and accuracy-grounded approach to assessing virtual IHC image quality, this research helps build trust in this technology and paves the way for its use in clinical settings. Ultimately, this could lead to faster and more accurate diagnoses, improved patient outcomes, and more efficient use of healthcare resources."
cs.CV,PixCLIP: Achieving Fine-grained Visual Language Understanding via   Any-granularity Pixel-Text Alignment Learning,"While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model's fine-grained vision-language alignment. However, the inherent token length limitation of CLIP's text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP's original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.",http://arxiv.org/abs/2511.04601v1,2025-11-06T17:54:12Z,"Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang","**Breakthrough in Visual Language Understanding: PixCLIP**

Imagine being able to understand images and text together, at a very detailed level. This is the goal of a new AI model called PixCLIP, which has made significant progress in this area. PixCLIP builds on the success of a previous model called CLIP, which can understand images and text, but not at a very fine-grained level.

The researchers behind PixCLIP wanted to improve CLIP's ability to align images and text at a detailed level, such as understanding which specific parts of an image correspond to specific words or phrases in a text description. To do this, they developed a new framework that can process both visual and textual information at a more detailed level.

**Key Innovations:**

1. **Automated annotation pipeline**: The researchers created a pipeline that can automatically generate detailed text descriptions for images, which are then aligned with specific parts of the image.
2. **New dataset**: Using this pipeline, they created a large dataset of nearly 1.5 million samples, which is a significant resource for training AI models.
3. **Three-branch pixel-text alignment learning framework**: PixCLIP uses a novel framework that enables fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity.

**What does this mean?**

PixCLIP's breakthroughs in pixel-level interaction and handling long-form texts have achieved state-of-the-art performance in visual language understanding tasks. This means that PixCLIP can:

* Understand images and text at a very detailed level
* Handle long text descriptions, which is useful for applications such as image captioning and visual question answering
* Align specific parts of an image with specific words or phrases in a text description

The potential applications of PixCLIP are vast, ranging from improving image search and recommendation systems to enabling more accurate and informative image captioning and visual question answering."
cs.CV,UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for   Dynamic Driving Scene Reconstruction,"Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.",http://arxiv.org/abs/2511.04595v1,2025-11-06T17:49:39Z,"Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang","Here's a summary of the research paper for a general audience:

**Title:** UniSplat: A New Way to Reconstruct Dynamic Scenes for Autonomous Driving

**What it's about:** Imagine you're driving a self-driving car. To navigate safely, the car needs to understand its surroundings, including other cars, pedestrians, and road conditions. One way to do this is by using cameras to capture the scene and then reconstructing it in 3D. However, this can be tricky, especially when the cameras are sparse (not many of them) and don't overlap, and when the scene is dynamic (things are moving around).

**The breakthrough:** Researchers have developed a new framework called UniSplat, which can reconstruct dynamic scenes in 3D more accurately and efficiently. UniSplat uses a clever technique called a ""3D latent scaffold"" to capture the scene's geometry and semantics. This allows it to fuse information from multiple camera views and time frames, creating a more complete and detailed picture of the scene.

**How it works:** UniSplat has two key components:

1. **Fusion mechanism:** This allows UniSplat to combine information from different camera views and time frames, ensuring that the reconstructed scene is consistent and accurate.
2. **Dual-branch decoder:** This generates a detailed and dynamic reconstruction of the scene, including moving objects and static background.

**The results:** Tests on real-world datasets show that UniSplat outperforms existing methods in reconstructing dynamic scenes, even when the camera views are limited or don't overlap. This technology has the potential to improve the safety and reliability of autonomous driving systems.

**In simple terms:** UniSplat is a new way to help self-driving cars understand their surroundings by reconstructing 3D scenes more accurately and efficiently. It's a significant step forward in making autonomous driving safer and more reliable."
cs.CV,Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration   from a Baseline Paper,"Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",http://arxiv.org/abs/2511.04583v1,2025-11-06T17:37:49Z,"Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa","**Breakthrough in AI-Driven Scientific Research: Jr. AI Scientist**

Imagine a future where artificial intelligence (AI) can assist scientists in conducting research, analyzing data, and even writing papers. A recent study introduces Jr. AI Scientist, a cutting-edge AI system that mimics the workflow of a junior researcher. Given a baseline paper, Jr. AI Scientist can analyze its limitations, come up with new ideas, test them through experiments, and write a paper with the results.

**What makes Jr. AI Scientist special?**

Unlike previous AI systems, Jr. AI Scientist follows a well-defined research workflow and can handle complex tasks, leading to scientifically valuable contributions. In evaluations, Jr. AI Scientist generated papers that received higher review scores than existing fully automated systems.

**But what are the risks?**

While Jr. AI Scientist shows promise, the study also identifies important limitations and potential risks. These include concerns about the accuracy and reliability of AI-generated research, the potential for bias, and the need for human oversight. The authors emphasize the need for further research to address these challenges and ensure that AI Scientist systems are trustworthy and sustainable.

**What's next?**

The development of Jr. AI Scientist and its risk report aims to deepen our understanding of the current progress and risks in AI-driven scientific research. As AI continues to play a larger role in scientific discovery, it's essential to ensure that these systems are transparent, reliable, and beneficial to society."
cs.CV,Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm,"""Thinking with Text"" and ""Thinking with Images"" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce ""Thinking with Video"", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions ""thinking with video"" as a unified multimodal reasoning paradigm.",http://arxiv.org/abs/2511.04570v1,2025-11-06T17:25:23Z,"Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu","**Unlocking a New Way of Thinking: ""Thinking with Video""**

Imagine being able to understand and generate information in a way that combines the strengths of text, images, and videos. Researchers have been exploring ways to improve the reasoning abilities of artificial intelligence (AI) models, and a new approach called ""Thinking with Video"" shows great promise.

Currently, AI models use either text or images to reason and make decisions. However, these approaches have limitations. Images, for example, can only capture a single moment in time and can't represent dynamic processes or continuous changes. To overcome these limitations, researchers introduced ""Thinking with Video,"" a new paradigm that uses video generation models to bridge the gap between visual and textual reasoning.

In a recent study, researchers developed a new benchmark called Video Thinking Benchmark (VideoThinkBench) to test the abilities of video generation models. They evaluated a model called Sora-2 and found that it was able to reason and make decisions in a unified way, combining the strengths of text and images.

The results were impressive: Sora-2 performed well on both vision-centric tasks, such as solving puzzles, and text-centric tasks, such as math problems and reading comprehension. In some cases, Sora-2 even outperformed state-of-the-art models that use text or images alone.

The study suggests that ""Thinking with Video"" has the potential to become a unified multimodal reasoning paradigm, enabling AI models to understand and generate information in a more comprehensive and dynamic way. This could have significant implications for applications such as robotics, education, and healthcare, where AI models need to be able to understand and interact with complex, dynamic environments."
cs.CV,Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic   Alignment,"Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.",http://arxiv.org/abs/2511.04555v1,2025-11-06T17:07:49Z,"Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao","**Breakthrough in Robotics: Introducing Evo-1, a Lightweight and Efficient Model**

Imagine a robot that can understand and respond to its environment, much like humans do. Researchers have made significant progress in developing a new type of artificial intelligence (AI) model called Vision-Language-Action (VLA) that enables robots to perform various tasks by combining visual perception, language understanding, and control.

However, existing VLA models are often large, complex, and require extensive training data, making them difficult to deploy in real-world applications. To address this challenge, a team of researchers has developed Evo-1, a lightweight VLA model that achieves state-of-the-art performance while being more efficient and deployable.

**Key Innovations:**

* **Efficient Architecture:** Evo-1 uses a novel architecture that integrates visual, language, and action components more effectively, reducing computational costs and improving performance.
* **Two-Stage Training:** The researchers developed a two-stage training paradigm that preserves the representations of the visual-language backbone, leading to better generalization and performance.

**Impressive Results:**

* Evo-1 outperforms existing models on several benchmarks, achieving a 12.4% and 6.9% improvement on Meta-World and RoboTwin, respectively.
* In real-world evaluations, Evo-1 achieves a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods.

**Impact:**

The development of Evo-1 marks a significant step towards creating more efficient and deployable VLA models for robotics. With its lightweight design and impressive performance, Evo-1 has the potential to enable robots to perform complex tasks in real-world applications, such as manufacturing, healthcare, and service industries. The researchers have made their code, data, and model weights publicly available, facilitating further research and innovation in this field."
cs.CV,Learning from Single Timestamps: Complexity Estimation in Laparoscopic   Cholecystectomy,"Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.",http://arxiv.org/abs/2511.04525v1,2025-11-06T16:39:55Z,"Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov","**Breakthrough in Surgical Complexity Assessment**

A team of researchers has developed a new artificial intelligence (AI) system, called STC-Net, that can accurately assess the complexity of a common surgical procedure called Laparoscopic Cholecystectomy (LC). The system analyzes videos of the surgery to estimate the level of inflammation, which is crucial in predicting the risk of complications and operative time.

**The Challenge**

Currently, assessing surgical complexity relies on manual evaluation of videos, which is time-consuming and subjective. The Parkland Grading Scale (PGS) is a widely used framework for stratifying inflammation severity, but its automation in surgical videos has been a challenge.

**The Solution**

STC-Net is a novel AI framework that can operate directly on full videos, without requiring manual curation or trimming. It uses a single timestamp, or a brief moment in the video, to estimate the surgical complexity. The system consists of three modules: temporal localization, window proposal, and grading. It also uses a new loss formulation that combines hard and soft localization objectives and background-aware grading supervision.

**The Results**

The researchers tested STC-Net on a large dataset of 1,859 LC videos and achieved an accuracy of 62.11% and an F1-score of 61.42%. This outperforms existing methods that require manual trimming or static images. The results highlight the effectiveness of weak supervision for surgical complexity assessment.

**The Impact**

The development of STC-Net has significant implications for post-operative analysis and surgical training. It can help surgeons and medical professionals to:

* Better predict and prepare for complex surgeries
* Improve patient outcomes and reduce complications
* Enhance surgical training and education

Overall, STC-Net represents a promising approach for automated surgical complexity estimation, and its applications have the potential to improve surgical care and outcomes."
cs.CV,THEval. Evaluation Framework for Talking Head Video Generation,"Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.",http://arxiv.org/abs/2511.04520v2,2025-11-06T16:34:10Z,"Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva","Here's a summary of the research paper for a general audience:

**Evaluating Talking Head Videos: A New Framework**

Talking head videos, like those used in virtual meetings or digital avatars, have become increasingly realistic thanks to advances in video generation technology. However, there's a problem: we don't have good ways to measure how realistic or natural these videos are. Currently, evaluation relies on limited metrics and human opinions, which can be subjective.

To address this issue, researchers have proposed a new evaluation framework called THEval. This framework uses 8 metrics to assess talking head videos across three key areas: quality, naturalness, and synchronization. The metrics focus on details like head movements, mouth and eyebrow expressions, and face quality.

The researchers tested their framework on 85,000 videos generated by 17 state-of-the-art models. They found that while many models are good at synchronizing lip movements, they struggle to generate natural expressions and avoid visual artifacts.

The goal of THEval is to provide a standardized way to evaluate and improve talking head video generation. The researchers have curated a new dataset and will publicly release their code, dataset, and leaderboards, which will be regularly updated to reflect progress in the field. This will help drive innovation and improvement in talking head video generation."
cs.CV,$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography   via Implicit Neural Representation,"Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.",http://arxiv.org/abs/2511.04510v1,2025-11-06T16:28:30Z,"Shihan Zhao, Jianru Zhang, Yanan Wu, Linlin Li, Siyuan Shen, Xingjun Zhu, Guoyan Zheng, Jiahua Jiang, Wuwei Ren","**Breakthrough in Medical Imaging: A New Method for Accurate 3D Visualization**

Scientists have developed a novel approach called $μ$NeuFMT, which improves the accuracy of Fluorescence Molecular Tomography (FMT), a non-invasive imaging technique used to visualize fluorescent probes in 3D. FMT is promising for medical applications, such as guiding surgery, but its accuracy has been limited by the complexity of light interactions with tissues.

The new method, $μ$NeuFMT, uses artificial intelligence to adapt to the unique optical properties of each patient's tissue, eliminating the need for prior knowledge of these properties. By jointly optimizing the fluorescence distribution and optical properties during reconstruction, $μ$NeuFMT provides more accurate and robust results.

**Key Benefits:**

* More accurate 3D visualization of fluorescent probes
* Adaptable to diverse tissue types and properties
* No need for precise prior knowledge of tissue optics
* Outperforms conventional and supervised deep learning approaches

**Implications:**

* Improved accuracy and reliability in molecular imaging
* Potential to enhance fluorescence-guided surgery and other medical applications
* New paradigm for robust and accurate FMT reconstruction

This innovation has the potential to significantly impact medical imaging and pave the way for more reliable and accurate diagnoses and treatments."
cs.AI,X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human   Demonstrations,"Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.",http://arxiv.org/abs/2511.04671v1,2025-11-06T18:56:30Z,"Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia","**Teaching Robots to Learn from Human Videos**

Imagine being able to teach a robot to perform tasks like picking up objects or cooking by simply showing it a human doing it. This is now a step closer to reality. Researchers have developed a new method called X-Diffusion, which allows robots to learn from human videos, even though humans and robots have different physical abilities.

The challenge is that humans and robots move and interact with objects in different ways. For example, a human can easily move their fingers in a way that a robot can't. To overcome this, X-Diffusion uses a technique called ""diffusion"" to gradually add noise to the human movements, making them more compatible with a robot's abilities.

The X-Diffusion framework works by first training a classifier to distinguish between human and robot movements. Then, it adds noise to the human movements until the classifier can't tell if they're human or robot. This noisy movement is then used to train the robot.

The results are impressive: X-Diffusion improved the robot's success rate by 16% across five different tasks, compared to other methods. This breakthrough could enable robots to learn from large collections of human videos, making it easier to teach them new tasks.

**In simple terms:** X-Diffusion is a new way to teach robots to learn from human videos by adding noise to the movements, making them more compatible with a robot's abilities. This method has shown promising results in improving a robot's ability to perform tasks like manipulation and interaction with objects."
cs.AI,VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical   Consistency Checks,"LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.",http://arxiv.org/abs/2511.04662v1,2025-11-06T18:50:08Z,"Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala","Here's a summary of the research paper for a general audience:

**Improving AI Reasoning with VeriCoT**

Large Language Models (LLMs) are great at solving complex problems by breaking them down into smaller steps, but they often can't verify if their own reasoning is correct. This can lead to flawed conclusions, which is a major concern in high-stakes situations like healthcare, law, or finance.

To address this issue, researchers have developed VeriCoT, a new method that checks the logical consistency of AI reasoning. VeriCoT works by:

1. Translating the AI's step-by-step reasoning into formal logical statements
2. Identifying the assumptions and premises that support each step
3. Using automated solvers to verify if the logical statements are valid

Experiments on various datasets showed that VeriCoT is effective in detecting flawed reasoning and predicting the accuracy of the final answer. The researchers also used VeriCoT to improve AI performance by:

* Allowing AI to reflect on its own reasoning and correct mistakes
* Fine-tuning AI models on datasets that have been verified by VeriCoT
* Optimizing AI training with rewards based on logical consistency

Overall, VeriCoT has the potential to increase trust in AI systems by ensuring that their reasoning is sound and logical. This advancement can lead to more reliable and accurate AI applications in various fields."
cs.AI,DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for   Embodied LLM-Based Multi-Agent Collaboration,"Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",http://arxiv.org/abs/2511.04646v1,2025-11-06T18:37:18Z,"Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong","Here's a summary of the research paper for a general audience:

**Title:** DR. WELL: A New Framework for Cooperative Robots and AI Agents to Work Together

**Imagine:** You're working on a project with a team of robots or AI agents to achieve a common goal, like moving blocks into place. However, each robot or agent only has partial information and limited communication with the others. How can they work together efficiently?

**The Problem:** Traditional methods for coordinating robots or AI agents often fail because small mistakes in timing or movement can lead to big conflicts.

**The Solution:** Researchers have developed a new framework called DR. WELL, which combines symbolic planning (thinking in abstract terms) with learning and reasoning. This framework allows agents to work together in a decentralized way, making joint decisions and adapting to changing situations.

**How it Works:** DR. WELL uses a two-phase negotiation protocol, where agents propose roles and then commit to a joint plan under consensus and environment constraints. Each agent then generates and executes a symbolic plan for its role, without revealing detailed trajectories. A shared world model encodes the current state and is updated as agents act, allowing them to reason over symbolic plans rather than raw trajectories.

**The Benefits:** DR. WELL enables higher-level operations that are reusable, synchronizable, and interpretable. In experiments on cooperative block-push tasks, agents adapted across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency.

**The Result:** The DR. WELL framework improves cooperation and efficiency among robots and AI agents, allowing them to work together more effectively to achieve complex goals. This research has the potential to advance the development of more sophisticated and autonomous multi-agent systems."
cs.AI,Addressing divergent representations from causal interventions on neural   networks,"A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two classes of such divergences: `harmless' divergences that occur in the null-space of the weights and from covariance within behavioral decision boundaries, and `pernicious' divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we modify the Counterfactual Latent (CL) loss from Grant (2025) that regularizes interventions to remain closer to the natural distributions, reducing the likelihood of harmful divergences while preserving the interpretive power of interventions. Together, these results highlight a path towards more reliable interpretability methods.",http://arxiv.org/abs/2511.04638v1,2025-11-06T18:32:34Z,"Satchel Grant, Simon Jerome Han, Alexa Tartaglini, Christopher Potts","**Unlocking the Secrets of AI: A New Approach to Understanding Neural Networks**

Imagine trying to understand how a complex machine works by poking and prodding it to see how it reacts. That's basically what researchers do when they try to interpret how artificial intelligence (AI) models, like neural networks, make decisions. One popular approach is to intervene on the model's internal representations to see what they encode. However, a new study asks: do these interventions create artificial or ""out-of-distribution"" representations that don't reflect how the model normally works?

The researchers found that common intervention techniques can indeed shift the model's internal representations away from their natural state. They identified two types of divergences: ""harmless"" ones that don't affect the model's behavior and ""pernicious"" ones that can activate hidden pathways and cause changes in behavior.

To mitigate these pernicious divergences, the researchers modified a technique called the Counterfactual Latent (CL) loss. This modified approach helps interventions stay closer to the model's natural distribution, reducing the likelihood of harmful divergences while preserving the interpretive power of interventions.

In simple terms, this study highlights the importance of ensuring that interventions on AI models are reliable and accurately reflect how the models work in their natural state. By developing more reliable interpretability methods, researchers can gain a deeper understanding of how AI models make decisions, which is crucial for building trust in these models and ensuring they are used responsibly."
cs.AI,Question the Questions: Auditing Representation in Online Deliberative   Processes,"A central feature of many deliberative processes, such as citizens' assemblies and deliberative polls, is the opportunity for participants to engage directly with experts. While participants are typically invited to propose questions for expert panels, only a limited number can be selected due to time constraints. This raises the challenge of how to choose a small set of questions that best represent the interests of all participants. We introduce an auditing framework for measuring the level of representation provided by a slate of questions, based on the social choice concept known as justified representation (JR). We present the first algorithms for auditing JR in the general utility setting, with our most efficient algorithm achieving a runtime of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number of proposed questions. We apply our auditing methods to historical deliberations, comparing the representativeness of (a) the actual questions posed to the expert panel (chosen by a moderator), (b) participants' questions chosen via integer linear programming, (c) summary questions generated by large language models (LLMs). Our results highlight both the promise and current limitations of LLMs in supporting deliberative processes. By integrating our methods into an online deliberation platform that has been used for over hundreds of deliberations across more than 50 countries, we make it easy for practitioners to audit and improve representation in future deliberations.",http://arxiv.org/abs/2511.04588v1,2025-11-06T17:45:12Z,"Soham De, Lodewijk Gelauff, Ashish Goel, Smitha Milli, Ariel Procaccia, Alice Siu","**Ensuring Fair Representation in Online Deliberations**

Imagine a town hall meeting where citizens get to ask questions to experts. But with limited time, only a few questions can be chosen. How do we ensure that the selected questions represent the interests of all participants?

Researchers have developed a new framework to evaluate how well a set of questions represents the diverse views of participants. They created algorithms to analyze the questions proposed by participants and determine how well they reflect the group's interests.

The researchers tested their methods on past deliberations and compared the results to:

1. Questions chosen by a moderator
2. Questions selected using a mathematical optimization technique
3. Summary questions generated by artificial intelligence (large language models)

The findings show that AI can be helpful in supporting deliberative processes, but there are still limitations. The good news is that the researchers have integrated their methods into an online platform used for hundreds of deliberations worldwide, making it easier for practitioners to ensure fair representation in future discussions.

This work aims to improve the way we gather and represent people's opinions, leading to more inclusive and informed decision-making processes."
cs.AI,Are We Asking the Right Questions? On Ambiguity in Natural Language   Queries for Tabular Data Analysis,"Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.",http://arxiv.org/abs/2511.04584v1,2025-11-06T17:39:18Z,"Daniel Gomm, Cornelius Wolff, Madelon Hulsebos","**The Future of Asking Questions: How to Improve Interactions with Computers**

Imagine asking a computer a question about a spreadsheet, like ""What's the average salary?"" But what if the computer doesn't quite understand what you mean? That's because natural language queries, like those we use to ask questions, can be ambiguous. Researchers have traditionally tried to eliminate this ambiguity, but a new study suggests a different approach.

Instead of trying to fix ambiguity, the researchers propose that we view it as an opportunity for cooperation between the user and the computer. They've developed a framework to distinguish between ""cooperative"" queries, which can be easily understood, and ""uncooperative"" queries, which are too vague.

The study analyzed 15 popular datasets of natural language queries and found that many of them mix both types of queries. This makes it difficult to evaluate how well computers can understand and execute queries.

The researchers' new perspective on ambiguity has important implications for designing and evaluating computer systems that interact with humans. By embracing cooperation and ambiguity, we can create more effective and user-friendly interfaces that help us get the answers we need from computers. This shift in thinking could lead to significant advances in areas like data analysis and artificial intelligence."
cs.AI,Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration   from a Baseline Paper,"Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",http://arxiv.org/abs/2511.04583v1,2025-11-06T17:37:49Z,"Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa","**Breakthrough in AI-Driven Scientific Research: Jr. AI Scientist**

Imagine a future where artificial intelligence (AI) systems can conduct scientific research on their own, analyzing data, formulating hypotheses, and even writing papers. A recent study has made significant progress towards making this vision a reality. Researchers have developed a state-of-the-art AI system called Jr. AI Scientist, which can mimic the workflow of a junior researcher.

**How it works**

Jr. AI Scientist starts with a ""baseline paper"" provided by a human mentor. It then analyzes the paper's limitations, comes up with new ideas to improve it, and designs experiments to test these ideas. The system can even write a paper with its findings. What's impressive is that Jr. AI Scientist can handle complex tasks, such as working with multiple files and coding.

**The results**

The researchers tested Jr. AI Scientist by having it submit papers to a special venue called Agents4Science, where AI-driven scientific contributions are reviewed. The results showed that Jr. AI Scientist produced papers that received higher review scores than existing fully automated systems.

**The limitations and risks**

However, the researchers also identified some important limitations and potential risks. For example, Jr. AI Scientist may not always be able to critically evaluate its own results or consider the broader implications of its findings. The researchers also reported various risks that they encountered during development, highlighting the need for careful consideration and further research.

**What's next**

The development of Jr. AI Scientist marks an exciting step forward in AI-driven scientific research. However, the researchers emphasize that there is still much work to be done to ensure that these systems are trustworthy, reliable, and beneficial to society. By understanding the current capabilities and risks of AI Scientist systems, we can work towards a future where AI and humans collaborate to advance scientific knowledge and progress."
cs.AI,Integrating Temporal and Structural Context in Graph Transformers for   Relational Deep Learning,"In domains such as healthcare, finance, and e-commerce, the temporal dynamics of relational data emerge from complex interactions-such as those between patients and providers, or users and products across diverse categories. To be broadly useful, models operating on these data must integrate long-range spatial and temporal dependencies across diverse types of entities, while also supporting multiple predictive tasks. However, existing graph models for relational data primarily focus on spatial structure, treating temporal information merely as a filtering constraint to exclude future events rather than a modeling signal, and are typically designed for single-task prediction. To address these gaps, we introduce a temporal subgraph sampler that enhances global context by retrieving nodes beyond the immediate neighborhood to capture temporally relevant relationships. In addition, we propose the Relational Graph Perceiver (RGP), a graph transformer architecture for relational deep learning that leverages a cross-attention-based latent bottleneck to efficiently integrate information from both structural and temporal contexts. This latent bottleneck integrates signals from different node and edge types into a common latent space, enabling the model to build global context across the entire relational system. RGP also incorporates a flexible cross-attention decoder that supports joint learning across tasks with disjoint label spaces within a single model. Experiments on RelBench, SALT, and CTU show that RGP delivers state-of-the-art performance, offering a general and scalable solution for relational deep learning with support for diverse predictive tasks.",http://arxiv.org/abs/2511.04557v1,2025-11-06T17:08:21Z,"Divyansha Lachi, Mahmoud Mohammadi, Joe Meyer, Vinam Arora, Tom Palczewski, Eva L. Dyer","**Unlocking the Power of Relationships Over Time**

Imagine trying to understand complex relationships between people, products, or services over time. This is a common challenge in fields like healthcare, finance, and e-commerce. For instance, how do patients interact with different doctors and treatments over time? How do users engage with various products across different categories on an e-commerce platform?

Researchers have developed a new approach to tackle this challenge by creating a more advanced artificial intelligence (AI) model. This model, called Relational Graph Perceiver (RGP), can analyze relationships between different entities (like people, products, or services) and how they change over time.

The key innovation of RGP is its ability to integrate two types of information:

1. **Structural context**: This refers to the relationships between different entities, such as a patient's relationships with doctors and treatments.
2. **Temporal context**: This refers to how these relationships change over time, such as a patient switching doctors or trying new treatments.

RGP uses a novel technique called temporal subgraph sampling to capture temporally relevant relationships. It also employs a cross-attention-based latent bottleneck to efficiently integrate information from both structural and temporal contexts. This allows the model to build a comprehensive understanding of the relationships and make accurate predictions.

The benefits of RGP are:

* **Improved accuracy**: RGP outperforms existing models in predicting outcomes in complex relational systems.
* **Flexibility**: RGP can handle multiple predictive tasks, such as forecasting patient outcomes, recommending products, or identifying potential financial risks.
* **Scalability**: RGP can be applied to large and diverse datasets, making it a general and scalable solution for relational deep learning.

The researchers tested RGP on several benchmark datasets and found that it delivers state-of-the-art performance. This breakthrough has the potential to transform various industries by enabling more accurate predictions, better decision-making, and improved outcomes."
cs.AI,Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse   Sensing Approach,"Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource.",http://arxiv.org/abs/2511.04556v1,2025-11-06T17:08:19Z,"Zihang Ding, Kun Zhang","**Summary: Optimizing Sensor Placement in Urban Storm Sewers**

Urban flooding caused by heavy rainfall is becoming more frequent and widespread. To mitigate this issue, researchers need to monitor and predict flooding in cities, but limited resources (time, budget, and technology) make it challenging. A new study presents a data-driven approach to optimize sensor placement in urban stormwater systems, allowing for accurate monitoring with minimal sensors.

The researchers used a computer model to simulate a stormwater system in Duluth, Minnesota, and applied a data-driven framework to identify the most important locations to place sensors. They found that just three optimally placed sensors among 77 possible locations could accurately reconstruct peak flowrates (a measure of water flow) with an accuracy of 92-95%. The approach is robust to measurement uncertainties and sensor failures, and can be integrated with predictive models to enable early flood warnings and real-time control.

This study offers a cost-effective solution for monitoring urban stormwater systems, enabling cities to better prepare for and respond to flooding events. The approach can be applied to other cities, helping to reduce the risk of urban flooding and protect communities."
cs.AI,LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems,"Modeling user preferences across domains remains a key challenge in slate recommendation (i.e. recommending an ordered sequence of items) research. We investigate how Large Language Models (LLM) can effectively act as world models of user preferences through pairwise reasoning over slates. We conduct an empirical study involving several LLMs on three tasks spanning different datasets. Our results reveal relationships between task performance and properties of the preference function captured by LLMs, hinting towards areas for improvement and highlighting the potential of LLMs as world models in recommender systems.",http://arxiv.org/abs/2511.04541v1,2025-11-06T16:54:54Z,"Baptiste Bonin, Maxime Heuillet, Audrey Durand","**Can AI Models Help Recommend Items More Effectively?**

Researchers are working on improving recommendation systems, which suggest items to users in a specific order. A key challenge is understanding what users like across different areas of interest. This study explores using Large Language Models (LLMs), a type of artificial intelligence, to better understand user preferences.

The researchers tested several LLMs on three tasks using different datasets. They found that LLMs can effectively capture user preferences and make recommendations. The study also identified areas where LLMs can be improved, highlighting their potential to become a crucial part of recommendation systems.

In simple terms, the study shows that AI models can help recommend items in a more personalized and effective way. This could lead to better experiences for users when browsing online, such as streaming services or online shopping platforms."
cs.AI,Are language models aware of the road not taken? Token-level uncertainty   and hidden state dynamics,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",http://arxiv.org/abs/2511.04527v1,2025-11-06T16:43:25Z,"Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow","**Unlocking the ""What Ifs"" of Language Models**

Imagine you're chatting with a conversational AI, and you ask it to explain a complex topic. The AI generates a response, but did you know that it considered multiple possible paths before choosing the words it did? Researchers have made a fascinating discovery about how language models work, and it has to do with ""what ifs.""

In a recent study, scientists investigated whether language models can represent alternative paths they could take during text generation. They found that when a language model is generating text, it does indeed consider different possible paths, and this uncertainty is reflected in its internal workings.

The researchers used a clever technique to control and predict the model's uncertainty during complex reasoning tasks. They discovered that when the model is unsure or has multiple options, it's easier to steer its responses by manipulating its internal state. This suggests that the model is aware of the ""road not taken"" and can be nudged in different directions.

The study's findings have significant implications for understanding how language models think and make decisions. By recognizing that language models can implicitly represent multiple possible paths, researchers can develop more sophisticated and transparent AI systems. This could lead to more accurate and reliable language models, which could be used in a wide range of applications, from chatbots to language translation.

In simple terms, this research shows that language models are not just generating text based on fixed rules, but are actually considering multiple possibilities and making decisions based on uncertainty. This new understanding can help us build more advanced and trustworthy AI systems."
cs.AI,Alternative Fairness and Accuracy Optimization in Criminal Justice,"Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.",http://arxiv.org/abs/2511.04505v1,2025-11-06T16:24:43Z,"Shaolong Wu, James Blume, Geshi Yeung","**Fairness and Accuracy in Criminal Justice: A New Approach**

The use of algorithms in criminal justice has raised concerns about fairness and accuracy. Researchers have been working to ensure that these algorithms don't unfairly target certain groups of people. However, different ideas of fairness can sometimes conflict with each other.

In a new study, researchers propose a modified approach to fairness that balances accuracy and fairness. Instead of requiring exact equality across different groups, their approach allows for small differences in error rates between groups. This makes it easier to find solutions that are both fair and accurate.

The researchers also highlight three key challenges in achieving fairness in criminal justice:

1. **Biased data**: Algorithms can perpetuate existing biases if they're trained on incomplete or biased data.
2. **Hidden biases**: Algorithms can inadvertently perpetuate affirmative action policies, which can be problematic.
3. **Complexity**: There are many subgroups within a population, and ensuring fairness for each one can be overwhelming.

To address these challenges, the researchers propose a practical framework for deploying fair and accurate algorithms in public decision systems. This framework has three key elements:

1. **Need-based decisions**: Focus on making decisions based on individual needs rather than group characteristics.
2. **Transparency and accountability**: Ensure that algorithms are transparent and accountable to prevent biases and errors.
3. **Narrowly tailored solutions**: Develop solutions that are specifically designed to address particular problems, rather than relying on broad, one-size-fits-all approaches.

Overall, this study provides a new approach to fairness and accuracy in criminal justice, one that balances competing values and provides actionable guidance for agencies using risk assessment tools."
cs.AI,RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific   RAG,"Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.",http://arxiv.org/abs/2511.04502v1,2025-11-06T16:22:52Z,"Joshua Gao, Quoc Huy Pham, Subin Varghese, Silwal Saurav, Vedhus Hoskere","**Improving the Accuracy of AI Systems in Specialized Fields**

Large Language Models (LLMs) are powerful tools that can generate human-like text, but they can also make mistakes if they're not grounded in factual evidence. Retrieval-Augmented Generation (RAG) is a technique that helps LLMs access relevant information to provide more accurate answers. However, evaluating the performance of RAG systems in specialized fields like military operations, cybersecurity, and bridge engineering is a significant challenge.

A new framework called RAGalyst aims to address this challenge. RAGalyst is an automated system that generates high-quality test data and evaluates the performance of RAG systems in a way that aligns with human judgment. The framework uses a combination of natural language processing and machine learning techniques to assess the accuracy and reliability of RAG systems.

The researchers behind RAGalyst tested it across three distinct domains and found that the performance of RAG systems varies greatly depending on the context. They also identified common reasons why RAG systems make mistakes, highlighting the need for a systematic evaluation framework like RAGalyst.

The key benefits of RAGalyst are:

* **Improved accuracy**: RAGalyst helps ensure that RAG systems provide accurate answers by evaluating their performance in a way that aligns with human judgment.
* **Domain-specific evaluation**: RAGalyst is designed to evaluate RAG systems in specialized fields, taking into account the unique nuances and challenges of each domain.
* **Informed design choices**: By using RAGalyst, practitioners can make informed decisions about how to design and optimize their RAG systems for reliable and effective performance.

Overall, RAGalyst has the potential to improve the accuracy and reliability of AI systems in specialized fields, enabling practitioners to build more effective RAG systems. The framework is now available on Github for others to use."
cs.AI,Large language models replicate and predict human cooperation across   experiments in game theory,"Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.",http://arxiv.org/abs/2511.04500v1,2025-11-06T16:21:27Z,"Andrea Cera Palatsi, Samuel Martin-Gutierrez, Ana S. Cardenal, Max Pellert","**Can AI Models Mimic Human Cooperation?**

Researchers have made a significant breakthrough in understanding how well large language models (LLMs) can mimic human behavior in social situations. In a series of experiments, they tested three open-source LLMs - Llama, Mistral, and Qwen - to see how well they could replicate human cooperation patterns in game theory scenarios.

**The Experiment**

The researchers created a digital replica of classic game theory experiments, which are used to study human decision-making and cooperation. They then prompted the LLMs to play these games and analyzed their behavior. Surprisingly, one of the models, Llama, was able to accurately replicate human cooperation patterns, even capturing subtle deviations from rational decision-making.

**The Findings**

The study found that:

* Llama was able to mimic human cooperation patterns with high accuracy, without needing to be specifically programmed to act like a human.
* The model was able to predict human behavior in new, unexplored game scenarios, generating testable hypotheses for future research.
* The results suggest that LLMs can be a useful tool for social scientists, allowing them to explore complex social behaviors in a more efficient and cost-effective way.

**The Implications**

The study's findings have significant implications for the use of LLMs in social simulations and decision-making. By demonstrating that LLMs can accurately replicate human cooperation patterns, the researchers have opened up new possibilities for using these models to:

* Simulate human behavior in complex social situations, allowing researchers to test hypotheses and make predictions about human behavior.
* Develop more effective decision-making tools, by incorporating insights from LLMs into real-world applications.

**The Future of AI and Social Science**

The study highlights the potential for LLMs to complement traditional research in the social and behavioral sciences. By combining the strengths of LLMs with human expertise, researchers can gain a deeper understanding of human social decision-making and develop more effective solutions to complex social problems. As LLMs continue to evolve, they are likely to play an increasingly important role in shaping our understanding of human behavior and informing decision-making in a wide range of fields."
cs.AI,Decoding Emergent Big Five Traits in Large Language Models:   Temperature-Dependent Expression and Architectural Clustering,"As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",http://arxiv.org/abs/2511.04499v1,2025-11-06T16:20:52Z,"Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou","**Unlocking the Personality of AI: A Study on Large Language Models**

As AI-powered language models become more prevalent in our daily lives, researchers are seeking to understand their behavior and personality. A recent study explored the ""personality"" of six large language models (LLMs) using the Big Five personality traits framework, which is commonly used to assess human personality.

The study found that LLMs can exhibit different personality traits depending on the settings used to generate text. Specifically, the models' expressions of Neuroticism and Extraversion were sensitive to changes in ""temperature,"" a parameter that controls the randomness of the generated text. This means that by adjusting the temperature, the models can produce text that is more or less emotional, anxious, or outgoing.

Interestingly, the study also revealed that LLMs with similar architectural features tend to cluster together in terms of their personality traits. This suggests that the design of the model itself can influence its personality-like behavior.

These findings have important implications for the development and deployment of AI systems. By understanding the personality traits of LLMs, researchers and developers can better design and tune these models to ensure they behave in a responsible and trustworthy manner. The study's results also highlight the need for careful consideration of the potential biases and risks associated with AI systems.

The study's data and code are publicly available, providing a valuable resource for further research and development in this area."
cs.AI,OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code   Generation,"This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.",http://arxiv.org/abs/2511.04495v1,2025-11-06T16:16:32Z,"Cuong Huynh, Jie Cao","Here's a summary of the research paper for a general audience:

**Making Text Easier to Read: A New Approach to Text Simplification**

Researchers at OUNLP have developed a system to simplify complex text, making it easier to read for a wider audience. The system uses artificial intelligence (AI) to break down text into simpler language while maintaining its original meaning.

The researchers found that the bigger the gap between the original text's complexity and the desired simplicity, the harder it is to simplify. To overcome this challenge, they created two new methods:

1. **Rule-based simplification**: This method uses pre-defined rules to simplify text in multiple rounds.
2. **Joint rule-based LLM simplification**: This method uses a large language model (like GPT-4o) to simplify text in multiple rounds, building on the previous simplifications.

The researchers tested their system in a competition and ranked 7th out of 20 teams. Further improvements showed that using the AI-simplified text as a starting point can lead to even better results. This work has the potential to improve the accessibility of complex text for people who struggle with reading or language barriers."
cs.AI,RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within   Structured Tables,"Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.",http://arxiv.org/abs/2511.04491v1,2025-11-06T16:10:03Z,"Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy","**Unlocking the Potential of Large Language Models: A New Benchmark for Tabular Reasoning**

Large Language Models (LLMs) have made tremendous progress in understanding and processing natural language. However, their ability to reason with complex, real-world data, such as tables, remains largely untested. A new benchmark, called RUST-BENCH, aims to change that.

**The Challenge of Real-World Data**

Most existing benchmarks for tabular reasoning use small, simple tables that don't reflect the complexity of real-world data. In contrast, RUST-BENCH uses 2031 real-world tables from two domains - science (NSF grant records) and sports (NBA statistics) - to test LLMs' reasoning abilities. These tables are long, diverse, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens.

**What Did the Researchers Find?**

The researchers tested several open-source and proprietary LLMs on RUST-BENCH and found that they struggle with:

1. **Heterogeneous schemas**: LLMs have trouble understanding tables with diverse structures and formats.
2. **Complex multi-hop inference**: LLMs find it challenging to make connections between multiple pieces of information in a table.

**What Does This Mean?**

The results reveal persistent weaknesses in current LLM architectures and prompting strategies. RUST-BENCH establishes a new, challenging testbed for advancing tabular reasoning research. By pushing LLMs to their limits, researchers can identify areas for improvement and develop more sophisticated models that can tackle complex, real-world data. Ultimately, this work has the potential to unlock the full potential of LLMs and enable them to make more accurate and informed decisions."
cs.AI,Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank   Training,"Parameter-efficient training, based on low-rank optimization, has become a highly successful tool for fine-tuning large deep-learning models. However, these methods fail at low-rank pre-training tasks where maintaining the low-rank structure and the objective remains a challenging task. We propose the Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel low-rank inducing training strategy inspired by the iteratively reweighted least squares (IRLS) framework. Q3R is based on a quadratic regularizer term which majorizes a smoothed log determinant serving as rank surrogate objective. Unlike other low-rank training techniques, Q3R is able to train weight matrices with prescribed, low target ranks of models that achieve comparable predictive performance as dense models, with small computational overhead, while remaining fully compatible with existing architectures. For example, we demonstrated one experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance respectively. The efficacy of Q3R is confirmed on Transformers across both image and language tasks, including for low-rank fine-tuning.",http://arxiv.org/abs/2511.04485v1,2025-11-06T16:05:12Z,"Ipsita Ghosh, Ethan Nguyen, Christian Kümmerle","**Breakthrough in Efficient AI Training: Q3R Revolutionizes Low-Rank Model Training**

Researchers have made a significant advancement in training artificial intelligence (AI) models more efficiently. They've developed a new method called Q3R (Quadratic Reweighted Rank Regularizer), which enables the creation of ""low-rank"" models that require fewer parameters while maintaining similar performance to traditional models.

**What does this mean?**

In AI, models are essentially complex mathematical equations that learn from data. These models have millions or even billions of parameters, which are like adjustable knobs that help the model make predictions. However, many of these parameters may not be necessary, and reducing them can make the model more efficient and faster to train.

**The challenge**

Previous methods for creating low-rank models struggled with tasks that required maintaining a specific structure while optimizing the model's performance. The Q3R method overcomes this challenge by introducing a novel regularizer term that helps guide the training process.

**The Q3R solution**

Q3R works by adding a special term to the model's objective function that encourages the model to have a low-rank structure. This term is based on a mathematical concept called the log determinant, which serves as a proxy for the model's rank. The Q3R method is compatible with existing AI architectures and can be applied to a wide range of tasks.

**Impressive results**

In experiments, Q3R achieved remarkable results:

* For a vision transformer model (ViT-Tiny) on the CIFAR-10 image classification task, Q3R was able to reduce the model's parameters by 60% and 80% while maintaining 99% and 96% of the original accuracy, respectively.
* Q3R also demonstrated effectiveness on transformer models for both image and language tasks, including low-rank fine-tuning.

**Impact**

The Q3R method has the potential to significantly reduce the computational resources required for training large AI models, making them more accessible and sustainable. This breakthrough can lead to more efficient and environmentally friendly AI development, enabling wider adoption and applications in various fields."
cs.AI,Promoting Sustainable Web Agents: Benchmarking and Estimating Energy   Consumption through Empirical and Theoretical Analysis,"Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the user's behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks.",http://arxiv.org/abs/2511.04481v1,2025-11-06T15:59:59Z,"Lars Krupp, Daniel Geißler, Vishal Banwari, Paul Lukowicz, Jakob Karolus","Here's a summary of the research paper for a general audience:

**The Hidden Cost of Web Agents: Energy Consumption and Sustainability**

Web agents, like AI-powered tools that can browse the internet and perform tasks on our behalf, are becoming increasingly popular. However, researchers have largely overlooked the environmental impact of these agents. A new study explores the energy consumption and carbon footprint of web agents, like OpenAI's Operator and Google's Project Mariner.

The study found that different approaches to creating web agents can significantly affect their energy usage. Surprisingly, more energy consumption doesn't always lead to better results. The researchers also discovered that some web agents' creators don't disclose enough information about their models and processes, making it hard to estimate their energy consumption.

The study's findings highlight the need for a new way of evaluating web agents. The researchers propose developing dedicated metrics to measure energy consumption in benchmarks, which could help create more sustainable web agents. This work aims to raise awareness about the environmental impact of web agents and encourage a shift towards more eco-friendly AI development."
cs.AI,"Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop   Refinement of LLM Judges","The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but its effectiveness is often limited by the scarcity of diverse, representative data for refining criteria. We present a tool that integrates synthetic data generation into the LLM-as-a-judge workflow, empowering users to create tailored and challenging test cases with configurable domains, personas, lengths, and desired outcomes, including borderline cases. The tool also supports AI-assisted inline editing of existing test cases. To enhance transparency and interpretability, it reveals the prompts and explanations behind each generation. In a user study (N=24), 83% of participants preferred the tool over manually creating or selecting test cases, as it allowed them to rapidly generate diverse synthetic data without additional workload. The generated synthetic data proved as effective as hand-crafted data for both refining evaluation criteria and aligning with human preferences. These findings highlight synthetic data as a promising alternative, particularly in contexts where efficiency and scalability are critical.",http://arxiv.org/abs/2511.04478v1,2025-11-06T15:57:19Z,"Hyo Jin Do, Zahra Ashktorab, Jasmina Gajcin, Erik Miehling, Martín Santillán Cooper, Qian Pan, Elizabeth M. Daly, Werner Geyer","Here's a summary of the research paper for a general audience:

**Improving AI Decision-Making with Synthetic Data**

Imagine you're trying to teach a computer to make decisions, but it's struggling to understand what's fair or not. That's where humans come in - to help refine the computer's judgment. However, finding diverse and representative examples to teach the computer can be a challenge.

Researchers have developed a tool that uses synthetic data (artificially generated examples) to help improve AI decision-making. This tool allows users to create customized test cases that are challenging and representative of real-life scenarios. The tool also provides transparency into how it generates these test cases, making it easier to understand and trust the results.

In a study with 24 participants, 83% preferred using this tool over manually creating test cases. The good news is that the synthetic data generated by the tool was just as effective as human-created data in refining the AI's decision-making criteria. This breakthrough has the potential to make AI decision-making more efficient and scalable, especially in situations where time and resources are limited.

**In simple terms:** This research shows that using artificial examples (synthetic data) can help improve AI decision-making, making it more efficient and accurate. This could have significant implications for various applications, from healthcare to finance, where AI is used to make important decisions."
cs.CL,VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical   Consistency Checks,"LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.",http://arxiv.org/abs/2511.04662v1,2025-11-06T18:50:08Z,"Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala","**Improving AI Reasoning with VeriCoT**

Large Language Models (LLMs) are great at solving complex problems by breaking them down into steps, a process called Chain-of-Thought (CoT) reasoning. However, they often can't verify if their own reasoning is correct, which can lead to flawed conclusions. To address this issue, researchers have developed VeriCoT, a new method that checks the logical consistency of CoT reasoning.

**How VeriCoT Works**

VeriCoT converts each step of CoT reasoning into a formal logical argument, which can be automatically verified for validity. This allows VeriCoT to identify flawed reasoning steps and predict the accuracy of the final answer. The method also provides a way for humans to review and correct the reasoning process.

**Testing and Results**

VeriCoT was tested on several datasets and showed promising results. It effectively identified flawed reasoning and predicted the correctness of final answers. The researchers also used VeriCoT to improve the reasoning abilities of LLMs through self-reflection, fine-tuning, and preference optimization.

**Implications**

The development of VeriCoT has significant implications for high-stakes applications of AI, such as medicine, law, and finance, where accurate and reliable reasoning is crucial. By providing a way to verify the logical consistency of AI reasoning, VeriCoT can help build trust in AI systems and improve their performance."
cs.CL,Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning,"Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30--35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding.",http://arxiv.org/abs/2511.04654v1,2025-11-06T18:43:16Z,"Mohammad Atif Quamar, Mohammad Areeb","Here's a summary of the research paper for a general audience:

**Making AI Reasoning More Efficient**

Researchers have developed a new technique called LEASH that helps reduce the computational cost of complex reasoning in large language models. These models are used for tasks that require step-by-step thinking, such as solving math problems. The technique, called Logit-Entropy Adaptive Stopping Heuristic (LEASH), allows the model to stop generating reasoning steps when it has reached a stable conclusion, rather than generating a fixed number of steps.

**The Problem: Inefficient Reasoning**

Current methods for complex reasoning, known as Chain-of-Thought (CoT) prompting, can be computationally expensive and slow. This is because they require the model to generate a fixed number of reasoning steps, even if the model has already reached a conclusion.

**The Solution: LEASH**

LEASH solves this problem by monitoring two signals: the uncertainty of the model's predictions and the confidence in its top prediction. When both signals indicate that the model has reached a stable conclusion, LEASH stops the generation of reasoning steps. This approach reduces the number of steps generated by 30-35% and speeds up the process by 27%, while maintaining a high level of accuracy.

**Key Benefits**

The LEASH technique is model-agnostic, meaning it can be used with different language models without requiring additional training or supervision. This makes it a simple and efficient alternative to current methods for complex reasoning. Overall, LEASH has the potential to make AI reasoning more efficient and scalable, enabling faster and more accurate solutions to complex problems."
cs.CL,DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for   Embodied LLM-Based Multi-Agent Collaboration,"Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.",http://arxiv.org/abs/2511.04646v1,2025-11-06T18:37:18Z,"Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong","Here's a summary of the research paper for a general audience:

**Title:** DR. WELL: A New Framework for Cooperative Robots and AI Agents to Work Together

**Imagine:** You're working on a project with a team, but you can't always see what your teammates are doing, and you have to make decisions quickly. This can be challenging, especially if small mistakes can lead to big problems. Researchers have developed a new framework called DR. WELL, which helps multiple robots or AI agents work together more effectively.

**How it works:** DR. WELL uses a combination of reasoning, learning, and symbolic planning to enable cooperation among agents. The framework consists of two phases: first, agents propose roles and negotiate with each other to come to a consensus; second, each agent generates a plan to achieve its role without sharing detailed movements. A shared ""world model"" keeps track of the current state and updates as agents take actions.

**Benefits:** By using symbolic plans rather than detailed movements, DR. WELL enables more flexible and efficient collaboration. Agents can adapt to changing situations and learn from experience, leading to improved task completion rates and efficiency. In experiments, DR. WELL was tested on cooperative block-pushing tasks, where agents learned to work together more effectively over time.

**In simple terms:** DR. WELL is a new approach that helps multiple robots or AI agents work together more effectively, even with limited communication and partial information. By using a shared understanding of the world and negotiating with each other, agents can adapt and learn to achieve their goals more efficiently."
cs.CL,When retrieval outperforms generation: Dense evidence retrieval for   scalable fake news detection,"The proliferation of misinformation necessitates robust yet computationally efficient fact verification systems. While current state-of-the-art approaches leverage Large Language Models (LLMs) for generating explanatory rationales, these methods face significant computational barriers and hallucination risks in real-world deployments. We present DeReC (Dense Retrieval Classification), a lightweight framework that demonstrates how general-purpose text embeddings can effectively replace autoregressive LLM-based approaches in fact verification tasks. By combining dense retrieval with specialized classification, our system achieves better accuracy while being significantly more efficient. DeReC outperforms explanation-generating LLMs in efficiency, reducing runtime by 95% on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92% on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds), showcasing its effectiveness across varying dataset sizes. On the RAWFC dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art method L-Defense (61.20%). Our results demonstrate that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment.",http://arxiv.org/abs/2511.04643v1,2025-11-06T18:35:45Z,"Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir","**Fighting Fake News with a More Efficient Approach**

Researchers have developed a new system called DeReC to detect fake news more efficiently and accurately. Currently, top methods for fact-checking use large language models (LLMs) that generate explanations, but these models are computationally expensive and can produce incorrect information. DeReC uses a different approach called dense retrieval, which relies on general-purpose text embeddings to classify information as true or false.

The results show that DeReC outperforms LLM-based approaches in efficiency, reducing runtime by up to 95%. DeReC also achieves better accuracy, with an F1 score of 65.58% on one dataset, surpassing the state-of-the-art method L-Defense (61.20%). This makes it a more practical solution for real-world deployment. The study demonstrates that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks, paving the way for more efficient and effective fake news detection."
cs.CL,Are We Asking the Right Questions? On Ambiguity in Natural Language   Queries for Tabular Data Analysis,"Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.",http://arxiv.org/abs/2511.04584v1,2025-11-06T17:39:18Z,"Daniel Gomm, Cornelius Wolff, Madelon Hulsebos","**Understanding Ambiguity in Natural Language Queries**

Imagine asking a computer to analyze data from a spreadsheet using everyday language, like ""What's the average salary?"" or ""How many people are from New York?"" The computer needs to understand what you mean, but sometimes your question might be unclear or open to multiple interpretations. This is called ambiguity.

Researchers have developed a new framework to deal with ambiguity in natural language queries for tabular data analysis. Instead of trying to eliminate ambiguity, they suggest that it's a natural part of how humans and computers interact. The idea is to share the responsibility of clarifying the question between the user and the computer.

The researchers analyzed 15 popular datasets of natural language queries and found that they often mixed different types of questions, making it hard to evaluate how well a computer system can understand and execute them. By recognizing and embracing ambiguity, they propose a new approach to designing and evaluating natural language interfaces for tabular data. This could lead to more effective and user-friendly systems that can better understand and respond to our queries."
cs.CL,Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration   from a Baseline Paper,"Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",http://arxiv.org/abs/2511.04583v1,2025-11-06T17:37:49Z,"Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa","**Breakthrough in AI-Driven Scientific Research: Jr. AI Scientist**

Imagine a system that can analyze scientific research, identify areas for improvement, and even write its own papers. This is now a reality with Jr. AI Scientist, a cutting-edge AI system that mimics the workflow of a junior researcher. Given a ""baseline paper"" as a starting point, Jr. AI Scientist can analyze its limitations, come up with new ideas to improve it, test these ideas through experiments, and write a paper with the results.

**What makes Jr. AI Scientist special?**

Unlike previous AI systems, Jr. AI Scientist follows a well-defined research workflow and can handle complex coding tasks. This leads to scientifically valuable contributions that can be evaluated by human reviewers. In fact, Jr. AI Scientist generated papers that received higher review scores than existing fully automated systems.

**But what are the risks?**

While Jr. AI Scientist shows great promise, the researchers behind it also identified important limitations and potential risks. These include concerns about the accuracy and reliability of AI-generated research, as well as challenges in evaluating the quality of AI-driven scientific contributions.

**What's next?**

The development of Jr. AI Scientist provides valuable insights into the current progress and risks of AI-driven scientific research. As AI systems like Jr. AI Scientist continue to evolve, it's essential to understand their capabilities and limitations to ensure trustworthy and sustainable AI-driven scientific progress."
cs.CL,Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm,"""Thinking with Text"" and ""Thinking with Images"" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce ""Thinking with Video"", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions ""thinking with video"" as a unified multimodal reasoning paradigm.",http://arxiv.org/abs/2511.04570v1,2025-11-06T17:25:23Z,"Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu","**Unlocking a New Way of Thinking: ""Thinking with Video""**

Imagine being able to understand and generate not just text or images, but a combination of both, in a way that captures dynamic processes and continuous changes. Researchers have introduced a new paradigm called ""Thinking with Video,"" which uses video generation models to bridge the gap between visual and textual reasoning.

Currently, large language models (LLMs) and vision language models (VLMs) rely on either text or images to reason and make decisions. However, these approaches have limitations. Images, for instance, only capture single moments and can't represent dynamic processes or continuous changes. On the other hand, text and vision are often treated as separate modalities, making it difficult to achieve unified multimodal understanding and generation.

The ""Thinking with Video"" paradigm overcomes these limitations by leveraging video generation models, such as Sora-2, to provide a unified temporal framework for visual and textual reasoning. To test this approach, researchers created a new benchmark called Video Thinking Benchmark (VideoThinkBench), which includes tasks that require vision-centric and text-centric reasoning.

The results are promising: Sora-2, a video generation model, performed comparably to state-of-the-art VLMs on vision-centric tasks and even surpassed them on some tasks. On text-centric tasks, Sora-2 achieved high accuracy rates, such as 92% on math problems and 75.53% on a multimodal benchmark.

The study also found that self-consistency and in-context learning can improve Sora-2's performance. Overall, the findings suggest that video generation models have the potential to become unified multimodal understanding and generation models, paving the way for a new paradigm in reasoning and decision-making.

**In simple terms:** ""Thinking with Video"" is a new approach that uses video generation models to combine visual and textual reasoning, overcoming the limitations of current text-based and image-based models. The results show that this approach can lead to improved performance on various tasks and has the potential to revolutionize the way we think about reasoning and decision-making."
cs.CL,BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented   Generation Strategies for Bangla Biomedical Question Answering,"Developing accurate biomedical Question Answering (QA) systems in low-resource languages remains a major challenge, limiting equitable access to reliable medical knowledge. This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical artificial intelligence (AI). The study applies and benchmarks several Retrieval-Augmented Generation (RAG) strategies, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining textbook-based and web retrieval with generative reasoning to improve factual accuracy. A key novelty lies in integrating a Bangla medical textbook corpus through Optical Character Recognition (OCR) and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy 89.54% with openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical artificial intelligence.",http://arxiv.org/abs/2511.04560v1,2025-11-06T17:15:33Z,"Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, Tareque Mohmud Chowdhury","**Breaking Down Language Barriers in Medical Knowledge: A New Approach to Biomedical Question Answering**

Imagine being able to access reliable medical information in your native language. For millions of people around the world, this is a challenge, especially in low-resource languages. A recent research paper introduces two new datasets, BanglaMedQA and BanglaMMedBench, designed to evaluate the ability of artificial intelligence (AI) systems to answer medical questions in Bangla, a widely spoken language.

The researchers tested several strategies to improve the accuracy of biomedical question answering in Bangla. They combined textbook-based and web-based information retrieval with generative reasoning to create more accurate answers. One approach, called Agentic Retrieval-Augmented Generation (RAG), dynamically selects between retrieving information and generating answers. This approach achieved an impressive accuracy of 89.54% using a large language model.

The study's findings are significant, as they demonstrate the potential of RAG-based methods to enhance the reliability and accessibility of medical question answering in Bangla. This research establishes a foundation for future studies in multilingual medical AI, paving the way for more people to access reliable medical knowledge in their native languages. Ultimately, this work aims to bridge the gap in language barriers and promote equitable access to medical information worldwide."
cs.CL,From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities   Reporting,"As the role of Large Language Models (LLM)-based coding assistants in software development becomes more critical, so does the role of the bugs they generate in the overall cybersecurity landscape. While a number of LLM code security benchmarks have been proposed alongside approaches to improve the security of generated code, it remains unclear to what extent they have impacted widely used coding LLMs. Here, we show that even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios in a realistic use setting, suggesting that the safety-functionality trade-off has until now prevented effective patching of vulnerabilities. To help address this issue, we introduce a new severity metric that reflects the risk posed by an LLM-generated vulnerability, accounting for vulnerability severity, generation chance, and the formulation of the prompt that induces vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation of the most serious and prevalent vulnerabilities, we use PE to define the Model Exposure (ME) score, which indicates the severity and prevalence of vulnerabilities a model generates.",http://arxiv.org/abs/2511.04538v1,2025-11-06T16:52:27Z,"Cyril Vallez, Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic","**Large Language Models Can Generate Vulnerabilities in Code: A New Way to Assess and Mitigate Risks**

As AI-powered coding assistants become more widely used in software development, there's a growing concern about the security of the code they generate. Researchers have been working to improve the security of these models, but it's unclear how effective these efforts have been.

A recent study found that even the latest AI models can generate code with vulnerabilities, which are weaknesses that can be exploited by hackers. These vulnerabilities can put software and users at risk. The researchers discovered that these models can produce vulnerable code even in realistic use settings.

To address this issue, the researchers developed a new way to measure the severity of vulnerabilities generated by AI models. They introduced two new metrics: Prompt Exposure (PE) and Model Exposure (ME). PE assesses the risk posed by a specific vulnerability, taking into account how severe the vulnerability is, how likely it is to occur, and how it's generated. ME scores indicate the overall severity and prevalence of vulnerabilities generated by a model.

The goal of this research is to help developers and AI model creators identify and mitigate the most serious and common vulnerabilities, ultimately making software more secure. By using these new metrics, researchers and developers can work together to create safer AI-powered coding assistants and reduce the risk of cyber breaches."
cs.CL,IntelliProof: An Argumentation Network-based Conversational Helper for   Organized Reflection,"We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text. A live demo and the system are available here to try: \textbf{https://intelliproof.vercel.app}",http://arxiv.org/abs/2511.04528v1,2025-11-06T16:43:37Z,"Kaveh Eskandari Miandoab, Katharine Kowalyshyn, Kabir Pamnani, Anesu Gavhera, Vasanth Sarathy, Matthias Scheutz","Here's a summary of the research paper for a general audience:

**Introducing IntelliProof: A Conversational Helper for Better Essay Writing**

Researchers have developed a new interactive system called IntelliProof, which uses artificial intelligence to help users analyze and improve their argumentative essays. IntelliProof takes an essay and breaks it down into a visual graph, showing how different claims and evidence are connected. This helps users understand the strengths and weaknesses of their argument and identify areas for improvement.

Unlike other essay analysis tools, IntelliProof prioritizes user experience and understanding. It provides clear explanations for its classifications and scores, and allows users to explore their essay's argumentative quality in a interactive and visual way. The system also offers tools to help users better comprehend their essay's structure and meaning.

The goal of IntelliProof is to support users in reflecting on their writing and improving its clarity and coherence, while still allowing for human oversight and control. You can try out a live demo of IntelliProof at [https://intelliproof.vercel.app](https://intelliproof.vercel.app)."
cs.CL,Are language models aware of the road not taken? Token-level uncertainty   and hidden state dynamics,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",http://arxiv.org/abs/2511.04527v1,2025-11-06T16:43:25Z,"Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow","Here's a summary of the research paper for a general audience:

**Do Language Models Consider Alternative Paths?**

Imagine you're writing a story, and at each sentence, you have to choose the next word. A language model, like a computer program that generates text, makes similar choices. But have you ever wondered if the model considers alternative paths, or ""what ifs,"" as it generates text?

Researchers investigated this question by analyzing how language models work. They found that when a model is uncertain about which word to choose next, it's more likely to be considering multiple paths. By controlling the model's internal workings, they showed that it's easier to steer the model's output when it's uncertain. This suggests that the model is implicitly aware of the alternative paths it could take.

In other words, the model is not just choosing one path; it's also keeping track of other possible paths, even if it's not explicitly showing them. This discovery has implications for how we understand and improve language models, which are used in applications like chatbots, language translation, and text summarization.

**Key Takeaway:** Language models are capable of considering multiple paths as they generate text, and their internal workings reflect this uncertainty. This awareness can help researchers develop more sophisticated and accurate language models."
cs.CL,Modeling Clinical Uncertainty in Radiology Reports: from Explicit   Uncertainty Markers to Implicit Reasoning Pathways,"Radiology reports are invaluable for clinical decision-making and hold great potential for automated analysis when structured into machine-readable formats. These reports often contain uncertainty, which we categorize into two distinct types: (i) Explicit uncertainty reflects doubt about the presence or absence of findings, conveyed through hedging phrases. These vary in meaning depending on the context, making rule-based systems insufficient to quantify the level of uncertainty for specific findings; (ii) Implicit uncertainty arises when radiologists omit parts of their reasoning, recording only key findings or diagnoses. Here, it is often unclear whether omitted findings are truly absent or simply unmentioned for brevity. We address these challenges with a two-part framework. We quantify explicit uncertainty by creating an expert-validated, LLM-based reference ranking of common hedging phrases, and mapping each finding to a probability value based on this reference. In addition, we model implicit uncertainty through an expansion framework that systematically adds characteristic sub-findings derived from expert-defined diagnostic pathways for 14 common diagnoses. Using these methods, we release Lunguage++, an expanded, uncertainty-aware version of the Lunguage benchmark of fine-grained structured radiology reports. This enriched resource enables uncertainty-aware image classification, faithful diagnostic reasoning, and new investigations into the clinical impact of diagnostic uncertainty.",http://arxiv.org/abs/2511.04506v1,2025-11-06T16:24:53Z,"Paloma Rabaey, Jong Hak Moon, Jung-Oh Lee, Min Gwan Kim, Hangyul Yoon, Thomas Demeester, Edward Choi","**Understanding Uncertainty in Radiology Reports**

Radiology reports play a crucial role in helping doctors make informed decisions about patient care. However, these reports often contain uncertainty, which can make it challenging to interpret the results. Researchers have identified two types of uncertainty in radiology reports: explicit and implicit.

**Explicit Uncertainty: What Doctors Say**

Explicit uncertainty refers to phrases used by doctors to express doubt about the presence or absence of a finding. For example, phrases like ""may be"" or ""possibly"" can convey uncertainty. However, these phrases can vary in meaning depending on the context, making it difficult to quantify the level of uncertainty.

**Implicit Uncertainty: What's Left Out**

Implicit uncertainty occurs when doctors omit parts of their reasoning or only report key findings, leaving it unclear whether omitted findings are truly absent or simply not mentioned. For instance, if a doctor only mentions a patient's symptoms but not their medical history, it may be unclear whether the doctor considered the patient's medical history or simply didn't think it was relevant.

**A New Framework for Understanding Uncertainty**

To address these challenges, researchers have developed a two-part framework. First, they created a reference ranking of common phrases used to express uncertainty, which helps to quantify explicit uncertainty. Second, they developed an expansion framework that systematically adds characteristic sub-findings derived from expert-defined diagnostic pathways for common diagnoses, which helps to model implicit uncertainty.

**A New Resource for Radiology Reports**

Using this framework, researchers have created Lunguage++, an updated version of a benchmark for structured radiology reports. This new resource enables:

* **Uncertainty-aware image classification**: accurately classifying medical images while taking into account the uncertainty in the radiology reports
* **Faithful diagnostic reasoning**: making more accurate diagnoses by considering the uncertainty in the radiology reports
* **Investigations into the clinical impact of diagnostic uncertainty**: studying how uncertainty in radiology reports affects patient care and outcomes

By providing a more nuanced understanding of uncertainty in radiology reports, this research has the potential to improve the accuracy and reliability of medical diagnoses and treatments. For example, this new resource could help doctors to:

* Better communicate with patients about the uncertainty surrounding their diagnosis or treatment
* Make more informed decisions about patient care by taking into account the uncertainty in radiology reports
* Develop more effective treatment plans that account for the uncertainty in radiology reports

Overall, this research aims to improve the way doctors understand and communicate uncertainty in radiology reports, ultimately leading to better patient care."
cs.CL,RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific   RAG,"Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.",http://arxiv.org/abs/2511.04502v1,2025-11-06T16:22:52Z,"Joshua Gao, Quoc Huy Pham, Subin Varghese, Silwal Saurav, Vedhus Hoskere","**Improving the Accuracy of AI Systems in Specialized Fields**

Retrieval-Augmented Generation (RAG) is a technique used to ensure that Large Language Models (LLMs) provide accurate and reliable information. However, evaluating the performance of RAG systems in specialized fields, such as military operations, cybersecurity, and bridge engineering, is a significant challenge. 

Researchers have developed a new framework called RAGalyst, which automates the evaluation process and aligns with human judgment. RAGalyst generates high-quality test datasets and uses advanced metrics to assess the accuracy of RAG systems. 

The study found that the performance of RAG systems varies greatly depending on the specific domain and context. No single approach works best across all domains, highlighting the need for a systematic evaluation framework like RAGalyst. 

By using RAGalyst, practitioners can make informed design choices and build more reliable and effective RAG systems. The RAGalyst framework is now available on Github, providing a valuable tool for researchers and developers working to improve the accuracy of AI systems in specialized fields."
cs.CL,Large language models replicate and predict human cooperation across   experiments in game theory,"Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.",http://arxiv.org/abs/2511.04500v1,2025-11-06T16:21:27Z,"Andrea Cera Palatsi, Samuel Martin-Gutierrez, Ana S. Cardenal, Max Pellert","**Can AI Models Mimic Human Behavior in Decision-Making?**

Researchers have made a significant breakthrough in understanding how well large language models (LLMs) can mimic human behavior in decision-making situations. In a series of experiments, they tested three open-source LLMs - Llama, Mistral, and Qwen - to see how well they could replicate human cooperation patterns in game theory scenarios.

**What did the researchers find?**

The results showed that one of the models, Llama, was able to accurately replicate human cooperation patterns, even capturing subtle deviations from rational decision-making that are characteristic of human behavior. This is a significant finding, as it suggests that LLMs can be used to simulate human behavior in social situations.

**What's the big deal?**

This research has important implications for various fields, including healthcare, education, and law, where LLMs are increasingly being used to make decisions. If LLMs can accurately mimic human behavior, they can be used to predict how humans will behave in different situations, allowing for more informed decision-making.

**What's next?**

The researchers also used their LLM to generate new predictions about human behavior in novel game scenarios that haven't been tested before. This demonstrates the potential of LLMs to complement traditional research in the social and behavioral sciences, generating new insights and predictions about human social decision-making.

**In simple terms...**

Imagine you're playing a game with a friend, and you have to decide whether to cooperate or compete. Researchers found that a type of AI model can predict how humans will behave in these situations, and even make new predictions about how humans will behave in new, untested situations. This could lead to better decision-making in areas like healthcare, education, and law."
cs.CL,Decoding Emergent Big Five Traits in Large Language Models:   Temperature-Dependent Expression and Architectural Clustering,"As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1",http://arxiv.org/abs/2511.04499v1,2025-11-06T16:20:52Z,"Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou","**Unlocking the Personality of AI: A Study on Large Language Models**

Imagine interacting with a chatbot or virtual assistant that seems outgoing and friendly one moment, but anxious and moody the next. What if AI systems could exhibit personality traits similar to humans? A recent study explored this idea by analyzing six large language models (LLMs) using a framework that assesses human personality, known as the Big Five Inventory-2.

The researchers found that LLMs can indeed exhibit different personality-like behaviors, and these traits can change depending on the ""temperature"" at which the model is run. Think of temperature like a dial that controls how random or predictable the model's responses are. The study discovered that some LLMs are more prone to expressing neurotic or extraverted traits, and that these traits can shift as the temperature changes.

Interestingly, the researchers also found that LLMs with similar architectural features tend to cluster together in terms of their personality profiles. This suggests that the design of an LLM can influence its personality-like behaviors.

These findings have important implications for the development and deployment of AI systems. By understanding how LLMs exhibit personality-like traits, researchers and developers can create more responsible and transparent AI systems that align with human values. The study's results also highlight the need for careful consideration of AI governance and ethics.

The study's data and code are publicly available, allowing other researchers to build upon these findings and further explore the fascinating world of AI personality."
cs.CL,OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code   Generation,"This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.",http://arxiv.org/abs/2511.04495v1,2025-11-06T16:16:32Z,"Cuong Huynh, Jie Cao","Here's a summary of the research paper for a general audience:

**Making Text Easier to Read: A New Approach to Text Simplification**

Researchers at OUNLP have developed a system to simplify complex text, making it easier to read for a wider audience. The system uses artificial intelligence (AI) to break down text into simpler language while maintaining its original meaning.

The researchers found that the bigger the gap between the original text's complexity and the desired simplicity, the harder it is to simplify. To overcome this challenge, they proposed two new methods: ""rule-based simplification"" and ""jointly rule-based LLM simplification"". These methods involve using AI to simplify text in multiple rounds, with each round making the text a little easier to read.

The researchers tested their system and ranked 7th out of 20 teams in a competition. They also made further improvements to their system, which showed that using AI-simplified text as a starting point can lead to even better results. This work has the potential to improve the accessibility of complex text for people who struggle with reading or language barriers."
cs.CL,RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within   Structured Tables,"Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.",http://arxiv.org/abs/2511.04491v1,2025-11-06T16:10:03Z,"Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy","**Unlocking the Potential of Large Language Models: A New Benchmark for Tabular Reasoning**

Large Language Models (LLMs) have made significant progress in understanding and processing information. However, their ability to reason with complex, real-world data has been a challenge. To address this, researchers have introduced RUST-BENCH, a new benchmark that tests LLMs on their ability to reason with unstructured text within structured tables.

**The Limitations of Current Benchmarks**

Current benchmarks for tabular reasoning have limitations. They typically use small, uniform tables that don't reflect the complexity of real-world data. In contrast, real-world tables are often long, diverse, and domain-specific, mixing structured fields with free text. This requires LLMs to perform multi-hop reasoning across thousands of tokens.

**Introducing RUST-BENCH**

RUST-BENCH is a new benchmark that consists of 7966 questions from 2031 real-world tables in two domains: NSF grant records and NBA statistics. It evaluates LLMs on four key aspects:

1. **Scale**: How well do LLMs perform on large tables?
2. **Heterogeneity**: Can LLMs handle diverse and complex table structures?
3. **Domain specificity**: How well do LLMs understand domain-specific knowledge?
4. **Reasoning complexity**: Can LLMs perform multi-hop reasoning across thousands of tokens?

**Key Findings**

Researchers tested open-source and proprietary LLMs on RUST-BENCH and found that they struggle with:

* Heterogeneous schemas (diverse table structures)
* Complex multi-hop inference (reasoning across multiple steps)

These findings highlight persistent weaknesses in current LLM architectures and prompting strategies.

**Advancing Tabular Reasoning Research**

RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research. By using this benchmark, researchers can develop more robust and effective LLMs that can handle complex, real-world data. This can lead to significant improvements in various applications, such as data analysis, decision-making, and information retrieval."
cs.CL,ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding   in Thai,"We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.",http://arxiv.org/abs/2511.04479v2,2025-11-06T15:57:39Z,"Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan, Warit Sirichotedumrong, Adisai Na-Thalang, Kunat Pipatanakul","**Improving AI's Understanding of Thai Language and Documents**

Researchers have created a new benchmark called ThaiOCRBench to test the ability of artificial intelligence (AI) models to understand Thai language and documents. Currently, most AI models are trained on high-resource languages, leaving languages like Thai underrepresented. This benchmark aims to address this gap by providing a comprehensive dataset of 2,808 samples across 13 task categories.

The researchers tested various state-of-the-art AI models, both proprietary and open-source, on ThaiOCRBench. The results showed that proprietary models outperformed open-source models, with significant performance drops in tasks like fine-grained text recognition and handwritten content extraction.

The study highlights key challenges in developing AI models for low-resource languages like Thai, including language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing AI models in these settings and offers insights for improving Thai-language document understanding.

**In simple terms:** This research aims to improve AI's ability to understand Thai language and documents. The researchers created a benchmark to test AI models and found that they struggle with tasks like reading Thai text and extracting information from documents. The study provides a framework for developing better AI models for low-resource languages like Thai."
cs.CL,Probabilistic Textual Time Series Depression Detection,"Accurate and interpretable predictions of depression severity are essential for clinical decision support, yet existing models often lack uncertainty estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time Series Depression Detection framework that predicts PHQ-8 scores from utterance-level clinical interviews while modeling uncertainty over time. PTTSD includes sequence-to-sequence and sequence-to-one variants, both combining bidirectional LSTMs, self-attention, and residual connections with Gaussian or Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated prediction intervals. Ablations confirm the value of attention and probabilistic modeling, while comparisons with MentalBERT establish generality. A three-part calibration analysis and qualitative case studies further highlight the interpretability and clinical relevance of uncertainty-aware forecasting.",http://arxiv.org/abs/2511.04476v1,2025-11-06T15:50:33Z,"Fabian Schmidt, Seyedehmoniba Ravan, Vladimir Vlassov","**Breakthrough in Depression Detection: AI Model Predicts Severity with Uncertainty**

Researchers have developed a new AI framework called PTTSD that can accurately predict the severity of depression in individuals by analyzing their speech patterns over time. The model uses clinical interviews to forecast depression scores, providing not only predictions but also estimates of uncertainty.

**What makes PTTSD innovative?**

* It can model depression severity over time, capturing changes and trends in a person's mental health.
* It provides uncertainty estimates, which are crucial for clinical decision-making and can help doctors and therapists better understand the reliability of predictions.

**How well does PTTSD perform?**

* The model achieved state-of-the-art results on two datasets, outperforming existing text-only systems.
* It produced well-calibrated prediction intervals, meaning that the uncertainty estimates are reliable and trustworthy.

**Why is this important?**

* Accurate and interpretable predictions of depression severity can support clinical decision-making and improve patient care.
* The model's ability to provide uncertainty estimates can help clinicians better understand the complexities of depression and make more informed treatment decisions.

**What's next?**

* Further research will focus on refining the model and exploring its applications in real-world clinical settings."
cs.CL,Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge   Graph Augmented LLMs,"Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.",http://arxiv.org/abs/2511.04473v1,2025-11-06T15:45:18Z,"Alberto Cattaneo, Carlo Luschi, Daniel Justus","Here's a summary of the research paper for a general audience:

**Improving AI's Access to Facts**

Large Language Models (LLMs) are a type of artificial intelligence that can process and generate human-like text. However, they sometimes struggle with factual accuracy. One way to improve this is by connecting LLMs to graph-structured knowledge bases, which are like vast networks of information.

The problem is that it's hard to compare different methods for retrieving information from these knowledge bases because there aren't enough good datasets to test them on. A dataset is like a collection of questions and answers that can be used to train and evaluate AI models.

To address this issue, researchers created a framework called SynthKGQA, which can generate high-quality datasets from any knowledge graph. This framework provides a set of ""ground-truth"" facts that the AI can use to reason and answer questions.

Using SynthKGQA, the researchers created a new dataset called GTSQA, which tests the ability of AI models to generalize to new and unseen information. They then used this dataset to evaluate popular solutions for KG-augmented LLMs, which are AI models that use knowledge graphs to improve their performance.

**In Simple Terms**

Imagine you're asking a virtual assistant a question, and it needs to find the answer from a huge database of information. The researchers created a tool that helps virtual assistants do this more accurately by providing a set of correct answers to learn from. This tool can be used to improve virtual assistants and make them more reliable."
stat.ML,Forgetting is Everywhere,"A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.",http://arxiv.org/abs/2511.04666v1,2025-11-06T18:52:57Z,"Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey","**The Science of Forgetting: A New Perspective on Learning**

Imagine you're trying to learn a new skill, but every time you acquire new information, you seem to forget what you learned before. This phenomenon, known as forgetting, is a major challenge in developing artificial intelligence and machine learning algorithms. Researchers have been studying forgetting for decades, but a unified understanding of the concept has been elusive.

A recent study proposes a new theory that characterizes forgetting as a lack of self-consistency in a learner's ability to make predictions about future experiences. In other words, when a learner forgets, they lose predictive information that they had previously acquired. This theory provides a general measure of an algorithm's tendency to forget and can be applied to various learning settings, including classification, regression, generative modeling, and reinforcement learning.

The study's experiments demonstrate that forgetting is a widespread problem that affects learning efficiency across all these settings. The findings provide a foundation for understanding and addressing forgetting in machine learning algorithms, which could lead to more efficient and effective learning systems. Ultimately, this research has the potential to improve the performance of AI systems and pave the way for more advanced learning technologies."
stat.ML,Geometric Decomposition of Statistical Inference through Gradient Flow   and Co-Monotonicity Measures,"Understanding feature-outcome associations in high-dimensional data remains   challenging when relationships vary across subpopulations, yet standard   methods assuming global associations miss context-dependent patterns, reducing   statistical power and interpretability. We develop a geometric decomposition   framework offering two strategies for partitioning inference problems into   regional analyses on data-derived Riemannian graphs. Gradient flow   decomposition uses path-monotonicity-validated discrete Morse theory to   partition samples into basins where outcomes exhibit monotonic behavior.   Co-monotonicity decomposition leverages association structure: vertex-level   coefficients measuring directional concordance between outcome and features,   or between feature pairs, define embeddings of samples into association space.   These embeddings induce Riemannian k-NN graphs on which biclustering   identifies co-monotonicity cells (coherent regions) and feature modules. This   extends naturally to multi-modal integration across multiple feature sets.   Both strategies apply independently or jointly, with Bayesian posterior   sampling providing credible intervals.",http://arxiv.org/abs/2511.04599v1,2025-11-06T17:51:32Z,"Pawel Gajer, Jacques Ravel","**Unlocking Hidden Patterns in Complex Data**

Imagine trying to understand how different factors, such as age, diet, and lifestyle, affect a person's health. Traditional statistical methods assume that these relationships are the same across the entire population. However, in reality, these relationships can vary significantly across different subgroups, making it challenging to identify meaningful patterns.

Researchers have developed a new framework that addresses this challenge by breaking down complex data into smaller, more manageable parts. This approach uses advanced mathematical techniques to identify regions in the data where the relationships between factors and outcomes are consistent.

The framework offers two strategies:

1. **Gradient Flow Decomposition**: This method identifies areas in the data where the outcome (e.g., health) changes smoothly and consistently with a particular factor (e.g., age). This helps to identify subgroups with similar patterns.
2. **Co-Monotonicity Decomposition**: This approach examines the relationships between multiple factors and the outcome, identifying clusters of factors that tend to change together.

Both strategies can be used separately or together, and they provide a way to quantify the uncertainty of the results. This framework has the potential to reveal new insights in complex data, such as:

* Identifying specific subgroups with unique patterns of factor-outcome associations
* Discovering new relationships between factors and outcomes
* Improving the accuracy and interpretability of statistical models

This research has implications for various fields, including medicine, social sciences, and economics, where understanding complex relationships in data is crucial for making informed decisions."
stat.ML,Physics-Informed Neural Networks and Neural Operators for Parametric   PDEs: A Human-AI Collaborative Analysis,"PDEs arise ubiquitously in science and engineering, where solutions depend on parameters (physical properties, boundary conditions, geometry). Traditional numerical methods require re-solving the PDE for each parameter, making parameter space exploration prohibitively expensive. Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. We critically analyze two main paradigms: (1) PINNs, which embed physical laws as soft constraints and excel at inverse problems with sparse data, and (2) neural operators (e.g., DeepONet, Fourier Neural Operator), which learn mappings between infinite-dimensional function spaces and achieve unprecedented generalization. Through comparisons across fluid dynamics, solid mechanics, heat transfer, and electromagnetics, we show neural operators can achieve computational speedups of $10^3$ to $10^5$ times faster than traditional solvers for multi-query scenarios, while maintaining comparable accuracy. We provide practical guidance for method selection, discuss theoretical foundations (universal approximation, convergence), and identify critical open challenges: high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive, incrementally updated resource for this rapidly evolving field",http://arxiv.org/abs/2511.04576v2,2025-11-06T17:31:59Z,"Zhuo Zhang, Xiong Xiong, Sen Zhang, Yuan Zhao, Xi Yang","**Breakthrough in Solving Complex Mathematical Equations**

Scientists and engineers often encounter complex mathematical equations, known as partial differential equations (PDEs), that describe various phenomena in fields like fluid dynamics, solid mechanics, and heat transfer. Solving these equations is crucial, but traditional methods can be time-consuming and expensive, especially when exploring different parameters.

Recently, researchers have made significant progress by combining machine learning with physical laws to solve PDEs more efficiently. They've developed two main approaches:

1. **Physics-Informed Neural Networks (PINNs)**: These networks embed physical laws as soft constraints, making them particularly useful for solving inverse problems with limited data.
2. **Neural Operators**: These learn mappings between infinite-dimensional function spaces, enabling unprecedented generalization and speed.

The study compared these approaches across various fields and found that neural operators can solve PDEs 1,000 to 100,000 times faster than traditional methods while maintaining comparable accuracy. This is particularly useful for scenarios where multiple solutions are needed, such as optimizing system performance or simulating different environmental conditions.

The researchers provide guidance on selecting the best approach, discuss the theoretical foundations, and identify open challenges, including handling high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive resource for this rapidly evolving field.

**In simple terms:** Imagine being able to simulate complex systems, like ocean currents or building structures, much faster and more efficiently. This research brings us closer to that goal by combining machine learning with physical laws, enabling faster and more accurate solutions to complex mathematical equations."
stat.ML,Riesz Regression As Direct Density Ratio Estimation,"Riesz regression has garnered attention as a tool in debiased machine learning for causal and structural parameter estimation (Chernozhukov et al., 2021). This study shows that Riesz regression is closely related to direct density-ratio estimation (DRE) in important cases, including average treat- ment effect (ATE) estimation. Specifically, the idea and objective in Riesz regression coincide with the one in least-squares importance fitting (LSIF, Kanamori et al., 2009) in direct density-ratio estimation. While Riesz regression is general in the sense that it can be applied to Riesz representer estimation in a wide class of problems, the equivalence with DRE allows us to directly import exist- ing results in specific cases, including convergence-rate analyses, the selection of loss functions via Bregman-divergence minimization, and regularization techniques for flexible models, such as neural networks. Conversely, insights about the Riesz representer in debiased machine learning broaden the applications of direct density-ratio estimation methods. This paper consolidates our prior results in Kato (2025a) and Kato (2025b).",http://arxiv.org/abs/2511.04568v1,2025-11-06T17:25:05Z,Masahiro Kato,"**Unlocking Connections between Machine Learning Techniques**

Researchers have discovered a surprising link between two machine learning techniques: Riesz regression and direct density-ratio estimation (DRE). Riesz regression is a tool used to improve the accuracy of machine learning models in certain situations, such as estimating the effect of a treatment. Direct density-ratio estimation is a method used to compare the probability distributions of two groups.

The study found that Riesz regression and a specific type of DRE, called least-squares importance fitting (LSIF), are essentially the same thing. This connection allows researchers to borrow results and techniques from one field and apply them to the other. For example, researchers can use existing methods for choosing the right mathematical functions and regularization techniques to improve the performance of Riesz regression.

This breakthrough has two main implications:

1. **Improved Riesz regression**: By leveraging existing results from DRE, researchers can gain a deeper understanding of Riesz regression, including its convergence rates and optimal loss functions.
2. **Broader applications for DRE**: The connection to Riesz regression also expands the potential applications of DRE, enabling researchers to tackle a wider range of problems.

Overall, this research reveals a powerful connection between two machine learning techniques, enabling researchers to develop more accurate models and tackle complex problems."
stat.ML,Generative Bayesian Filtering and Parameter Learning,"Generative Bayesian Filtering (GBF) provides a powerful and flexible framework for performing posterior inference in complex nonlinear and non-Gaussian state-space models. Our approach extends Generative Bayesian Computation (GBC) to dynamic settings, enabling recursive posterior inference using simulation-based methods powered by deep neural networks. GBF does not require explicit density evaluations, making it particularly effective when observation or transition distributions are analytically intractable. To address parameter learning, we introduce the Generative-Gibbs sampler, which bypasses explicit density evaluation by iteratively sampling each variable from its implicit full conditional distribution. Such technique is broadly applicable and enables inference in hierarchical Bayesian models with intractable densities, including state-space models. We assess the performance of the proposed methodologies through both simulated and empirical studies, including the estimation of $\alpha$-stable stochastic volatility models. Our findings indicate that GBF significantly outperforms existing likelihood-free approaches in accuracy and robustness when dealing with intractable state-space models.",http://arxiv.org/abs/2511.04552v1,2025-11-06T17:04:48Z,"Edoardo Marcelli, Sean O'Hagan, Veronika Rockova","**Unlocking Insights in Complex Systems: A New Approach to Bayesian Filtering and Parameter Learning**

Imagine trying to understand a complex system, like the stock market or the weather, where many factors interact and influence each other. Traditional methods for analyzing such systems often rely on simplifying assumptions or mathematical formulas that don't always hold up in reality. A new research paper introduces a powerful approach called Generative Bayesian Filtering (GBF) that can handle complex, nonlinear systems with greater accuracy and flexibility.

GBF uses simulation-based methods, powered by deep neural networks, to recursively infer the state of a system over time. This approach is particularly useful when the underlying mathematical distributions are unknown or difficult to work with. The researchers also developed a technique called the Generative-Gibbs sampler, which enables efficient learning of model parameters, even when the underlying densities are intractable.

The study demonstrates the effectiveness of GBF and the Generative-Gibbs sampler through simulations and real-world applications, including estimating stochastic volatility models. The results show that GBF significantly outperforms existing methods in terms of accuracy and robustness, especially when dealing with complex systems that are difficult to model.

**In simple terms:** This research presents a new, more powerful approach to analyzing complex systems, which can lead to better insights and predictions in fields like finance, climate science, and more. The approach can handle complex, nonlinear relationships and doesn't require explicit mathematical formulas, making it a valuable tool for researchers and practitioners."
stat.ML,Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom   Parity,"We present a new benchmarking study comparing a boundary-constrained Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical finite element method combined with Crank--Nicolson time stepping (CN-FEM) for solving the two-dimensional wave equation with homogeneous Dirichlet boundary conditions. The B-EPGP construction leverages exponential-polynomial bases derived from the characteristic variety to enforce the PDE and boundary conditions exactly and employs penalized least squares to estimate the coefficients. To ensure fairness across paradigms, we introduce a degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP consistently attains lower space-time $L^2$-error and maximum-in-time $L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of magnitude.",http://arxiv.org/abs/2511.04518v1,2025-11-06T16:33:17Z,"Obed Amo, Samit Ghosh, Markus Lange-Hegermann, Bogdan Raiţă, Michael Pokojovy","**Breakthrough in Solving Complex Math Problems: A New Method Outperforms Traditional Approach**

Researchers have made a significant discovery in the field of mathematics, comparing two methods for solving a fundamental problem in physics: the two-dimensional wave equation. This equation is crucial in understanding various natural phenomena, such as ocean waves, sound waves, and light waves.

The researchers tested a new method called the Boundary-constrained Ehrenpreis-Palamodov Gaussian Process (B-EPGP) against a traditional approach known as the finite element method combined with Crank-Nicolson time stepping (CN-FEM). To ensure a fair comparison, they matched the two methods in terms of complexity, measured by the number of ""degrees of freedom"" (DoF).

The results showed that the B-EPGP method consistently outperformed the traditional CN-FEM approach, achieving much lower errors in both space and time. In fact, the B-EPGP method was about 100 times more accurate than CN-FEM. This breakthrough has the potential to improve our ability to model and simulate complex wave phenomena, leading to advancements in fields such as physics, engineering, and computer science."
stat.ML,ForecastGAN: A Decomposition-Based Adversarial Framework for   Multi-Horizon Time Series Forecasting,"Time series forecasting is essential across domains from finance to supply chain management. This paper introduces ForecastGAN, a novel decomposition based adversarial framework addressing limitations in existing approaches for multi-horizon predictions. Although transformer models excel in long-term forecasting, they often underperform in short-term scenarios and typically ignore categorical features. ForecastGAN operates through three integrated modules: a Decomposition Module that extracts seasonality and trend components; a Model Selection Module that identifies optimal neural network configurations based on forecasting horizon; and an Adversarial Training Module that enhances prediction robustness through Conditional Generative Adversarial Network training. Unlike conventional approaches, ForecastGAN effectively integrates both numerical and categorical features. We validate our framework on eleven benchmark multivariate time series datasets that span various forecasting horizons. The results show that ForecastGAN consistently outperforms state-of-the-art transformer models for short-term forecasting while remaining competitive for long-term horizons. This research establishes a more generalizable approach to time series forecasting that adapts to specific contexts while maintaining strong performance across diverse data characteristics without extensive hyperparameter tuning.",http://arxiv.org/abs/2511.04445v1,2025-11-06T15:19:23Z,"Syeda Sitara Wishal Fatima, Afshin Rahimi","**Improving Time Series Forecasting with ForecastGAN**

Time series forecasting is a crucial task in many fields, such as finance and supply chain management. Researchers have developed various methods to predict future trends, but existing approaches have limitations, particularly when it comes to short-term predictions and incorporating categorical features.

A new framework, called ForecastGAN, has been introduced to address these limitations. ForecastGAN uses a unique combination of three modules:

1. **Decomposition Module**: breaks down time series data into its underlying components, such as seasonality and trend.
2. **Model Selection Module**: chooses the best neural network configuration based on the forecasting horizon (short-term or long-term).
3. **Adversarial Training Module**: enhances prediction robustness through a type of machine learning called Conditional Generative Adversarial Network training.

What sets ForecastGAN apart is its ability to effectively integrate both numerical and categorical features, which is often ignored in existing approaches. The framework was tested on eleven benchmark datasets and showed impressive results:

* **Outperformed state-of-the-art transformer models for short-term forecasting**: ForecastGAN made more accurate predictions for short-term forecasts, which is a significant improvement over existing methods.
* **Remained competitive for long-term horizons**: ForecastGAN performed well for long-term forecasts, making it a robust and reliable approach.

The development of ForecastGAN marks a significant step towards creating a more generalizable approach to time series forecasting. This approach adapts to specific contexts and maintains strong performance across diverse data characteristics without requiring extensive hyperparameter tuning. In simpler terms, ForecastGAN can be applied to various situations and still produce accurate predictions, making it a valuable tool for many industries."
stat.ML,Online Bayesian Experimental Design for Partially Observed Dynamical   Systems,"Bayesian experimental design (BED) provides a principled framework for optimizing data collection, but existing approaches do not apply to crucial real-world settings such as dynamical systems with partial observability, where only noisy and incomplete observations are available. These systems are naturally modeled as state-space models (SSMs), where latent states mediate the link between parameters and data, making the likelihood -- and thus information-theoretic objectives like the expected information gain (EIG) -- intractable. In addition, the dynamical nature of the system requires online algorithms that update posterior distributions and select designs sequentially in a computationally efficient manner. We address these challenges by deriving new estimators of the EIG and its gradient that explicitly marginalize latent states, enabling scalable stochastic optimization in nonlinear SSMs. Our approach leverages nested particle filters (NPFs) for efficient online inference with convergence guarantees. Applications to realistic models, such as the susceptible-infected-recovered (SIR) and a moving source location task, show that our framework successfully handles both partial observability and online computation.",http://arxiv.org/abs/2511.04403v1,2025-11-06T14:29:05Z,"Sara Pérez-Vieites, Sahel Iqbal, Simo Särkkä, Dominik Baumann","**Optimizing Data Collection for Complex Systems**

Imagine trying to understand a complex system, like the spread of a disease or the movement of a object, but you can only get noisy and incomplete information about it. This is a common problem in many fields, from epidemiology to robotics. A new research paper proposes a solution to this problem by developing a framework for optimizing data collection, called online Bayesian experimental design.

The researchers, who published their findings in the paper ""Online Bayesian Experimental Design for Partially Observed Dynamical Systems,"" have created a method that can handle complex systems that change over time and can only be observed indirectly. Their approach uses a technique called nested particle filters to efficiently update our understanding of the system as new data comes in.

**Key Breakthroughs**

The researchers made two key breakthroughs. First, they developed new estimators that can calculate the expected information gain (EIG) of a proposed experiment. The EIG is a measure of how much new information an experiment is likely to provide. Second, they showed how to use these estimators to optimize the design of experiments in real-time.

**What does this mean?**

This research has significant implications for many fields. For example, in epidemiology, it could help optimize the collection of data on disease outbreaks, leading to better understanding and control of the spread of disease. In robotics, it could help optimize the movement of robots in complex environments.

**In simple terms**

The researchers created a framework that helps us collect data more efficiently from complex systems that are hard to observe directly. This framework can be used in real-time, making it practical for many applications. By optimizing data collection, we can gain a better understanding of these complex systems and make more informed decisions."
stat.ML,Simultaneous Optimization of Geodesics and Fréchet Means,"A central part of geometric statistics is to compute the Fr\'echet mean. This is a well-known intrinsic mean on a Riemannian manifold that minimizes the sum of squared Riemannian distances from the mean point to all other data points. The Fr\'echet mean is simple to define and generalizes the Euclidean mean, but for most manifolds even minimizing the Riemannian distance involves solving an optimization problem. Therefore, numerical computations of the Fr\'echet mean require solving an embedded optimization problem in each iteration. We introduce the GEORCE-FM algorithm to simultaneously compute the Fr\'echet mean and Riemannian distances in each iteration in a local chart, making it faster than previous methods. We extend the algorithm to Finsler manifolds and introduce an adaptive extension such that GEORCE-FM scales to a large number of data points. Theoretically, we show that GEORCE-FM has global convergence and local quadratic convergence and prove that the adaptive extension converges in expectation to the Fr\'echet mean. We further empirically demonstrate that GEORCE-FM outperforms existing baseline methods to estimate the Fr\'echet mean in terms of both accuracy and runtime.",http://arxiv.org/abs/2511.04301v1,2025-11-06T12:08:15Z,"Frederik Möbius Rygaard, Søren Hauberg, Steen Markvorsen","**Unlocking Efficient Calculations on Curved Spaces**

Imagine trying to find the average location of a set of points on a map. On a flat surface, this is a simple task, but what if the points are on a curved surface, like a sphere or a mountain range? Mathematicians have developed a concept called the Fréchet mean to solve this problem, which is a way to calculate the average location on curved spaces.

However, calculating the Fréchet mean can be computationally expensive, especially when dealing with large datasets. Researchers have now developed a new algorithm called GEORCE-FM, which simultaneously optimizes the calculation of the Fréchet mean and the distances between points on the curved surface. This approach makes the calculation faster and more efficient.

The GEORCE-FM algorithm has been shown to be globally convergent, meaning it will always find the correct solution, and locally quadratically convergent, meaning it will get faster and more accurate as it gets closer to the solution. The researchers have also developed an adaptive extension of the algorithm that can handle large datasets.

In tests, GEORCE-FM outperformed existing methods in terms of both accuracy and speed. This breakthrough has the potential to enable faster and more accurate analysis of complex data on curved spaces, with applications in fields such as computer science, engineering, and data analysis."
stat.ML,Robustness of Minimum-Volume Nonnegative Matrix Factorization under an   Expanded Sufficiently Scattered Condition,"Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used successfully in many applications, such as hyperspectral imaging, chemical kinetics, spectroscopy, topic modeling, and audio source separation. However, its robustness to noise has been a long-standing open problem. In this paper, we prove that min-vol NMF identifies the groundtruth factors in the presence of noise under a condition referred to as the expanded sufficiently scattered condition which requires the data points to be sufficiently well scattered in the latent simplex generated by the basis vectors.",http://arxiv.org/abs/2511.04291v1,2025-11-06T11:36:32Z,"Giovanni Barbarino, Nicolas Gillis, Subhayan Saha","Here's a summary of the research paper for a general audience:

**Title:** Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition

**What it's about:** Researchers have been working on a mathematical technique called Nonnegative Matrix Factorization (NMF) that helps analyze complex data in various fields, such as chemistry, audio processing, and topic modeling. Specifically, they've been focusing on a variant of NMF called minimum-volume NMF, which has shown great promise in many applications.

**The problem:** However, one major concern with this technique is its sensitivity to noise or errors in the data. In other words, if the data is noisy or imperfect, the results obtained from minimum-volume NMF might not be reliable.

**The breakthrough:** In this paper, the researchers have made a significant progress in addressing this issue. They've discovered a condition, called the ""expanded sufficiently scattered condition"", under which minimum-volume NMF can still produce accurate results even when the data is noisy. This condition essentially requires that the data points are well-distributed and diverse in the underlying space.

**What it means:** This finding is a major step forward in making minimum-volume NMF a more robust and reliable tool for data analysis. It provides a theoretical foundation for the technique's ability to handle noisy data, which is a common problem in many real-world applications. This research has the potential to improve the accuracy and reliability of various data-driven applications, from chemical analysis to audio processing and beyond."
stat.ML,Online Conformal Inference with Retrospective Adjustment for Faster   Adaptation to Distribution Shift,"Conformal prediction has emerged as a powerful framework for constructing distribution-free prediction sets with guaranteed coverage assuming only the exchangeability assumption. However, this assumption is often violated in online environments where data distributions evolve over time. Several recent approaches have been proposed to address this limitation, but, typically, they slowly adapt to distribution shifts because they update predictions only in a forward manner, that is, they generate a prediction for a newly observed data point while previously computed predictions are not updated. In this paper, we propose a novel online conformal inference method with retrospective adjustment, which is designed to achieve faster adaptation to distributional shifts. Our method leverages regression approaches with efficient leave-one-out update formulas to retroactively adjust past predictions when new data arrive, thereby aligning the entire set of predictions with the most recent data distribution. Through extensive numerical studies performed on both synthetic and real-world data sets, we show that the proposed approach achieves faster coverage recalibration and improved statistical efficiency compared to existing online conformal prediction methods.",http://arxiv.org/abs/2511.04275v1,2025-11-06T11:11:51Z,"Jungbin Jun, Ilsang Ohn","**Improving Online Predictions with Retrospective Adjustment**

Imagine you're trying to predict the weather, but the climate is changing over time. Traditional prediction methods might struggle to keep up with these changes. A new approach called online conformal inference with retrospective adjustment aims to improve predictions in such situations.

This method is designed to adapt quickly to changes in data distribution, which is common in online environments. Unlike existing methods that only update predictions as new data arrives, this approach retroactively adjusts past predictions to better match the current data distribution.

The researchers behind this study tested their method on both simulated and real-world data and found that it adapts faster to changes in the data and provides more accurate predictions compared to existing methods. This innovation has the potential to improve predictions in a wide range of applications, from weather forecasting to finance and healthcare, where data distributions are constantly evolving."
stat.ML,On Joint Regularization and Calibration in Deep Ensembles,"Deep ensembles are a powerful tool in machine learning, improving both model performance and uncertainty calibration. While ensembles are typically formed by training and tuning models individually, evidence suggests that jointly tuning the ensemble can lead to better performance. This paper investigates the impact of jointly tuning weight decay, temperature scaling, and early stopping on both predictive performance and uncertainty quantification. Additionally, we propose a partially overlapping holdout strategy as a practical compromise between enabling joint evaluation and maximizing the use of data for training. Our results demonstrate that jointly tuning the ensemble generally matches or improves performance, with significant variation in effect size across different tasks and metrics. We highlight the trade-offs between individual and joint optimization in deep ensemble training, with the overlapping holdout strategy offering an attractive practical solution. We believe our findings provide valuable insights and guidance for practitioners looking to optimize deep ensemble models. Code is available at: https://github.com/lauritsf/ensemble-optimality-gap",http://arxiv.org/abs/2511.04160v2,2025-11-06T08:04:19Z,"Laurits Fredsgaard, Mikkel N. Schmidt","**Improving Deep Learning Models: A New Approach to Ensemble Optimization**

Deep learning models are a powerful tool for making predictions, but they can be improved by combining multiple models into an ""ensemble"". Researchers have found that tuning these ensembles individually can lead to suboptimal performance. A new study explores the benefits of jointly tuning deep ensembles, which involves adjusting multiple models together to achieve better results.

The study focused on three key aspects of ensemble optimization: weight decay, temperature scaling, and early stopping. By jointly tuning these factors, the researchers found that ensemble performance and uncertainty calibration can be significantly improved. However, they also identified trade-offs between individual and joint optimization, highlighting the need for a practical compromise.

To address this challenge, the researchers proposed a ""partially overlapping holdout strategy"", which allows for joint evaluation while still maximizing the use of data for training. This approach offers a practical solution for optimizing deep ensemble models.

The study's findings provide valuable insights and guidance for practitioners looking to improve the performance of their deep learning models. By adopting a joint optimization approach, researchers and developers can potentially achieve better results and more accurate uncertainty estimates. The code for the study is publicly available, making it easy for others to build on these findings."
stat.ML,Towards Scalable Meta-Learning of near-optimal Interpretable Models via   Synthetic Model Generations,"Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.",http://arxiv.org/abs/2511.04000v1,2025-11-06T02:50:23Z,"Kyaw Hpone Myint, Zhe Wu, Alexandre G. R. Day, Giri Iyengar","Here's a summary of the research paper for a general audience:

**Making Decision Trees Smarter and More Efficient**

Decision trees are a popular tool used in important fields like finance and healthcare because they are easy to understand. However, creating accurate decision trees can be time-consuming and requires a lot of data. Researchers have found a way to speed up this process by generating artificial data that mimics real-world data. This approach uses a special algorithm to create many sample decision trees that are nearly optimal, or very close to the best possible solution.

By training a machine learning model on these artificial decision trees, researchers were able to achieve similar performance to training on real-world data or using highly accurate decision trees. The best part is that this method is much faster and more flexible, making it possible to create smarter and more efficient decision trees that are still easy to understand. This breakthrough has the potential to make decision trees more widely available and useful in a variety of applications."
stat.ML,Non-Asymptotic Optimization and Generalization Bounds for Stochastic   Gauss-Newton in Overparameterized Models,"An important question in deep learning is how higher-order optimization methods affect generalization. In this work, we analyze a stochastic Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in a regression setting. Our theoretical contributions are twofold. First, we establish finite-time convergence bounds via a variable-metric analysis in parameter space, with explicit dependencies on the batch size, network width and depth. Second, we derive non-asymptotic generalization bounds for SGN using uniform stability in the overparameterized regime, characterizing the impact of curvature, batch size, and overparameterization on generalization performance. Our theoretical results identify a favorable generalization regime for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along the optimization path yields tighter stability bounds.",http://arxiv.org/abs/2511.03972v1,2025-11-06T01:50:45Z,Semih Cayci,"Here's a summary of the research paper for a general audience:

**How a Popular Optimization Method Affects Performance in Deep Learning Models**

Deep learning models, like those used in image and speech recognition, are often made up of many layers and millions of parameters. Training these models involves adjusting these parameters to minimize errors, a process known as optimization. Researchers have long wondered how different optimization methods impact the performance of these models.

This study focuses on a specific optimization method called the Gauss-Newton method, which is commonly used in machine learning. The researchers analyzed how this method, combined with a technique called Levenberg-Marquardt damping and mini-batch sampling, affects the performance of deep neural networks.

The study's main findings are:

1. **Faster Convergence**: The researchers showed that the Gauss-Newton method converges to a good solution in a finite amount of time, and they provided mathematical bounds on how quickly it converges. These bounds depend on factors like the size of the mini-batches used to train the model, the width and depth of the network, and the number of parameters.
2. **Better Generalization**: The researchers also derived bounds on how well the model generalizes to new, unseen data. They found that the Gauss-Newton method can lead to better generalization performance when the model has a certain number of parameters (overparameterization) and when the curvature of the optimization problem is favorable.

In simple terms, the study provides new insights into how the Gauss-Newton optimization method affects the performance of deep learning models. The results suggest that this method can lead to faster convergence and better generalization performance, especially when the model has a large number of parameters and is trained with a suitable batch size."
stat.ML,Robust inference using density-powered Stein operators,"We introduce a density-power weighted variant for the Stein operator, called the $\gamma$-Stein operator. This is a novel class of operators derived from the $\gamma$-divergence, designed to build robust inference methods for unnormalized probability models. The operator's construction (weighting by the model density raised to a positive power $\gamma$ inherently down-weights the influence of outliers, providing a principled mechanism for robustness. Applying this operator yields a robust generalization of score matching that retains the crucial property of being independent of the model's normalizing constant. We extend this framework to develop two key applications: the $\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and $\gamma$-Stein variational gradient descent for robust Bayesian posterior approximation. Empirical results on contaminated Gaussian and quartic potential models show our methods significantly outperform standard baselines in both robustness and statistical efficiency.",http://arxiv.org/abs/2511.03963v1,2025-11-06T01:32:17Z,Shinto Eguchi,"**New Statistical Method Improves Robustness in Data Analysis**

Researchers have developed a novel statistical method that enhances the reliability of data analysis by reducing the impact of outliers or erroneous data points. This method, called the $\gamma$-Stein operator, uses a weighted approach to downplay the influence of unusual data, making it more robust than traditional methods.

The $\gamma$-Stein operator is particularly useful for analyzing complex data models where the normalizing constant is unknown. It generalizes a popular technique called score matching, making it more resilient to outliers while maintaining its accuracy.

The researchers applied their method to two key applications:

1. **Goodness-of-fit testing**: They developed a robust test to determine if a statistical model accurately represents the underlying data distribution.
2. **Bayesian posterior approximation**: They created a robust method for approximating the posterior distribution in Bayesian inference, which is essential for making predictions and estimating uncertainties.

The results show that the new method outperforms traditional approaches in both robustness and statistical efficiency, especially when dealing with contaminated or noisy data. This breakthrough has the potential to improve the accuracy and reliability of data-driven decision-making in various fields."
stat.ML,Conditional Score Learning for Quickest Change Detection in Markov   Transition Kernels,"We address the problem of quickest change detection in Markov processes with unknown transition kernels. The key idea is to learn the conditional score $\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs $( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are high-dimensional data generated by the same transition kernel. In this way, we avoid explicit likelihood evaluation and provide a practical way to learn the transition dynamics. Based on this estimation, we develop a score-based CUSUM procedure that uses conditional Hyvarinen score differences to detect changes in the kernel. To ensure bounded increments, we propose a truncated version of the statistic. With Hoeffding's inequality for uniformly ergodic Markov processes, we prove exponential lower bounds on the mean time to false alarm. We also prove asymptotic upper bounds on detection delay. These results give both theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models.",http://arxiv.org/abs/2511.03953v1,2025-11-06T01:07:36Z,"Wuxia Chen, Taposh Banerjee, Vahid Tarokh","**Detecting Changes in Complex Systems**

Imagine you're monitoring a complex system, like a power grid or a financial market, and you want to detect any changes that might indicate a problem. But, the system is so complex that you don't fully understand how it works, making it hard to detect changes.

Researchers have developed a new method to tackle this challenge. They focus on learning the ""conditional score,"" which is a way to understand how the system changes from one state to another. By analyzing pairs of data points from the system, they can learn this score without needing to know the underlying details of the system.

Using this score, they've created a new procedure to detect changes in the system. This procedure, called a score-based CUSUM, looks for differences in the conditional score to identify changes. To make the procedure more reliable, they've also developed a way to truncate the statistic, which helps prevent false alarms.

The researchers have tested their method and obtained promising results. They've proven that their approach provides strong guarantees against false alarms and can detect changes quickly. This work has important implications for monitoring complex systems in various fields, such as finance, engineering, and healthcare. The new method offers a practical and theoretically sound way to detect changes in high-dimensional Markov models."
stat.ML,High-dimensional limit theorems for SGD: Momentum and Adaptive   Step-sizes,"We develop a high-dimensional scaling limit for Stochastic Gradient Descent with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework to rigourously compare online SGD with some of its popular variants. We show that the scaling limits of SGD-M coincide with those of online SGD after an appropriate time rescaling and a specific choice of step-size. However, if the step-size is kept the same between the two algorithms, SGD-M will amplify high-dimensional effects, potentially degrading performance relative to online SGD. We demonstrate our framework on two popular learning problems: Spiked Tensor PCA and Single Index Models. In both cases, we also examine online SGD with an adaptive step-size based on normalized gradients. In the high-dimensional regime, this algorithm yields multiple benefits: its dynamics admit fixed points closer to the population minimum and widens the range of admissible step-sizes for which the iterates converge to such solutions. These examples provide a rigorous account, aligning with empirical motivation, of how early preconditioners can stabilize and improve dynamics in settings where online SGD fails.",http://arxiv.org/abs/2511.03952v1,2025-11-06T01:05:18Z,"Aukosh Jagannath, Taj Jones-McCormick, Varnan Sarangian","**Unlocking the Secrets of Stochastic Gradient Descent**

Imagine you're trying to teach a computer to recognize pictures of cats and dogs. You show it many pictures, and it tries to learn from them. But how does it learn? One popular way is called Stochastic Gradient Descent (SGD). It's like taking small steps towards the right answer, based on each picture.

Researchers have been working on improving SGD. They've added things like ""momentum"" (think of it like taking bigger strides) and ""adaptive step-sizes"" (like adjusting your stride based on the terrain). But do these improvements really help?

This study looks at how these improved versions of SGD work, especially when dealing with many, many features (like pixels in a picture). The researchers found that adding momentum can actually make things worse if not done carefully. But, they also discovered that using an adaptive step-size can help.

**What does this mean?**

* Adaptive step-sizes can help the computer learn better and faster.
* Momentum can be helpful, but only if used carefully.
* These improvements can be especially useful when dealing with complex problems, like recognizing patterns in large datasets.

**Why does this matter?**

Understanding how these improved versions of SGD work can help us build better computer models that can learn from large datasets. This can lead to breakthroughs in areas like image recognition, natural language processing, and more.

In short, this study provides new insights into how to improve Stochastic Gradient Descent, a popular algorithm used in machine learning. By understanding how these improvements work, we can build more efficient and effective computer models."
stat.ML,A general technique for approximating high-dimensional empirical kernel   matrices,"We present simple, user-friendly bounds for the expected operator norm of a random kernel matrix under general conditions on the kernel function $k(\cdot,\cdot)$. Our approach uses decoupling results for U-statistics and the non-commutative Khintchine inequality to obtain upper and lower bounds depending only on scalar statistics of the kernel function and a ``correlation kernel'' matrix corresponding to $k(\cdot,\cdot)$. We then apply our method to provide new, tighter approximations for inner-product kernel matrices on general high-dimensional data, where the sample size and data dimension are polynomially related. Our method obtains simplified proofs of existing results that rely on the moment method and combinatorial arguments while also providing novel approximation results for the case of anisotropic Gaussian data. Finally, using similar techniques to our approximation result, we show a tighter lower bound on the bias of kernel regression with anisotropic Gaussian data.",http://arxiv.org/abs/2511.03892v1,2025-11-05T22:36:52Z,"Chiraag Kaushik, Justin Romberg, Vidya Muthukumar","Here's a summary of the research paper for a general audience:

**Simplifying Complex Data Analysis**

Researchers have developed a new technique to simplify the analysis of complex data sets. When working with large amounts of data, it's often necessary to calculate something called a ""kernel matrix"" to understand the relationships between different data points. However, as the amount of data grows, calculating this matrix can become computationally expensive.

The researchers have found a way to approximate the kernel matrix using simple and easy-to-understand formulas. Their approach works by using mathematical tools to bound the error of the approximation, ensuring that it's accurate and reliable.

The technique has several benefits. It can be applied to a wide range of data types, including high-dimensional data, which is common in fields like computer vision and genomics. The researchers have also used their technique to improve our understanding of kernel regression, a popular method for making predictions from data.

The new technique provides a more efficient and accurate way to analyze complex data sets, which could lead to breakthroughs in various fields, from medicine to finance. By making it easier to work with large data sets, the researchers hope to enable new discoveries and insights that would be difficult or impossible to achieve with traditional methods."
stat.ML,Higher-Order Causal Structure Learning with Additive Models,"Causal structure learning has long been the central task of inferring causal insights from data. Despite the abundance of real-world processes exhibiting higher-order mechanisms, however, an explicit treatment of interactions in causal discovery has received little attention. In this work, we focus on extending the causal additive model (CAM) to additive models with higher-order interactions. This second level of modularity we introduce to the structure learning problem is most easily represented by a directed acyclic hypergraph which extends the DAG. We introduce the necessary definitions and theoretical tools to handle the novel structure we introduce and then provide identifiability results for the hyper DAG, extending the typical Markov equivalence classes. We next provide insights into why learning the more complex hypergraph structure may actually lead to better empirical results. In particular, more restrictive assumptions like CAM correspond to easier-to-learn hyper DAGs and better finite sample complexity. We finally develop an extension of the greedy CAM algorithm which can handle the more complex hyper DAG search space and demonstrate its empirical usefulness in synthetic experiments.",http://arxiv.org/abs/2511.03831v1,2025-11-05T19:55:55Z,"James Enouen, Yujia Zheng, Ignavier Ng, Yan Liu, Kun Zhang","**Unlocking Complex Relationships: A New Approach to Understanding Cause and Effect**

Imagine you're trying to understand how different factors, like weather, traffic, and road conditions, affect your daily commute. Traditional methods for analyzing cause-and-effect relationships assume that these factors interact with each other in simple, linear ways. However, real-world processes often involve more complex interactions, like how traffic congestion affects road conditions, which in turn affects your commute.

Researchers have developed a new approach to uncovering these complex relationships, called higher-order causal structure learning. This method uses a type of mathematical model called an additive model, which can capture interactions between multiple factors. The researchers have also created a new way to represent these complex relationships using a directed acyclic hypergraph, which is like a flowchart that shows how different factors interact with each other.

The study shows that this new approach can lead to better results in understanding cause-and-effect relationships, especially in complex systems. By considering these higher-order interactions, researchers can identify more accurate relationships between factors and make more reliable predictions. For example, in the context of the daily commute, this approach could help identify how different factors interact to cause traffic congestion, allowing for more effective solutions to be developed.

The researchers have also developed a new algorithm, an extension of the greedy CAM algorithm, which can efficiently search for these complex relationships. They tested this algorithm using computer simulations and found that it works well in practice. This new approach has the potential to improve our understanding of complex systems and lead to more effective solutions in a wide range of fields, from transportation to healthcare."
stat.ML,Learning Paths for Dynamic Measure Transport: A Control Perspective,We bring a control perspective to the problem of identifying paths of measures for sampling via dynamic measure transport (DMT). We highlight the fact that commonly used paths may be poor choices for DMT and connect existing methods for learning alternate paths to mean-field games. Based on these connections we pose a flexible family of optimization problems for identifying tilted paths of measures for DMT and advocate for the use of objective terms which encourage smoothness of the corresponding velocities. We present a numerical algorithm for solving these problems based on recent Gaussian process methods for solution of partial differential equations and demonstrate the ability of our method to recover more efficient and smooth transport models compared to those which use an untilted reference path.,http://arxiv.org/abs/2511.03797v1,2025-11-05T19:02:23Z,"Aimee Maurais, Bamdad Hosseini, Youssef Marzouk","Here's a summary of the research paper for a general audience:

**Title:** Finding Better Routes for Sampling: A New Approach to Dynamic Measure Transport

**What it's about:** Imagine you want to understand a complex system, like the behavior of a crowd or the movement of particles. To do this, you need to take samples of the system at different points in time. But how do you ensure that these samples accurately represent the system? That's where dynamic measure transport (DMT) comes in. DMT is a mathematical technique that helps us understand how to move samples from one point in time to another.

**The problem:** Currently, people often use simple, straight-line paths to move these samples, but this can lead to inaccurate results. Think of it like trying to navigate through a crowded city using only main roads - you might miss important details.

**The solution:** The researchers in this paper propose a new approach to DMT that uses a control perspective. They suggest finding more flexible and efficient paths for moving samples, which they call ""tilted paths"". These paths are designed to be smooth and efficient, reducing the risk of inaccurate results.

**The method:** The researchers developed a numerical algorithm to solve optimization problems that identify the best tilted paths. They used a recent technique called Gaussian process methods to solve partial differential equations, which are complex mathematical equations that describe how systems change over time.

**The result:** The researchers demonstrated that their method can recover more efficient and smooth transport models compared to traditional methods that use simple, straight-line paths. This means that their approach can lead to more accurate results when sampling complex systems.

**Why it matters:** This research has implications for various fields, such as physics, engineering, and data science, where understanding complex systems is crucial. By finding better routes for sampling, researchers can gain a deeper understanding of these systems and make more accurate predictions."
