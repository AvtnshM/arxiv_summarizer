category,title,summary,link,published,authors,summary_short
cs.LG,Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything,"Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.",http://arxiv.org/abs/2511.02834v2,2025-11-04T18:59:09Z,"Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh","Here's a summary of the research paper for a general audience:

**Introducing Agent-Omni: A Breakthrough in Multimodal Reasoning**

Imagine having a conversational AI that can understand and respond to a wide range of inputs, including text, images, audio, and video. Current AI models are limited to specific types of data and require extensive training to work with multiple formats. Researchers have now developed a new framework called Agent-Omni, which enables flexible and robust multimodal reasoning without requiring costly retraining.

**How it works**

Agent-Omni uses a master-agent system that coordinates existing AI models, or ""agents,"" to work together to understand and respond to user queries. The master agent interprets the user's intent, delegates tasks to specialized agents, and integrates their outputs into a coherent response. This approach allows Agent-Omni to seamlessly integrate with a wide range of AI models, making it adaptable to diverse inputs and transparent in its decision-making process.

**Key benefits**

The Agent-Omni framework has several key benefits:

* **Flexibility**: It can work with various types of data, including text, images, audio, and video.
* **State-of-the-art performance**: Agent-Omni achieves top-notch performance on a range of tasks, particularly those requiring complex cross-modal reasoning.
* **Modularity**: The framework is easily extensible, allowing researchers to integrate new AI models and improve its capabilities over time.

**The future of AI**

The development of Agent-Omni marks an important step towards creating more versatile and intelligent AI systems. Its modular design and adaptability make it an attractive solution for a wide range of applications, from chatbots and virtual assistants to more complex tasks like multimedia analysis and decision-making."
cs.LG,In Good GRACEs: Principled Teacher Selection for Knowledge Distillation,"Knowledge distillation is an efficient strategy to use data generated by large ""teacher"" language models to train smaller capable ""student"" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.",http://arxiv.org/abs/2511.02833v1,2025-11-04T18:58:47Z,"Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Sham Kakade, Surbhi Goel","Here's a summary of the research paper for a general audience:

**Efficiently Training AI Models with ""Teacher"" Help**

Imagine trying to train a small AI model to perform a specific task, but instead of starting from scratch, you could leverage the knowledge of a much larger, more powerful AI model. This technique is called ""knowledge distillation."" However, finding the best larger model (or ""teacher"") to help train the smaller model (or ""student"") can be a time-consuming and costly process.

Researchers have developed a new method called GRACE, which helps identify the most effective teacher for a given student model and task. GRACE uses a simple score to predict how well a teacher will help a student model learn, without needing to test the teacher or student on actual data.

In tests, GRACE was able to accurately predict which teachers would help student models perform best, with a strong correlation of up to 86%. By using GRACE to select the best teacher, researchers were able to improve student model performance by up to 7.4% compared to using a randomly selected teacher.

GRACE also provides guidance on other important aspects of knowledge distillation, such as how to generate data from the teacher and which teacher to choose given certain constraints. Overall, GRACE offers a efficient and effective way to identify the best teacher for a given student model, making it easier to train high-performing AI models."
cs.LG,"TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System","Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .",http://arxiv.org/abs/2511.02832v1,2025-11-04T18:58:35Z,"Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu","**Breakthrough in Humanoid Robotics: Introducing TWIST2**

Imagine a robot that can move and perform tasks like a human. To make this a reality, researchers need a lot of data on how humans move and interact with their environment. However, collecting this data has been a challenge, especially for humanoid robots. 

A team of researchers has developed a new system called TWIST2, which makes it easy to collect data on human movements and transfer them to humanoid robots. This system uses a virtual reality (VR) headset and a custom-built robot neck to track a person's movements and translate them into robot actions.

The best part? TWIST2 is portable, affordable, and doesn't require expensive equipment. It can collect a large amount of data quickly and efficiently, with a near 100% success rate. 

The researchers also developed a new framework that allows robots to learn from this data and perform complex tasks on their own, such as manipulating objects and kicking. 

The TWIST2 system and the collected dataset are open-sourced, making it accessible to anyone who wants to advance humanoid robotics. This breakthrough has the potential to accelerate progress in robotics and create more sophisticated humanoid robots that can assist and interact with humans in various settings."
cs.LG,GeoCrossBench: Cross-Band Generalization for Remote Sensing,"The number and diversity of remote sensing satellites grows over time, while the vast majority of labeled data comes from older satellites. As the foundation models for Earth observation scale up, the cost of (re-)training to support new satellites grows too, so the generalization capabilities of the models towards new satellites become increasingly important. In this work we introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a new evaluation protocol: it tests the in-distribution performance; generalization to satellites with no band overlap; and generalization to satellites with additional bands with respect to the training set. We also develop a self-supervised extension of ChannelViT, ChiViT, to improve its cross-satellite performance. First, we show that even the best foundation models for remote sensing (DOFA, TerraFM) do not outperform general purpose models like DINOv3 in the in-distribution setting. Second, when generalizing to new satellites with no band overlap, all models suffer 2-4x drop in performance, and ChiViT significantly outperforms the runner-up DINOv3. Third, the performance of all tested models drops on average by 5-25\% when given additional bands during test time. Finally, we show that fine-tuning just the last linear layer of these models using oracle labels from all bands can get relatively consistent performance across all satellites, highlighting that the benchmark is far from being saturated. We publicly release the code and the datasets to encourage the development of more future-proof remote sensing models with stronger cross-satellite generalization.",http://arxiv.org/abs/2511.02831v1,2025-11-04T18:58:20Z,"Hakob Tamazyan, Ani Vanyan, Alvard Barseghyan, Anna Khosrovyan, Evan Shelhamer, Hrant Khachatrian","**Improving Remote Sensing Models for a Changing World**

Remote sensing satellites play a crucial role in monitoring our planet, but the technology behind these satellites is constantly evolving. This creates a challenge for machine learning models that are trained on data from older satellites and need to work with new ones. A team of researchers has developed a new benchmark, called GeoCrossBench, to test how well these models can generalize to new satellites.

The researchers found that even the best models for remote sensing struggle to perform well when faced with new satellites, especially when the new satellites have different types of sensors or more sensors than the ones used for training. They also developed a new model, called ChiViT, which showed improved performance when generalizing to new satellites.

The study's key findings include:

* Even the best models for remote sensing don't perform well when faced with new satellites.
* When new satellites have different sensors, all models struggle, but ChiViT performs better.
* When new satellites have more sensors, all models' performance drops, but fine-tuning the models can improve results.

The researchers have made their code and data publicly available, encouraging others to develop more robust remote sensing models that can adapt to changing satellite technology. This work has important implications for applications such as environmental monitoring, crop management, and disaster response, where accurate and reliable remote sensing data is critical."
cs.LG,Accelerated Frank-Wolfe Algorithms: Complementarity Conditions and   Sparsity,"We develop new accelerated first-order algorithms in the Frank-Wolfe (FW) family for minimizing smooth convex functions over compact convex sets, with a focus on two prominent constraint classes: (1) polytopes and (2) matrix domains given by the spectrahedron and the unit nuclear-norm ball. A key technical ingredient is a complementarity condition that captures solution sparsity -- face dimension for polytopes and rank for matrices. We present two algorithms: (1) a purely linear optimization oracle (LOO) method for polytopes that has optimal worst-case first-order (FO) oracle complexity and, aside of a finite \emph{burn-in} phase and up to a logarithmic factor, has LOO complexity that scales with $r/\sqrt{\epsilon}$, where $\epsilon$ is the target accuracy and $r$ is the solution sparsity $r$ (independently of the ambient dimension), and (2) a hybrid scheme that combines FW with a sparse projection oracle (e.g., low-rank SVDs for matrix domains with low-rank solutions), which also has optimal FO oracle complexity, and after a finite burn-in phase, only requires $O(1/\sqrt{\epsilon})$ sparse projections and LOO calls (independently of both the ambient dimension and the rank of optimal solutions). Our results close a gap on how to accelerate recent advancements in linearly-converging FW algorithms for strongly convex optimization, without paying the price of the dimension.",http://arxiv.org/abs/2511.02821v1,2025-11-04T18:47:07Z,Dan Garber,"**Breakthrough in Optimization Algorithms: Accelerated Frank-Wolfe Methods**

Researchers have made a significant advancement in developing faster algorithms for solving complex optimization problems. These problems involve minimizing a smooth function over a specific region, with applications in various fields such as machine learning, computer vision, and data analysis.

The team has created two new algorithms, both of which are improvements over existing methods. The key innovation is a ""complementarity condition"" that helps identify the sparse structure of the solution. Sparsity refers to the idea that many solutions to these optimization problems have a simple, compact representation.

The first algorithm is particularly efficient for problems with polytope constraints, which are common in machine learning and statistics. It achieves optimal performance guarantees and requires a number of operations that scales with the solution's sparsity, rather than the problem's overall size.

The second algorithm is a hybrid method that combines the strengths of different optimization techniques. It achieves similar performance guarantees and requires a significantly reduced number of operations, especially for problems with matrix constraints. This is particularly important for applications involving large datasets and high-dimensional matrices.

The implications of this research are significant, as it provides a way to accelerate recent advancements in optimization algorithms without sacrificing performance. This could lead to faster and more efficient solutions for a wide range of applications, from image and video processing to natural language processing and recommendation systems."
cs.LG,Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning,"Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .",http://arxiv.org/abs/2511.02818v1,2025-11-04T18:43:44Z,"Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu","**Breakthrough in Tabular Data Analysis: Introducing Orion-MSP**

Tabular data, which consists of rows and columns of information, is a common format used in many real-world applications. However, analyzing and making predictions from this type of data can be challenging due to the complexity of the relationships between different features. Researchers have made significant progress in developing neural models that can learn from tabular data, but existing models have limitations.

A team of researchers has introduced a new model called Orion-MSP, which addresses these limitations and achieves state-of-the-art performance in tabular data analysis. The key innovations of Orion-MSP include:

* **Multi-scale processing**: Orion-MSP can capture complex interactions between features at different scales, allowing it to better understand the relationships between them.
* **Efficient attention mechanism**: The model uses a block-sparse attention mechanism that enables it to focus on the most relevant features while reducing computational costs.
* **Bidirectional information flow**: Orion-MSP allows for safe bidirectional communication between different components of the model, enabling it to refine its representations and make more accurate predictions.

The researchers tested Orion-MSP on diverse benchmarks and found that it matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables. This breakthrough has the potential to improve the analysis and prediction of tabular data in various applications, and the model is publicly available for further research and development."
cs.LG,Assessing win strength in MLB win prediction models,"In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset. In addition, we relate the win probabilities produced by these models to win strength as measured by score differential. In doing so we show that the most common machine learning models do indeed demonstrate a relationship between predicted win probability and the strength of the win. Finally, we analyze the results of using predicted win probabilities as a decision making mechanism on run-line betting. We demonstrate positive returns when utilizing appropriate betting strategies, and show that naive use of machine learning models for betting lead to significant loses.",http://arxiv.org/abs/2511.02815v1,2025-11-04T18:40:10Z,"Morgan Allen, Paul Savala","**Predicting Baseball Game Outcomes: A New Approach**

Researchers have been working to develop machine learning models that can predict which team will win a baseball game. In a new study, a team of researchers trained several of these models using a shared dataset to see how well they could predict game outcomes. They also explored how well the models' predictions correlated with the actual strength of a team's win, measured by the difference in score.

The study found that most machine learning models were able to accurately predict win strength, meaning that teams with higher predicted win probabilities tended to win by larger margins. The researchers then used these predictions to inform betting strategies on run-line bets, a type of bet where the goal is to predict the margin of victory.

The study showed that using machine learning models to inform betting strategies can lead to positive returns, but only if done carefully. If used naively, these models can lead to significant losses. The researchers' findings have implications for teams, fans, and bettors looking to gain a edge in predicting baseball game outcomes."
cs.LG,TabTune: A Unified Library for Inference and Fine-Tuning Tabular   Foundation Models,"Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.",http://arxiv.org/abs/2511.02802v2,2025-11-04T18:25:17Z,"Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu","Here's a summary of the research paper for a general audience:

**Introducing TabTune: A Game-Changer for Working with Tabular Data**

Imagine having a toolbox that makes it easy to work with complex data, like tables and spreadsheets. That's what TabTune is - a unified library that helps researchers and developers use ""tabular foundation models"" more efficiently. These models are like super-powerful computers that can learn from large amounts of data, but they were hard to use with tabular data.

**The Problem: Working with Tabular Data is Hard**

Currently, working with tabular data requires a lot of manual work, like preparing the data, choosing the right model, and fine-tuning it. This process is time-consuming and often leads to inconsistent results. Moreover, it's challenging to compare different models and methods, making it hard to know what's working best.

**TabTune: A Solution**

TabTune solves these problems by providing a single interface to access seven state-of-the-art models. It automates many tasks, such as data preparation and model evaluation, making it easier to use and compare different models. With TabTune, users can:

* Easily try out different models and methods
* Evaluate model performance, accuracy, and fairness
* Fine-tune models for specific tasks

**Why it Matters**

TabTune makes it easier for researchers and developers to work with tabular data, which is a crucial step in many fields, such as business, healthcare, and finance. By streamlining the process, TabTune enables more people to use powerful models to gain insights and make better decisions. Ultimately, TabTune has the potential to accelerate progress in many areas by making it easier to work with complex data."
cs.LG,"Fast, Private, and Protected: Safeguarding Data Privacy and Defending   Against Model Poisoning Attacks in Federated Learning","Federated Learning (FL) is a distributed training paradigm wherein participants collaborate to build a global model while ensuring the privacy of the involved data, which remains stored on participant devices. However, proposals aiming to ensure such privacy also make it challenging to protect against potential attackers seeking to compromise the training outcome. In this context, we present Fast, Private, and Protected (FPP), a novel approach that aims to safeguard federated training while enabling secure aggregation to preserve data privacy. This is accomplished by evaluating rounds using participants' assessments and enabling training recovery after an attack. FPP also employs a reputation-based mechanism to mitigate the participation of attackers. We created a dockerized environment to validate the performance of FPP compared to other approaches in the literature (FedAvg, Power-of-Choice, and aggregation via Trimmed Mean and Median). Our experiments demonstrate that FPP achieves a rapid convergence rate and can converge even in the presence of malicious participants performing model poisoning attacks.",http://arxiv.org/abs/2511.02797v1,2025-11-04T18:20:45Z,"Nicolas Riccieri Gardin Assumpcao, Leandro Villas","Here's a summary of the research paper for a general audience:

**Protecting Sensitive Data in AI Training**

Imagine you're training a self-driving car system, but you don't want to share the sensitive data used to train it, such as video footage of pedestrians or traffic patterns. Federated Learning (FL) is a way to train AI models without sharing the data, by having multiple devices work together to build a global model. However, this approach can be vulnerable to attacks, where malicious actors try to compromise the training process.

Researchers have developed a new approach called Fast, Private, and Protected (FPP), which aims to safeguard FL while preserving data privacy. FPP uses a secure way to combine data from different devices, and includes a reputation system to detect and prevent malicious actors from participating.

**Key Benefits**

* **Fast and Accurate**: FPP enables rapid convergence of the AI model, even in the presence of malicious actors.
* **Private**: FPP preserves the privacy of sensitive data, which remains stored on individual devices.
* **Protected**: FPP defends against model poisoning attacks, where malicious actors try to compromise the training process.

**Real-World Impact**

FPP has significant implications for various industries, such as healthcare, finance, and transportation, where sensitive data is used to train AI models. By providing a secure and private way to train AI models, FPP can help build trust in AI systems and ensure that they are trained on high-quality data."
cs.LG,Can LLMs subtract numbers?,"We present a systematic study of subtraction in large language models (LLMs). While prior benchmarks emphasize addition and multiplication, subtraction has received comparatively little attention despite being structurally distinct as a non-commutative operation. We evaluate eight pretrained LLMs spanning four families on addition and subtraction problems. Our experiments reveal that subtraction accuracy lags behind addition by a wide margin. We find that the errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs frequently produce the correct magnitude but omit the negative sign. Probing analyses show that LLMs internally encode whether results should be negative, yet this information is often not reflected in generated outputs. We further test well-known techniques such as few-shot learning and instruction-tuning to see if they can improve the LLMs' performance. Our results suggest that while few-shot prompting yields modest gains, the instruction-tuned models achieve near-perfect accuracies in generating the negative sign. Together, these findings provide a clearer characterization of the limitations and recoverability of LLMs' arithmetic capabilities in subtraction.",http://arxiv.org/abs/2511.02795v1,2025-11-04T18:20:17Z,"Mayank Jobanputra, Nils Philipp Walter, Maitrey Mehta, Blerta Veseli, Evan Parker Kelly Chapple, Yifan Wang, Sneha Chetani, Ellie Pavlick, Antonio Vergari, Vera Demberg","**Can Large Language Models Subtract Numbers?**

Large language models (LLMs) are artificial intelligence systems that can understand and generate human-like text. But can they perform basic math operations like subtraction? A recent study investigated this question and found that LLMs struggle with subtraction, especially when the result is a negative number.

The study tested eight LLMs on addition and subtraction problems and found that while they were good at addition, they were much less accurate at subtraction. When the result of a subtraction problem was negative (e.g., 2 - 5 = -3), the LLMs often got the magnitude right (e.g., 3) but forgot to include the negative sign.

The researchers also tried to improve the LLMs' performance using techniques like few-shot learning (providing a few examples to learn from) and instruction-tuning (fine-tuning the model on specific tasks). They found that these techniques helped, especially instruction-tuning, which enabled the models to generate negative signs correctly.

Overall, the study provides new insights into the limitations of LLMs' arithmetic capabilities and highlights areas where they need improvement. While LLMs have made significant progress in understanding and generating text, they still have a way to go when it comes to basic math operations like subtraction."
cs.LG,Enhancing Federated Learning Privacy with QUBO,"Federated learning (FL) is a widely used method for training machine learning (ML) models in a scalable way while preserving privacy (i.e., without centralizing raw data). Prior research shows that the risk of exposing sensitive data increases cumulatively as the number of iterations where a client's updates are included in the aggregated model increase. Attackers can launch membership inference attacks (MIA; deciding whether a sample or client participated), property inference attacks (PIA; inferring attributes of a client's data), and model inversion attacks (MI; reconstructing inputs), thereby inferring client-specific attributes and, in some cases, reconstructing inputs. In this paper, we mitigate risk by substantially reducing per client exposure using a quantum computing-inspired quadratic unconstrained binary optimization (QUBO) formulation that selects a small subset of client updates most relevant for each training round. In this work, we focus on two threat vectors: (i) information leakage by clients during training and (ii) adversaries who can query or obtain the global model. We assume a trusted central server and do not model server compromise. This method also assumes that the server has access to a validation/test set with global data distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with 147 clients' updates never being used during training while maintaining in general the full-aggregation accuracy or even better. The method proved to be efficient at lower scale and more complex model as well. A CINIC-10 dataset-based experiment with 30 clients resulted in 82% per-round privacy improvement and 33% cumulative privacy.",http://arxiv.org/abs/2511.02785v1,2025-11-04T18:06:30Z,"Andras Ferenczi, Sutapa Samanta, Dagen Wang, Todd Hodges","**Protecting Sensitive Data in Machine Learning with QUBO**

Machine learning models are increasingly used to make predictions and improve decision-making. However, these models often require large amounts of data to be effective, which can compromise individual privacy. Federated learning is a technique that allows multiple devices or clients to train a model together without sharing their raw data. Despite this, there is still a risk that sensitive information can be inferred from the model's updates.

Researchers have found a way to reduce this risk by using a method inspired by quantum computing, called QUBO (Quadratic Unconstrained Binary Optimization). This method helps select a small subset of client updates that are most relevant for each training round, reducing the exposure of sensitive data.

In experiments, this approach showed significant improvements in privacy protection. For example, in one test with 300 clients, the method reduced the risk of exposing sensitive data by 95.2% per round and 49% cumulatively, while maintaining the accuracy of the model. This means that the model was still able to make accurate predictions, but with much less risk of revealing individual data.

This breakthrough has important implications for protecting sensitive information in machine learning applications, such as healthcare, finance, and social media. By using QUBO, developers can create more private and secure machine learning models that respect individual data protection."
cs.LG,Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the   Minimizer Manifold,"Despite the popularity of the Adam optimizer in practice, most theoretical analyses study Stochastic Gradient Descent (SGD) as a proxy for Adam, and little is known about how the solutions found by Adam differ. In this paper, we show that Adam implicitly reduces a unique form of sharpness measure shaped by its adaptive updates, leading to qualitatively different solutions from SGD. More specifically, when the training loss is small, Adam wanders around the manifold of minimizers and takes semi-gradients to minimize this sharpness measure in an adaptive manner, a behavior we rigorously characterize through a continuous-time approximation using stochastic differential equations. We further demonstrate how this behavior differs from that of SGD in a well-studied setting: when training overparameterized models with label noise, SGD has been shown to minimize the trace of the Hessian matrix, $\tr(\mH)$, whereas we prove that Adam minimizes $\tr(\Diag(\mH)^{1/2})$ instead. In solving sparse linear regression with diagonal linear networks, this distinction enables Adam to achieve better sparsity and generalization than SGD. Finally, our analysis framework extends beyond Adam to a broad class of adaptive gradient methods, including RMSProp, Adam-mini, Adalayer and Shampoo, and provides a unified perspective on how these adaptive optimizers reduce sharpness, which we hope will offer insights for future optimizer design.",http://arxiv.org/abs/2511.02773v1,2025-11-04T17:58:57Z,"Xinghan Li, Haodong Wen, Kaifeng Lyu","**Unlocking the Secrets of Adam Optimizer: A New Perspective on Machine Learning**

Machine learning models are trained using optimization algorithms that help them learn from data. Two popular algorithms are Stochastic Gradient Descent (SGD) and Adam. While SGD is often used in theoretical studies, Adam is widely used in practice. Researchers have wondered how the solutions found by Adam differ from those found by SGD. A new study provides insights into how Adam works and how it differs from SGD.

The study shows that Adam reduces a unique form of ""sharpness"" in the solutions it finds. Sharpness refers to how sensitive a solution is to small changes in the data. Adam's adaptive updates allow it to explore the solution space in a way that SGD does not. When the training loss is small, Adam ""wanders"" around the optimal solution and takes steps to minimize this sharpness measure.

The study also compares Adam and SGD in a specific scenario: training overparameterized models with noisy labels. It finds that SGD minimizes the trace of the Hessian matrix, while Adam minimizes a different measure, $\tr(\Diag(\mH)^{1/2})$. This difference leads to better sparsity and generalization performance for Adam in certain problems.

The researchers' analysis framework extends beyond Adam to other adaptive gradient methods, including RMSProp and Shampoo. Their work provides a unified perspective on how these optimizers reduce sharpness, which can inform the design of future optimization algorithms.

**In simple terms:** Adam, a popular optimization algorithm, works differently than SGD, a commonly studied algorithm. Adam's adaptive approach helps it find solutions that are less sensitive to small changes in the data, leading to better performance in certain situations. This study sheds light on the strengths of Adam and provides a new perspective on optimization in machine learning."
cs.LG,STAR-VAE: Latent Variable Transformers for Scalable and Controllable   Molecular Generation,"The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.",http://arxiv.org/abs/2511.02769v1,2025-11-04T17:56:00Z,"Bum Chul Kwon, Ben Shapira, Moshiko Raboh, Shreyans Sethi, Shruti Murarka, Joseph A Morrone, Jianying Hu, Parthasarathy Suryanarayanan","**Breakthrough in Molecular Generation: Introducing STAR-VAE**

Imagine a vast library of molecules with unique properties, like a huge box of LEGOs with an infinite number of pieces. Scientists need to find the right LEGO pieces to create new medicines, but searching through this enormous library is a daunting task. To tackle this challenge, researchers have developed STAR-VAE, a powerful tool that uses artificial intelligence to generate new molecules with specific properties.

**What is STAR-VAE?**

STAR-VAE is a computer program that uses a type of AI called a transformer to learn about the structure and properties of molecules. It's trained on a massive dataset of 79 million molecules and can generate new molecules that are similar to existing ones. The program uses a clever approach called a latent-variable framework, which allows it to learn a compressed representation of the molecules and generate new ones based on that.

**Key Innovations**

The STAR-VAE program has several key innovations:

1. **Scalable and efficient**: STAR-VAE can handle large datasets and generate new molecules quickly.
2. **Conditional generation**: The program can generate molecules with specific properties, such as a certain shape or chemical structure.
3. **Flexible and adaptable**: STAR-VAE can be fine-tuned with a small amount of data to adapt to new tasks and properties.

**Results and Impact**

The researchers tested STAR-VAE on several benchmarks and found that it performs as well as or better than existing methods. The generated molecules have smooth and structured representations, making it easier to explore and design new molecules. This breakthrough has the potential to accelerate the discovery of new medicines and materials.

**In Simple Terms**

STAR-VAE is a powerful tool that helps scientists generate new molecules with specific properties. It's like a LEGO builder that can create new pieces based on a set of instructions. This tool can help scientists design new medicines and materials more efficiently, which could lead to breakthroughs in healthcare and other fields."
cs.LG,VecComp: Vector Computing via MIMO Digital Over-the-Air Computation,"Recently, the ChannelComp framework has proposed digital over-the-air computation by designing digital modulations that enable the computation of arbitrary functions. Unlike traditional analog over-the-air computation, which is restricted to nomographic functions, ChannelComp enables a broader range of computational tasks while maintaining compatibility with digital communication systems. This framework is intended for applications that favor local information processing over the mere acquisition of data. However, ChannelComp is currently designed for scalar function computation, while numerous data-centric applications necessitate vector-based computations, and it is susceptible to channel fading. In this work, we introduce a generalization of the ChannelComp framework, called VecComp, by integrating ChannelComp with multiple-antenna technology. This generalization not only enables vector function computation but also ensures scalability in the computational complexity, which increases only linearly with the vector dimension. As such, VecComp remains computationally efficient and robust against channel impairments, making it suitable for high-dimensional, data-centric applications. We establish a non-asymptotic upper bound on the mean squared error of VecComp, affirming its computation efficiency under fading channel conditions. Numerical experiments show the effectiveness of VecComp in improving the computation of vector functions and fading compensation over noisy and fading multiple-access channels.",http://arxiv.org/abs/2511.02765v1,2025-11-04T17:46:11Z,"Saeed Razavikia, JosÃ© Mairton Barros Da Silva Junior, Carlo Fischione","**Breakthrough in Wireless Computing: VecComp Enables Efficient Vector Computing Over-the-Air**

Imagine a future where devices can wirelessly share and process data in real-time, without the need for cumbersome cables or centralized processing. Researchers have made a significant step towards making this vision a reality with the development of VecComp, a novel technology that enables vector computing over-the-air.

VecComp builds upon a previous framework called ChannelComp, which allowed for digital computation of functions over wireless channels. However, ChannelComp had limitations, only supporting simple (scalar) computations and being vulnerable to signal degradation due to channel fading. VecComp overcomes these limitations by integrating ChannelComp with multiple-antenna technology.

The result is a system that can efficiently compute complex vector functions, which are crucial in many data-driven applications. VecComp's key advantages include:

* **Scalability**: VecComp can handle high-dimensional data and performs computations with increasing efficiency as the data size grows.
* **Robustness**: VecComp is resilient to channel impairments, such as fading and noise, ensuring reliable computation over wireless channels.

The researchers behind VecComp have established a mathematical upper bound on the system's error rate, confirming its computation efficiency under realistic channel conditions. Numerical experiments have also demonstrated VecComp's effectiveness in computing vector functions and mitigating the effects of fading.

VecComp has the potential to transform various applications, including edge computing, IoT, and machine learning, by enabling fast, efficient, and wireless data processing. This innovation paves the way for a new generation of wireless computing systems that can efficiently process and share data over-the-air."
cs.LG,From Solo to Symphony: Orchestrating Multi-Agent Collaboration with   Single-Agent Demos,"Training a team of agents from scratch in multi-agent reinforcement learning (MARL) is highly inefficient, much like asking beginners to play a symphony together without first practicing solo. Existing methods, such as offline or transferable MARL, can ease this burden, but they still rely on costly multi-agent data, which often becomes the bottleneck. In contrast, solo experiences are far easier to obtain in many important scenarios, e.g., collaborative coding, household cooperation, and search-and-rescue. To unlock their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that transfers solo knowledge into cooperative learning. SoCo first pretrains a shared solo policy from solo demonstrations, then adapts it for cooperation during multi-agent training through a policy fusion mechanism that combines an MoE-like gating selector and an action editor. Experiments across diverse cooperative tasks show that SoCo significantly boosts the training efficiency and performance of backbone algorithms. These results demonstrate that solo demonstrations provide a scalable and effective complement to multi-agent data, making cooperative learning more practical and broadly applicable.",http://arxiv.org/abs/2511.02762v1,2025-11-04T17:44:11Z,"Xun Wang, Zhuoran Li, Yanshan Lin, Hai Zhong, Longbo Huang","**Unlocking Efficient Teamwork with Solo Training**

Imagine trying to teach a team of musicians to play a symphony together without any prior practice. It's a daunting task! Similarly, in the field of artificial intelligence, training a team of agents to work together can be highly inefficient. Researchers have proposed various solutions, but they often rely on expensive and time-consuming team training data.

A new approach, called Solo-to-Collaborative RL (SoCo), offers a more practical solution. SoCo uses solo demonstrations, where individual agents learn to perform tasks on their own, and then combines this knowledge to enable efficient team learning. This method is particularly useful in scenarios like collaborative coding, household cooperation, and search-and-rescue missions.

SoCo works by first training a shared policy from solo demonstrations and then adapting it for teamwork during multi-agent training. The results show that SoCo significantly improves the training efficiency and performance of team learning algorithms. This breakthrough makes cooperative learning more practical and widely applicable, paving the way for more efficient and effective teamwork in various fields."
cs.LG,ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free   Finetuning of Large Language Models,"Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy for finetuning large language models (LLMs) because it eliminates the memory overhead of backpropagation. However, it converges slowly due to the inherent curse of dimensionality when searching for descent directions in the high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a novel zeroth-order optimizer that accelerates convergence by adaptive directional sampling. Instead of drawing the direction uniformly at random, ConMeZO restricts the sampling to a cone centered around a momentum estimate. This concentrates the search in directions where the true gradient is more likely to lie and thus reduces the effect of high dimensions. We prove that ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically, when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than MeZO while retaining the low-memory footprint of zeroth-order methods.",http://arxiv.org/abs/2511.02757v1,2025-11-04T17:35:52Z,"Lejs Deen Behric, Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil","Here's a summary of the research paper for a general audience:

**Improving the Efficiency of Fine-Tuning Large Language Models**

Large language models, like those used in chatbots and language translation tools, are incredibly powerful but also require a lot of computational resources to fine-tune for specific tasks. One way to fine-tune these models is to use a method called ""zeroth-order optimization,"" which doesn't require storing a lot of information about the model's internal workings. However, this method can be slow, especially for very large models.

Researchers have proposed a new method called ConMeZO, which improves the efficiency of zeroth-order optimization. ConMeZO works by focusing the search for the best model parameters on a specific region of the vast space of possible parameters, rather than searching randomly. This approach allows ConMeZO to converge faster than existing methods, making it up to 2 times faster while still using a low-memory footprint.

In simple terms, ConMeZO is a more efficient way to fine-tune large language models, which could lead to faster and more accurate language processing tools. This research has the potential to make a significant impact on the development of more efficient and effective language models."
cs.LG,DANIEL: A Distributed and Scalable Approach for Global Representation   Learning with EHR Applications,"Classical probabilistic graphical models face fundamental challenges in modern data environments, which are characterized by high dimensionality, source heterogeneity, and stringent data-sharing constraints. In this work, we revisit the Ising model, a well-established member of the Markov Random Field (MRF) family, and develop a distributed framework that enables scalable and privacy-preserving representation learning from large-scale binary data with inherent low-rank structure. Our approach optimizes a non-convex surrogate loss function via bi-factored gradient descent, offering substantial computational and communication advantages over conventional convex approaches. We evaluate our algorithm on multi-institutional electronic health record (EHR) datasets from 58,248 patients across the University of Pittsburgh Medical Center (UPMC) and Mass General Brigham (MGB), demonstrating superior performance in global representation learning and downstream clinical tasks, including relationship detection, patient phenotyping, and patient clustering. These results highlight a broader potential for statistical inference in federated, high-dimensional settings while addressing the practical challenges of data complexity and multi-institutional integration.",http://arxiv.org/abs/2511.02754v1,2025-11-04T17:35:12Z,"Zebin Wang, Ziming Gan, Weijing Tang, Zongqi Xia, Tianrun Cai, Tianxi Cai, Junwei Lu","**Breakthrough in Medical Data Analysis: A New Approach to Understanding Patient Data**

Researchers have developed a novel method called DANIEL, which enables the analysis of large amounts of patient data from multiple hospitals while maintaining patient privacy. The method, called a ""distributed and scalable approach,"" allows researchers to identify patterns and relationships in data from different sources, such as electronic health records (EHRs), without sharing sensitive information.

**The Problem: Analyzing Complex Medical Data**

Traditional methods for analyzing medical data face significant challenges. Modern data environments are characterized by **high dimensionality**, meaning there are many variables to consider; **source heterogeneity**, meaning data comes from different sources; and **stringent data-sharing constraints**, meaning that sensitive information must be protected. These challenges make it difficult to analyze large amounts of data and identify meaningful patterns.

**The Solution: DANIEL**

The DANIEL approach addresses these challenges by using a technique called the Ising model, a type of mathematical model that helps identify relationships between different variables. By optimizing a special type of mathematical function, DANIEL can efficiently analyze large datasets and identify patterns that may not be apparent through other methods.

**Key Benefits**

The DANIEL approach has several key benefits:

* **Scalability**: DANIEL can handle large amounts of data from multiple sources.
* **Privacy preservation**: DANIEL maintains patient privacy by not sharing sensitive information.
* **Improved performance**: DANIEL outperforms traditional methods in identifying relationships and patterns in patient data.

**Real-World Applications**

The researchers tested DANIEL on a large dataset of EHRs from over 58,000 patients across two major medical centers. The results showed that DANIEL was able to:

* **Detect relationships** between different medical conditions and patient characteristics
* **Identify patient phenotypes** (groups of patients with similar characteristics)
* **Cluster patients** with similar needs and outcomes

**Implications and Future Directions**

The development of DANIEL has significant implications for the analysis of complex medical data. By enabling the analysis of large amounts of data while maintaining patient privacy, DANIEL can help researchers and clinicians better understand patient needs and develop more effective treatments. Future studies will focus on applying DANIEL to other types of medical data and exploring its potential applications in different fields."
cs.LG,Agentic World Modeling for 6G: Near-Real-Time Generative State-Space   Reasoning,"We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative ""what-if"" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening.",http://arxiv.org/abs/2511.02748v1,2025-11-04T17:22:22Z,"Farhad Rezazadeh, Hatim Chergui, Merouane Debbah, Houbing Song, Dusit Niyato, Lingjia Liu","**Imagine a Smarter Future: A New Approach to 6G Technology**

Imagine a world where wireless networks can not only process information but also make informed decisions and predict future outcomes. This is the vision behind a new research paper on 6G technology, the next generation of wireless communication.

The authors propose a new approach to 6G intelligence, which they call ""agentic world modeling."" This approach enables wireless networks to simulate future scenarios, weigh trade-offs, and act with uncertainty. In essence, it's like giving wireless networks the ability to imagine and choose the best course of action.

The researchers developed a new model, called WM-MS3M, which can predict the performance of wireless networks under different scenarios. This model is more accurate and efficient than existing models, reducing errors by up to 80% and processing information faster.

The implications of this research are significant. With agentic world modeling, wireless networks could become more autonomous and efficient, enabling new applications such as:

* **Smart cities**: Wireless networks could optimize traffic flow, energy consumption, and public safety.
* **Autonomous vehicles**: Wireless networks could provide critical communication infrastructure for self-driving cars.
* **Remote healthcare**: Wireless networks could enable remote monitoring and telemedicine.

The authors believe that their approach could revolutionize the way we design and operate wireless networks, enabling a more intelligent, efficient, and responsive communication infrastructure.

**Key Takeaways:**

* A new approach to 6G technology that enables wireless networks to simulate future scenarios and make informed decisions.
* A more accurate and efficient model for predicting wireless network performance.
* Potential applications in smart cities, autonomous vehicles, and remote healthcare.

This research has the potential to transform the way we interact with wireless networks and could have a significant impact on various industries and aspects of our lives."
cs.LG,Calibration improves detection of mislabeled examples,"Mislabeled data is a pervasive issue that undermines the performance of machine learning systems in real-world applications. An effective approach to mitigate this problem is to detect mislabeled instances and subject them to special treatment, such as filtering or relabeling. Automatic mislabeling detection methods typically rely on training a base machine learning model and then probing it for each instance to obtain a trust score that each provided label is genuine or incorrect. The properties of this base model are thus of paramount importance. In this paper, we investigate the impact of calibrating this model. Our empirical results show that using calibration methods improves the accuracy and robustness of mislabeled instance detection, providing a practical and effective solution for industrial applications.",http://arxiv.org/abs/2511.02738v1,2025-11-04T17:03:33Z,"Ilies Chibane, Thomas George, Pierre Nodet, Vincent Lemaire","**Improving Machine Learning Accuracy: A New Approach to Detecting Mislabeled Data**

Machine learning systems are only as good as the data they're trained on. But what if that data is incorrect? Mislabeled data is a common problem that can significantly undermine the performance of these systems. Researchers have been working on ways to detect mislabeled data, and a new study has found that a technique called ""calibration"" can improve the accuracy of these detection methods.

In machine learning, a model is trained on labeled data to make predictions. But if the labels are incorrect, the model can learn to make incorrect predictions as well. To address this, researchers train a ""base model"" and then use it to evaluate the trustworthiness of each label. The study found that by ""calibrating"" this base model - essentially fine-tuning it to provide more accurate probability estimates - the detection of mislabeled data becomes more accurate and robust.

The results of this study are promising, suggesting that calibration can be a practical and effective solution for industries that rely on machine learning. By improving the detection of mislabeled data, companies can build more reliable and trustworthy machine learning systems."
cs.LG,Does Interpretability of Knowledge Tracing Models Support Teacher   Decision Making?,"Knowledge tracing (KT) models are a crucial basis for pedagogical decision-making, namely which task to select next for a learner and when to stop teaching a particular skill. Given the high stakes of pedagogical decisions, KT models are typically required to be interpretable, in the sense that they should implement an explicit model of human learning and provide explicit estimates of learners' abilities. However, to our knowledge, no study to date has investigated whether the interpretability of KT models actually helps human teachers to make teaching decisions. We address this gap. First, we perform a simulation study to show that, indeed, decisions based on interpretable KT models achieve mastery faster compared to decisions based on a non-interpretable model. Second, we repeat the study but ask $N=12$ human teachers to make the teaching decisions based on the information provided by KT models. As expected, teachers rate interpretable KT models higher in terms of usability and trustworthiness. However, the number of tasks needed until mastery hardly differs between KT models. This suggests that the relationship between model interpretability and teacher decisions is not straightforward: teachers do not solely rely on KT models to make decisions and further research is needed to investigate how learners and teachers actually understand and use KT models.",http://arxiv.org/abs/2511.02718v1,2025-11-04T16:40:24Z,"Adia Khalid, Alina Deriyeva, Benjamin Paassen","**Can Teachers Make Better Decisions with More Transparent Learning Models?**

Imagine a tool that helps teachers decide what to teach next and when to stop teaching a particular skill. This tool uses complex math to track a student's progress, but some versions are more transparent than others, providing clear explanations of how they work and what they think the student knows.

Researchers tested whether these more transparent models actually help teachers make better decisions. They found that when teachers used these transparent models, they felt more confident and trusted the model's suggestions more. However, surprisingly, the transparent models didn't lead to significantly better teaching decisions or faster learning.

The study suggests that teachers don't rely solely on these models to make decisions. Instead, they likely use their own experience and judgment, along with the model's suggestions. More research is needed to understand how teachers and students use these models and how to make them more useful for teaching."
cs.CV,"TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System","Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .",http://arxiv.org/abs/2511.02832v1,2025-11-04T18:58:35Z,"Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu","**Breakthrough in Humanoid Robotics: Introducing TWIST2**

Imagine a robot that can move and perform tasks like a human. To make this a reality, researchers need large amounts of data on human movements and actions. However, collecting this data has been a challenge, especially for humanoid robots. A team of researchers has now developed a system called TWIST2, which makes it easy to collect data on human movements and transfer them to humanoid robots.

**What is TWIST2?**

TWIST2 is a portable and affordable system that allows researchers to collect data on human movements in real-time. It uses a special VR headset and a custom-built robot neck to capture whole-body human motions. This system is scalable, meaning it can be used to collect large amounts of data quickly and efficiently.

**Key Features of TWIST2**

* **Portable and Affordable**: TWIST2 is designed to be easy to use and affordable, with a custom robot neck that costs around $250.
* **Real-time Data Collection**: The system can collect data on human movements in real-time, allowing for efficient data collection.
* **Scalable**: TWIST2 can collect large amounts of data quickly, with a success rate of almost 100%.

**How does it work?**

The system uses a VR headset to track the user's movements and a custom-built robot neck to capture egocentric vision (the user's point of view). This allows the researchers to collect data on whole-body human motions and transfer them to a humanoid robot.

**What are the benefits?**

The TWIST2 system has several benefits:

* **Faster data collection**: The system can collect 100 demonstrations in just 15 minutes, which is a significant improvement over existing methods.
* **Improved robot control**: The system enables holistic human-to-humanoid control, allowing robots to perform complex tasks like dexterous manipulation and dynamic kicking.
* **Open-source**: The entire system, including the code and dataset, is open-sourced and available for researchers to use and build upon.

**What's next?**

The researchers behind TWIST2 are excited about the potential applications of their system. They plan to use it to develop more advanced humanoid robots that can perform complex tasks and interact with humans in a more natural way. With TWIST2, the possibilities for humanoid robotics are endless!"
cs.CV,Densemarks: Learning Canonical Embeddings for Human Heads Images via   Point Tracks,"We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face/head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.",http://arxiv.org/abs/2511.02830v1,2025-11-04T18:58:03Z,"Dmitrii Pozdeev, Alexey Artemov, Ananta R. Bhattarai, Artem Sevastopolsky","**Breakthrough in Human Head Image Analysis: Introducing DenseMarks**

Imagine being able to accurately map and analyze human head images in 3D, regardless of pose or expression. Researchers have made a significant step towards achieving this goal with the development of DenseMarks, a new AI-powered representation for human head images.

DenseMarks uses a type of neural network called a Vision Transformer to predict a 3D ""embedding"" for each pixel in a 2D image of a human head. This embedding corresponds to a specific location in a virtual 3D cube, allowing for precise and detailed analysis of the head's shape and features.

To train the network, the researchers collected a large dataset of point matches from videos of people talking, and used a contrastive loss function to encourage the network to map similar points to similar locations in the 3D cube. They also added additional constraints to ensure that the representation is consistent across different poses and individuals.

The results are impressive: DenseMarks can accurately identify common semantic parts of the head, track the head in 3D, and even reconstruct the head's shape from a single image. This technology has many potential applications, from facial recognition and animation to medical imaging and robotics.

What's more, the researchers have made their code and model publicly available, which will enable other researchers and developers to build upon this work and explore new applications. Overall, DenseMarks represents a significant advance in the field of computer vision and human head image analysis."
cs.CV,PLUTO-4: Frontier Pathology Foundation Models,"Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.",http://arxiv.org/abs/2511.02826v2,2025-11-04T18:54:58Z,"Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed","**Breakthrough in AI-Powered Pathology: Introducing PLUTO-4**

Imagine a computer program that can analyze medical images of tissue samples to help doctors diagnose diseases more accurately and efficiently. Researchers have just developed a cutting-edge AI model called PLUTO-4, which is a significant advancement in this field.

PLUTO-4 is a type of ""foundation model"" that can learn from a vast library of medical images, allowing it to recognize patterns and make predictions about various diseases. The model comes in two versions: a compact and efficient version (PLUTO-4S) designed for everyday use, and a more powerful version (PLUTO-4G) that pushes the boundaries of what's possible in pathology.

The researchers trained PLUTO-4 on a massive dataset of 551,164 images from over 137,000 patients, covering more than 60 diseases and 100 different staining techniques. They found that PLUTO-4 outperforms existing models in various tasks, such as classifying tissue samples, segmenting images, and diagnosing diseases.

The compact PLUTO-4S model offers fast and reliable performance, making it suitable for practical use in hospitals and clinics. Meanwhile, the more powerful PLUTO-4G model sets new standards in pathology, achieving an 11% improvement in diagnosing skin diseases.

The development of PLUTO-4 has the potential to revolutionize the field of pathology, enabling doctors to make more accurate diagnoses and develop more effective treatments. This technology could also help to reduce the workload of pathologists, allowing them to focus on more complex and high-priority cases."
cs.CV,AI-Generated Image Detection: An Empirical Study and Future Research   Directions,"The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.",http://arxiv.org/abs/2511.02791v1,2025-11-04T18:13:48Z,"Nusrat Tasnim, Kutub Uddin, Khalid Mahmood Malik","**The Growing Concern of AI-Generated Fake Images**

Artificial intelligence (AI) has made it easier to create realistic fake images, known as deepfakes, which can be used for malicious purposes such as spreading misinformation, committing fraud, and manipulating people. This has raised concerns about the reliability of digital media and the potential erosion of trust in institutions.

**The Challenge of Detecting AI-Generated Images**

Researchers have proposed various methods to detect AI-generated images, but these methods have limitations. They often use different datasets and evaluation metrics, making it difficult to compare their effectiveness. This lack of standardization hinders the development of robust and reliable detection methods.

**A New Framework for Evaluating Detection Methods**

To address these challenges, researchers have developed a unified benchmarking framework to systematically evaluate the performance of AI-generated image detection methods. This framework uses controlled and reproducible conditions to test the effectiveness of various detection methods.

**Key Findings**

The study evaluated ten state-of-the-art detection methods and seven publicly available datasets. The results showed significant variability in the performance of these methods, with some exhibiting strong performance in certain situations but struggling with others. The study also found that some methods were not very transparent or interpretable, making it difficult to understand why they made certain predictions.

**Implications and Future Directions**

The study aims to guide the research community towards developing more robust, generalizable, and explainable solutions for detecting AI-generated images. The findings highlight the need for:

1. **Standardized benchmarks**: Using standardized datasets and evaluation metrics to compare the effectiveness of detection methods.
2. **Improved evaluation metrics**: Developing metrics that capture generalization and explainability.
3. **More robust methods**: Developing detection methods that can perform well in a variety of situations.

By addressing these challenges, researchers can develop more effective solutions for detecting AI-generated fake images and mitigating their potential harm."
cs.CV,"When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for   Visual Chain-of-Thought","We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through ""drawing to think"". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.",http://arxiv.org/abs/2511.02779v1,2025-11-04T18:00:51Z,"Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye","**Unlocking the Power of Visual Thinking: A New Benchmark for AI Reasoning**

Imagine trying to solve a complex puzzle or understand a intricate system. Humans often use visual aids like sketches or diagrams to help guide their thinking. Researchers have created a new benchmark, called MIRA, to test whether AI models can do the same.

MIRA is a set of 546 challenging problems that require AI models to generate and use intermediate visual images to reason and arrive at a solution. These problems involve complex structures, spatial relationships, and reasoning steps that are hard to express with language alone.

The results show that current AI models struggle with these problems when only given textual prompts. However, when provided with visual cues, such as annotated images, their performance improves significantly - by an average of 33.7%. This highlights the importance of visual thinking in enabling successful reasoning.

The researchers also explored the limitations of current AI models and found that even with expanded search spaces and carefully designed textual prompts, they couldn't match the performance achieved with visual cues. This suggests that AI models need to be able to generate and utilize visual information to truly reason and solve complex problems.

The MIRA benchmark provides a new way to evaluate AI models and pushes the boundaries of what we expect from AI systems. By incorporating visual thinking, AI models can become more powerful and human-like in their problem-solving abilities."
cs.CV,VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual   Representation,"Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.",http://arxiv.org/abs/2511.02778v1,2025-11-04T18:00:18Z,"Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang","Here's a summary of the research paper for a general audience:

**The Future of Coding: A New Way to Represent Visual Information**

Imagine being able to describe a picture using code, just like a computer program. Researchers have made progress in using code to solve problems, but most of this work focuses on language-based tasks, like writing text. Now, a team of researchers has introduced a new benchmark called VCode, which explores the use of code to represent visual information, like images.

**The Challenge: Turning Images into Code**

The goal of VCode is to take an image and generate code that accurately represents what's in the image. This code is written in a format called SVG (Scalable Vector Graphics), which is like a set of instructions that a computer can understand. The researchers want to see if computers can generate code that not only looks like the image but also preserves the meaning and symbols in the image.

**The Benchmark: Testing Computers' Abilities**

VCode includes three different areas to test: everyday knowledge, professional skills, and visual perception. The researchers also created a new way to evaluate how well computers generate code, called CodeVQA. They found that even the best computers struggle to generate accurate code, showing a gap between language-based and visual-based coding.

**A New Solution: VCoder**

To bridge this gap, the researchers developed a new framework called VCoder. VCoder helps computers generate better code by using a process called ""thinking with revision,"" which refines the code over time. It also uses ""acting with visual tools,"" which provides structured information about the image, like objects and shapes.

**The Results: A Significant Improvement**

The researchers found that VCoder significantly improves the accuracy of generated code, outperforming the best computers by 12.3 points. They also showed that humans and computers perform better when working with code that accurately represents visual information.

**What's Next?**

The researchers have made their benchmark and code available online, which will help advance the field of visual-centric coding. This work has the potential to enable computers to better understand and generate visual information, leading to new applications in areas like computer vision, robotics, and more."
cs.CV,PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction   & Editing,"We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE",http://arxiv.org/abs/2511.02777v1,2025-11-04T17:59:15Z,"Antonio Oroz, Matthias NieÃner, Tobias Kirschstein","**Breakthrough in 3D Head Reconstruction and Editing**

Imagine being able to take a single photo of someone's head and create a 3D model that can be viewed from any angle, or even edited to change its shape and appearance. Researchers have made significant progress in achieving this with PercHead, a new method for single-image 3D head reconstruction and editing.

**What is PercHead?**

PercHead is a computer model that uses a single input image to create a 3D head model. It works by employing a dual-branch encoder and a special type of decoder that helps to lift 2D features into 3D space. The model also uses a novel perceptual supervision strategy that provides rich signals for both geometric and appearance fidelity.

**Key Innovations**

The PercHead model has several key innovations:

* **Improved accuracy**: PercHead achieves state-of-the-art performance in novel-view synthesis, meaning it can generate high-quality 3D models from a single image.
* **Robustness to extreme viewing angles**: The model is robust to extreme viewing angles, outperforming established baselines.
* **Seamless editing**: The model can be easily extended for semantic 3D editing, allowing users to change the geometry and appearance of the 3D model.

**Editing Capabilities**

The researchers have also developed a user-friendly interface that allows users to interactively edit the 3D model. With this interface, users can:

* **Sculpt geometry**: Draw segmentation maps to change the shape of the 3D model.
* **Stylize appearance**: Use natural language or image prompts to change the appearance of the 3D model.

**Implications and Future Directions**

The PercHead model has significant implications for various applications, including:

* **Computer-generated imagery (CGI)**: PercHead can be used to create realistic 3D models for movies, video games, and other forms of media.
* **Virtual reality (VR) and augmented reality (AR)**: PercHead can be used to create immersive experiences that require accurate 3D models of human heads.
* **Medical applications**: PercHead can be used to create 3D models of human heads for medical applications, such as surgical planning and reconstruction.

**Conclusion**

PercHead represents a significant breakthrough in 3D head reconstruction and editing. Its ability to create accurate and editable 3D models from a single image has the potential to revolutionize various industries and applications. With its user-friendly interface and robust performance, PercHead is an exciting development in the field of computer vision and 3D modeling."
cs.CV,Dynamic Reflections: Probing Video Representations with Text Alignment,"The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/",http://arxiv.org/abs/2511.02767v1,2025-11-04T17:52:14Z,"Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica PÄtrÄucean, Maks Ovsjanikov","**Unlocking the Secrets of Video Understanding through Text Alignment**

Imagine being able to understand videos like a human, recognizing objects, actions, and events. This is a challenging task for computers, but researchers are making progress by exploring how video and text representations can be aligned. A recent study, ""Dynamic Reflections: Probing Video Representations with Text Alignment,"" investigates this alignment and provides new insights into how well video and language models work together.

The study found that the alignment of video and text representations depends on the richness of both the visual and text data provided. For example, a video with multiple frames and a detailed text caption can lead to better alignment than a single image with a short caption. The researchers also discovered that strong alignment between video and text encoders may be linked to better performance on a range of tasks, from understanding video content to answering questions.

The study also explored the connection between temporal reasoning (understanding the sequence of events) and cross-modal alignment (how well video and text representations match). The findings suggest that video-text alignment can be a useful tool for evaluating the representation power of different models for spatio-temporal data, such as videos.

Overall, this research provides a new way to evaluate and improve video understanding models, which could have applications in areas like video search, recommendation, and analysis. By exploring the connections between video, text, and human understanding, researchers can develop more sophisticated models that can better comprehend the complex world of videos."
cs.CV,LLEXICORP: End-user Explainability of Convolutional Neural Networks,"Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.",http://arxiv.org/abs/2511.02720v1,2025-11-04T16:44:45Z,"VojtÄch KÅ¯r, Adam Bajger, Adam KukuÄka, Marek Hradil, VÃ­t Musil, TomÃ¡Å¡ BrÃ¡zdil","**Making AI More Transparent: A New Approach to Understanding Neural Networks**

Imagine you're using a self-driving car or a medical diagnosis tool that relies on artificial intelligence (AI). You might wonder how the AI makes its decisions. Researchers have been working to make AI more transparent, and a new study has made significant progress in this area.

The study focuses on a type of AI called convolutional neural networks (CNNs), which are used in many computer vision systems. While CNNs are powerful, they can be difficult to understand, making it challenging to trust their decisions. To address this issue, researchers developed a new method called LLEXICORP.

LLEXICORP uses a large language model to automatically explain how a CNN makes its decisions. The method works by identifying the key features that the CNN uses to classify images, and then generating natural-language explanations that describe these features. For example, if a CNN is used to diagnose medical images, LLEXICORP might explain that the AI is looking for certain patterns or features in the images to make its diagnosis.

The researchers tested LLEXICORP on a well-known CNN model and found that it can significantly improve our understanding of how the AI works. The method can generate explanations that are tailored to different audiences, from technical experts to non-technical stakeholders.

Overall, this study demonstrates the potential of LLEXICORP to make AI more transparent and trustworthy. By providing insights into how AI systems work, LLEXICORP can help build confidence in AI and facilitate its adoption in critical applications."
cs.CV,An unscented Kalman filter method for real time input-parameter-state   estimation,"The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.",http://arxiv.org/abs/2511.02717v1,2025-11-04T16:39:27Z,"Marios Impraimakis, Andrew W. Smyth","Here's a summary of the research paper for a general audience:

**Accurately Estimating What's Happening in Real-Time Systems**

Imagine trying to understand a complex system, like a bridge or a building, while it's in use. You can only measure its output, like how much it sways or vibrates, but you want to know what's causing those movements, like strong winds or traffic. Researchers have developed a new method to estimate what's happening inside the system in real-time, using a mathematical technique called an unscented Kalman filter.

This method can estimate not only the system's state (like its current movement) but also its parameters (like its physical properties) and the unknown inputs (like the wind or traffic) that are affecting it. The researchers tested their method on both simple and complex systems and found that it can accurately estimate all three: the system's state, parameters, and inputs.

This breakthrough has significant implications for fields like engineering, where understanding complex systems in real-time can help prevent failures, improve safety, and optimize performance. The method is particularly useful because it only requires measuring the system's output, making it a valuable tool for monitoring and controlling systems in a wide range of applications."
cs.CV,VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation   Models,"Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.",http://arxiv.org/abs/2511.02712v1,2025-11-04T16:31:09Z,"Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang","**Understanding Emotions in Videos: A New Breakthrough**

Researchers have made a significant advancement in developing artificial intelligence (AI) models that can understand and predict human emotions from videos. Emotions are complex and can change quickly, making it challenging for AI to accurately interpret them. To address this challenge, the researchers proposed a new framework called VidEmo, which uses a step-by-step approach to analyze videos and understand emotions.

**The VidEmo Framework**

The VidEmo framework consists of a family of video emotion foundation models that are specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: 

1. **Curriculum Emotion Learning**: The models learn about emotions through a structured curriculum, which helps them develop a deeper understanding of emotions.
2. **Affective-Tree Reinforcement Learning**: The models use a tree-like structure to reason about emotions and make connections between different emotional cues.

**A New Dataset for Emotion Understanding**

The researchers also created a large dataset called Emo-CFG, which consists of 2.1 million samples of videos with emotional annotations. This dataset provides a valuable resource for training and testing AI models to understand emotions.

**Achievements and Impact**

The VidEmo framework achieved competitive performance across 15 face perception tasks, setting a new milestone in emotion understanding. This breakthrough has the potential to improve various applications, such as:

* **Human-Computer Interaction**: VidEmo can enable computers to better understand human emotions, leading to more natural and intuitive interactions.
* **Mental Health Analysis**: VidEmo can help analyze emotional states, which can aid in mental health diagnosis and treatment.
* **Social Robotics**: VidEmo can enable robots to better understand and respond to human emotions, leading to more effective and empathetic interactions.

Overall, the VidEmo framework represents a significant step forward in developing AI models that can understand and interpret human emotions from videos."
cs.CV,Modality-Transition Representation Learning for Visible-Infrared Person   Re-Identification,"Visible-infrared person re-identification (VI-ReID) technique could associate the pedestrian images across visible and infrared modalities in the practical scenarios of background illumination changes. However, a substantial gap inherently exists between these two modalities. Besides, existing methods primarily rely on intermediate representations to align cross-modal features of the same person. The intermediate feature representations are usually create by generating intermediate images (kind of data enhancement), or fusing intermediate features (more parameters, lack of interpretability), and they do not make good use of the intermediate features. Thus, we propose a novel VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a middle generated image as a transmitter from visible to infrared modals, which are fully aligned with the original visible images and similar to the infrared modality. After that, using a modality-transition contrastive loss and a modality-query regularization loss for training, which could align the cross-modal features more effectively. Notably, our proposed framework does not need any additional parameters, which achieves the same inference speed to the backbone while improving its performance on VI-ReID task. Extensive experimental results illustrate that our model significantly and consistently outperforms existing SOTAs on three typical VI-ReID datasets.",http://arxiv.org/abs/2511.02685v1,2025-11-04T16:09:28Z,"Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li","**Breakthrough in Person Identification Across Different Lighting Conditions**

Imagine being able to identify a person in a crowded area, regardless of the lighting conditions. This is a challenging task, especially when trying to match images taken in visible light with those taken in infrared light, which is often used at night or in low-light environments.

Researchers have proposed a new method called Modality-Transition Representation Learning (MTRL) to tackle this problem. The goal is to create a system that can effectively match images of the same person taken in different lighting conditions.

The MTRL framework works by generating a ""middle"" image that acts as a bridge between visible and infrared images. This middle image is designed to be similar to the infrared image, but still closely related to the original visible image. The system then uses this middle image to align the features of the visible and infrared images, making it easier to identify the same person across different lighting conditions.

The good news is that this new method doesn't require any additional computational resources, making it as fast as existing systems. However, it significantly outperforms current state-of-the-art methods in identifying people across different lighting conditions. This breakthrough has the potential to improve surveillance systems, security applications, and more."
cs.CV,Differentiable Hierarchical Visual Tokenization,"Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.",http://arxiv.org/abs/2511.02652v1,2025-11-04T15:18:29Z,"Marius Aasan, Martine Hjelkrem-Tan, Nico Catalano, Changkyu Choi, AdÃ­n RamÃ­rez Rivera","Here's a summary of the research paper for a general audience:

**Breaking Down Images into Meaningful Pieces**

Computers have gotten really good at understanding images, but they often do it by chopping the image into small, fixed-size pieces called ""patches."" This approach can be limiting, as it doesn't take into account the image's structure or the relationships between different parts.

Researchers have now developed a new method called ""differentiable hierarchical visual tokenization"" that allows computers to break down images into more flexible and meaningful pieces. This approach can adapt to the content of the image, down to the pixel level, and can even work with existing computer vision systems.

The benefits of this new method are two-fold. Firstly, it can improve the accuracy of image classification tasks, such as identifying objects in an image. Secondly, it can also enable computers to make more precise predictions about specific parts of an image, such as detecting edges or shapes.

One of the most exciting aspects of this research is its potential to convert raster images (like those used in digital photos) into vector graphics (like those used in illustrations). This could have significant implications for fields such as computer-aided design, graphic design, and even robotics.

Overall, this new method represents a significant step forward in the field of computer vision, and could lead to more accurate and flexible image understanding systems in the future."
cs.CV,Can Visual Input Be Compressed? A Visual Token Compression Benchmark for   Large Multimodal Models,"Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.",http://arxiv.org/abs/2511.02650v1,2025-11-04T15:17:06Z,"Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui","Here's a summary of the research paper for a general audience:

**Can We Reduce the Amount of Visual Data that AI Models Need to Process?**

Large AI models that can understand both text and images are becoming increasingly popular. However, these models can be slow and inefficient, partly because they have to process a huge amount of visual data. Researchers have been exploring ways to compress this visual data to make the models faster and more efficient.

In this study, the researchers created a benchmark to test different methods for compressing visual data. They evaluated 10 different compression algorithms on three large AI models, using a variety of tasks and datasets. They found that:

* A simple method called ""random pruning"" works surprisingly well
* No single method is best for all tasks and scenarios
* Some tasks, like reading text in images, are more sensitive to compression than others
* The amount of compression applied has a bigger impact on performance than the method used

The researchers hope that their benchmark will help future studies on efficient AI modeling and enable the development of faster and more efficient multimodal models."
cs.CV,Robust Face Liveness Detection for Biometric Authentication using Single   Image,"Biometric technologies are widely adopted in security, legal, and financial systems. Face recognition can authenticate a person based on the unique facial features such as shape and texture. However, recent works have demonstrated the vulnerability of Face Recognition Systems (FRS) towards presentation attacks. Using spoofing (aka.,presentation attacks), a malicious actor can get illegitimate access to secure systems. This paper proposes a novel light-weight CNN framework to identify print/display, video and wrap attacks. The proposed robust architecture provides seamless liveness detection ensuring faster biometric authentication (1-2 seconds on CPU). Further, this also presents a newly created 2D spoof attack dataset consisting of more than 500 videos collected from 60 subjects. To validate the effectiveness of this architecture, we provide a demonstration video depicting print/display, video and wrap attack detection approaches. The demo can be viewed in the following link: https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa",http://arxiv.org/abs/2511.02645v1,2025-11-04T15:13:03Z,"Poulami Raha, Yeongnam Chae","**Protecting Face Recognition Systems from Spoofing Attacks**

Face recognition technology is widely used to secure systems, but it's vulnerable to spoofing attacks. Hackers can use fake faces, videos, or masks to trick the system and gain unauthorized access. To address this issue, researchers have developed a new method to detect whether a face is real or fake.

The proposed system uses a lightweight artificial intelligence (AI) framework that can quickly and accurately identify spoofing attacks, including printed or displayed fake faces, video recordings, and masks. The system can do this in just 1-2 seconds on a standard computer.

The researchers also created a new dataset of over 500 videos of fake face attacks from 60 different people. They demonstrated the effectiveness of their system with a video showcasing its ability to detect various types of spoofing attacks.

This breakthrough has the potential to make face recognition systems more secure and reliable, protecting against malicious attacks and ensuring the integrity of biometric authentication."
cs.CV,UniChange: Unifying Change Detection with Multimodal Large Language   Model,"Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.",http://arxiv.org/abs/2511.02607v1,2025-11-04T14:31:06Z,"Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li","**Breakthrough in Land Cover Change Detection: UniChange**

Imagine being able to track changes in the Earth's surface, such as deforestation, urbanization, or natural disasters, with unprecedented accuracy. Researchers have made a significant step forward in this area with the development of UniChange, a new model that uses a multimodal large language model (MLLM) to unify change detection tasks.

**The Problem: Limited Models and Data**

Current change detection models are limited by their reliance on single-type annotated data, which restricts their ability to generalize and adapt to different types of changes. This means that models trained on one type of data may not perform well on others.

**The Solution: UniChange**

UniChange overcomes this limitation by leveraging the language understanding capabilities of MLLMs to integrate information from diverse datasets. This allows the model to detect both binary changes (e.g., change vs. no change) and semantic changes (e.g., type of change, such as from forest to urban area).

**Key Innovations**

1. **Unified Framework**: UniChange provides a single framework for both binary and semantic change detection tasks.
2. **Special Tokens**: The model uses three special tokens to identify changes and guide the detection process.
3. **Text Prompts**: UniChange uses text prompts to identify change categories, eliminating the need for predefined classification heads.

**Results and Impact**

Experiments on four public benchmarks demonstrate that UniChange achieves state-of-the-art performance, surpassing all previous methods. The model's ability to unify change detection tasks and leverage diverse datasets has significant implications for monitoring and analyzing land cover dynamics.

**Availability**

The code for UniChange is available on GitHub, making it accessible to researchers and practitioners in the field."
cs.CV,Zero-Shot Multi-Animal Tracking in the Wild,"Multi-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot multi-animal tracking. By combining a Grounding Dino object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully designed heuristics, we develop a tracking framework that can be applied to new datasets without any retraining or hyperparameter adaptation. Evaluations on ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate strong and consistent performance across diverse species and environments. The code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.",http://arxiv.org/abs/2511.02591v1,2025-11-04T14:12:03Z,"Jan Frederik Meier, Timo LÃ¼ddecke","**Breakthrough in Animal Tracking: A New AI Approach**

Scientists have made a significant advancement in tracking multiple animals in their natural habitats. This is crucial for understanding animal behavior and ecology. However, traditional methods have struggled with variations in environments, animal movements, and appearances.

A new approach uses recent advancements in AI, specifically ""vision foundation models,"" to track multiple animals without needing extensive customization for each scenario. By combining two powerful AI models - Grounding Dino and Segment Anything Model 2 (SAM 2) - with some clever design, researchers have created a tracking framework that works well across different species and environments.

The exciting part? This framework can be applied to new datasets without requiring any additional training or adjustments. This means it can be used in a wide range of situations, from tracking chimpanzees to bird flocks, with strong and consistent results. The code for this framework is now publicly available, opening up new possibilities for researchers and conservationists to study and protect animals in their natural habitats."
cs.CV,TAUE: Training-free Noise Transplant and Cultivation Diffusion Model,"Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.",http://arxiv.org/abs/2511.02580v1,2025-11-04T13:56:39Z,"Daichi Nagai, Ryugo Morita, Shunsuke Kitada, Hitoshi Iyatomi","**Breakthrough in AI Image Generation: TAUE Model Enables Layer-by-Layer Control**

Imagine being able to generate complex images with multiple layers, like a background, foreground, and objects, using artificial intelligence. Researchers have made a significant step towards achieving this with the introduction of the Training-free Noise Transplant and Cultivation Diffusion Model (TAUE).

The TAUE model allows for the creation of images with multiple layers, without requiring extensive training or large datasets. This is a major improvement over existing methods, which either need fine-tuning with massive datasets or can only generate isolated elements.

The key innovation behind TAUE is a technique called Noise Transplantation and Cultivation (NTC). This method enables the model to extract and combine information from different parts of the image generation process, ensuring that the various layers are semantically and structurally coherent.

The results are impressive: TAUE achieves performance comparable to methods that require fine-tuning, while maintaining high image quality and fidelity. This breakthrough has the potential to unlock new applications, such as complex compositional editing, and make generative workflows more accessible and controllable.

In simple terms, TAUE enables AI to generate complex, multi-layered images with unprecedented control and flexibility, paving the way for more sophisticated and creative uses of AI in image generation."
cs.CV,Resource-efficient Automatic Refinement of Segmentations via Weak   Supervision from Light Feedback,"Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.",http://arxiv.org/abs/2511.02576v1,2025-11-04T13:53:10Z,"Alix de Langlais, Benjamin Billot, ThÃ©o Aguilar Vidal, Marc-Olivier Gauci, HervÃ© Delingette","Here's a summary of the research paper for a general audience:

**Improving Medical Image Analysis with AI**

Medical image analysis is a crucial task that helps doctors diagnose and treat diseases. One important step in this process is identifying specific anatomical regions, such as organs or bones, in images. While manual identification is accurate, it's time-consuming and can be prone to errors.

To address this challenge, researchers have developed automated approaches using artificial intelligence (AI). However, these AI models may not always meet the high accuracy standards required in medicine.

A new study presents a solution called SCORE, which helps refine AI-generated segmentations of anatomical regions using minimal feedback from human experts. SCORE uses a novel approach that leverages ""light feedback"" - simple quality scores and error labels - to improve the accuracy of AI-generated segmentations.

In tests on CT scans of the humerus (upper arm bone), SCORE significantly improved the accuracy of initial predictions and achieved performance comparable to existing refinement methods. The best part? SCORE requires much less human supervision and annotation time, making it a more efficient and practical solution for medical image analysis.

This breakthrough has the potential to accelerate the development of accurate and reliable AI models for medical image analysis, ultimately leading to better diagnosis and treatment of diseases."
cs.CV,A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain   Visual Decoding,"Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.",http://arxiv.org/abs/2511.02565v1,2025-11-04T13:39:34Z,"Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li","Here's a summary of the research paper in simple terms:

**Decoding Brain Signals to Reconstruct Visual Experiences**

Imagine being able to see what someone is thinking about just by looking at their brain activity. Researchers have made a breakthrough in this area, developing a new system that can reconstruct visual experiences from brain signals, without needing to train on data from the specific person being studied.

The system, called Visual Cortex Flow Architecture (VCFlow), works by mimicking the way the human visual system processes information. It uses a hierarchical approach to extract features from brain signals and combines them to create a more complete picture of what the person is seeing.

The innovation here is that VCFlow can work with brain signals from people it has never seen before, making it a potentially useful tool for clinical applications. It also works much faster than existing systems, taking just 10 seconds to generate a reconstructed video, compared to hours of computation and data collection.

This technology has the potential to help people who are unable to communicate or express themselves, and could lead to new treatments for conditions such as paralysis or blindness. The researchers plan to make their code publicly available, which could accelerate progress in this field."
cs.AI,Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything,"Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.",http://arxiv.org/abs/2511.02834v2,2025-11-04T18:59:09Z,"Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh","Here's a summary of the research paper for a general audience:

**Introducing Agent-Omni: A Breakthrough in Multimodal Reasoning**

Imagine being able to ask a computer to explain a funny meme that combines text, images, and audio. Or, picture a system that can understand a video with text, images, and sound, and provide a coherent response. This is now a reality, thanks to the Agent-Omni framework.

Researchers have developed a system that allows different computer models to work together to understand and respond to a wide range of inputs, including text, images, audio, and video. This approach, called Agent-Omni, uses a ""master agent"" to interpret user requests, assign tasks to specialized models, and combine their outputs into a single, coherent response.

The best part? This system doesn't require retraining or fine-tuning, making it flexible and adaptable to diverse inputs. Agent-Omni has achieved top-notch performance on various benchmarks, particularly on tasks that require complex reasoning across multiple modes (e.g., understanding a video with text and audio).

The Agent-Omni framework is also modular and extensible, allowing researchers to easily integrate new models and improve its capabilities over time. This innovation has the potential to revolutionize the way we interact with computers, enabling more natural and intuitive communication.

**In simple terms:** Agent-Omni is a system that helps computers understand and respond to different types of information (text, images, audio, video) by coordinating multiple models. It's flexible, adaptable, and can be improved over time, making it a significant breakthrough in multimodal reasoning."
cs.AI,Neurosymbolic Deep Learning Semantics,"Artificial Intelligence (AI) is a powerful new language of science as evidenced by recent Nobel Prizes in chemistry and physics that recognized contributions to AI applied to those areas. Yet, this new language lacks semantics, which makes AI's scientific discoveries unsatisfactory at best. With the purpose of uncovering new facts but also improving our understanding of the world, AI-based science requires formalization through a framework capable of translating insight into comprehensible scientific knowledge. In this paper, we argue that logic offers an adequate framework. In particular, we use logic in a neurosymbolic framework to offer a much needed semantics for deep learning, the neural network-based technology of current AI. Deep learning and neurosymbolic AI lack a general set of conditions to ensure that desirable properties are satisfied. Instead, there is a plethora of encoding and knowledge extraction approaches designed for particular cases. To rectify this, we introduced a framework for semantic encoding, making explicit the mapping between neural networks and logic, and characterizing the common ingredients of the various existing approaches. In this paper, we describe succinctly and exemplify how logical semantics and neural networks are linked through this framework, we review some of the most prominent approaches and techniques developed for neural encoding and knowledge extraction, provide a formal definition of our framework, and discuss some of the difficulties of identifying a semantic encoding in practice in light of analogous problems in the philosophy of mind.",http://arxiv.org/abs/2511.02825v1,2025-11-04T18:51:04Z,"Artur d'Avila Garcez, Simon Odense","Here's a summary of the research paper ""Neurosymbolic Deep Learning Semantics"" for a general audience:

**The Problem: AI Lacks Meaning**

Artificial Intelligence (AI) has made tremendous progress in recent years, even earning Nobel Prizes in chemistry and physics. However, AI's discoveries often lack clear meaning or understanding. This is because AI systems, particularly those using deep learning (a type of neural network technology), don't have a way to provide context or explanation for their findings.

**The Solution: Adding Logic to AI**

To address this issue, researchers propose using logic, a formal system for reasoning and deduction, to provide a framework for understanding AI's discoveries. They introduce a new approach called neurosymbolic AI, which combines neural networks (the basis of deep learning) with logical semantics (meaning and context). This approach aims to provide a clear mapping between AI's findings and the real world.

**The Framework: Linking Logic and Neural Networks**

The researchers present a framework that links logical semantics to neural networks, making it possible to extract meaningful insights from AI's discoveries. They review existing approaches to encoding and extracting knowledge from neural networks and provide a formal definition of their framework. They also discuss the challenges of implementing this framework in practice.

**The Goal: Improving AI's Scientific Contributions**

The ultimate goal of this research is to improve AI's contributions to science by providing a deeper understanding of the world. By adding logical semantics to AI, researchers hope to unlock new insights and discoveries that are not only accurate but also meaningful and comprehensible."
cs.AI,Kosmos: An AI Scientist for Autonomous Discovery,"Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.",http://arxiv.org/abs/2511.02824v2,2025-11-04T18:50:52Z,"Ludovico Mitchener, Angela Yiu, Benjamin Chang, Mathieu Bourdenx, Tyler Nadolski, Arvis Sulovari, Eric C. Landsness, Daniel L. Barabasi, Siddharth Narayanan, Nicky Evans, Shriya Reddy, Martha Foiani, Aizad Kamal, Leah P. Shriver, Fang Cao, Asmamaw T. Wassie, Jon M. Laurent, Edwin Melville-Green, Mayk Caldas, Albert Bou, Kaleigh F. Roberts, Sladjana Zagorac, Timothy C. Orr, Miranda E. Orr, Kevin J. Zwezdaryk, Ali E. Ghareeb, Laurie McCoy, Bruna Gomes, Euan A. Ashley, Karen E. Duff, Tonio Buonassisi, Tom Rainforth, Randall J. Bateman, Michael Skarlinski, Samuel G. Rodriques, Michaela M. Hinks, Andrew D. White","Here's a summary of the research paper ""Kosmos: An AI Scientist for Autonomous Discovery"" for a general audience:

**Introducing Kosmos: A Revolutionary AI Scientist**

Imagine a computer program that can conduct scientific research on its own, searching through vast amounts of data, reading scientific papers, and generating new ideas. Meet Kosmos, an artificial intelligence (AI) scientist that automates the process of scientific discovery.

**How Kosmos Works**

Kosmos is designed to work independently for up to 12 hours, performing tasks such as data analysis, literature search, and hypothesis generation. It uses a structured world model to organize and share information between different parts of its system, allowing it to coherently pursue a research objective over a long period.

**Impressive Results**

In tests, Kosmos was able to perform the equivalent of 6 months of research in just 20 cycles, generating valuable scientific findings that were verified by independent scientists. In fact, 79.4% of the statements in Kosmos' reports were found to be accurate. The AI scientist made several significant discoveries in fields such as metabolomics, materials science, neuroscience, and statistical genetics, including some that reproduced recent findings and others that made new contributions to the scientific literature.

**The Future of Scientific Research**

The development of Kosmos marks a significant breakthrough in AI-assisted scientific research. With its ability to automate and accelerate the discovery process, Kosmos has the potential to revolutionize the way scientists conduct research, enabling them to focus on higher-level tasks and make new breakthroughs."
cs.AI,Optimizing AI Agent Attacks With Synthetic Data,"As AI deployments become more complex and high-stakes, it becomes increasingly important to be able to estimate their risk. AI control is one framework for doing so. However, good control evaluations require eliciting strong attack policies. This can be challenging in complex agentic environments where compute constraints leave us data-poor. In this work, we show how to optimize attack policies in SHADE-Arena, a dataset of diverse realistic control environments. We do this by decomposing attack capability into five constituent skills -- suspicion modeling, attack selection, plan synthesis, execution and subtlety -- and optimizing each component individually. To get around the constraint of limited data, we develop a probabilistic model of attack dynamics, optimize our attack hyperparameters using this simulation, and then show that the results transfer to SHADE-Arena. This results in a substantial improvement in attack strength, reducing safety score from a baseline of 0.87 to 0.41 using our scaffold.",http://arxiv.org/abs/2511.02823v1,2025-11-04T18:48:56Z,"Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton","Here's a summary of the research paper for a general audience:

**Title:** Improving AI Safety by Simulating Cyber Attacks

**Summary:** As AI systems become more powerful and widespread, it's crucial to assess their potential risks. Researchers have developed a new method to evaluate and improve the safety of AI systems by simulating cyber attacks. They created a dataset of realistic scenarios, called SHADE-Arena, and broke down the attack process into five key skills. Using a computer model to simulate these attacks, they optimized each skill to create more effective attack strategies. This approach allowed them to significantly improve the strength of their simulated attacks, which can help identify vulnerabilities in AI systems and ultimately make them safer.

**In simpler terms:** Imagine you're trying to protect a computer system from hackers. To do this effectively, you need to understand how hackers might try to attack it. This research helps by creating a simulated environment to test and improve AI defenses against potential cyber threats. By doing so, researchers can identify areas where AI systems need to be strengthened, making them more secure and reliable."
cs.AI,Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning,"Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .",http://arxiv.org/abs/2511.02818v1,2025-11-04T18:43:44Z,"Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu","**Breakthrough in Tabular Data Analysis: Introducing Orion-MSP**

Tabular data, which is data organized in rows and columns, is widely used in many real-world applications. However, developing effective artificial intelligence (AI) models to analyze this type of data has been challenging due to its complex and diverse nature. Researchers have made recent progress in tabular in-context learning, a technique that enables AI models to learn from tabular data without requiring extensive fine-tuning.

A new AI model, called Orion-MSP, has been developed to address the limitations of existing models. Orion-MSP features three key innovations:

1. **Multi-scale processing**: This allows the model to capture complex interactions between different features at various scales, similar to how humans analyze data at different levels of detail.
2. **Efficient attention mechanism**: Orion-MSP uses a block-sparse attention mechanism that reduces computational costs and enables the model to handle large tables with thousands of columns.
3. **Bidirectional information flow**: The model enables safe communication between different components, allowing it to refine its representations and make more accurate predictions.

**Orion-MSP Achieves State-of-the-Art Performance**

Orion-MSP has been tested on diverse benchmarks and has achieved state-of-the-art performance, matching or surpassing existing models while scaling effectively to high-dimensional tables. This breakthrough has the potential to improve the analysis of tabular data in various applications, such as finance, healthcare, and marketing.

The Orion-MSP model is publicly available, and its code can be accessed at https://github.com/Lexsi-Labs/Orion-MSP. This open-source release enables researchers and practitioners to build upon this innovation and apply it to real-world problems."
cs.AI,Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities,"As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.",http://arxiv.org/abs/2511.02817v1,2025-11-04T18:42:12Z,"Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley","Here's a summary of the research paper for a general audience:

**The Challenge of Long Context Reasoning in AI Models**

As AI models become better at processing longer texts, researchers are wondering if these models are truly using all the information they're given. To test this, a team of researchers created a new benchmark called Oolong, which evaluates a model's ability to analyze and reason over long texts.

**What makes Oolong unique?**

Unlike previous tests, Oolong requires models to analyze individual parts of a text and then combine those analyses to answer questions about the text as a whole. This is a more complex task that requires models to reason and aggregate information.

**The Oolong benchmark**

The Oolong benchmark consists of two sets of tasks: Oolong-synth, which uses synthetic data to test specific reasoning skills, and Oolong-real, which uses real-world conversational data. Models are tested on their ability to classify, count, and reason over temporal and user relationships in large amounts of text.

**The results**

Even the most advanced AI models, such as GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro, struggled with Oolong, achieving less than 50% accuracy on both task sets. This suggests that there is still much work to be done to develop models that can effectively reason over long texts.

**What's next?**

The researchers are releasing the Oolong data and evaluation harness to the public, which will enable other researchers to develop and test new models that can better handle long context reasoning tasks."
cs.AI,Assessing win strength in MLB win prediction models,"In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset. In addition, we relate the win probabilities produced by these models to win strength as measured by score differential. In doing so we show that the most common machine learning models do indeed demonstrate a relationship between predicted win probability and the strength of the win. Finally, we analyze the results of using predicted win probabilities as a decision making mechanism on run-line betting. We demonstrate positive returns when utilizing appropriate betting strategies, and show that naive use of machine learning models for betting lead to significant loses.",http://arxiv.org/abs/2511.02815v1,2025-11-04T18:40:10Z,"Morgan Allen, Paul Savala","Here's a summary of the research paper for a general audience:

**Predicting Baseball Game Outcomes: How Accurate are Machine Learning Models?**

Researchers have been using machine learning models to predict which team will win a baseball game. But how well do these models really work? A new study trained several machine learning models on a common dataset to predict the outcome of Major League Baseball games. The study found that these models can accurately predict not only which team will win, but also how convincingly they will win (measured by the score differential).

The researchers also explored using these models to inform betting decisions on run-line bets (a type of bet where the goal is to predict the margin of victory). They found that using machine learning models to make betting decisions can lead to positive returns, but only if done strategically. Simply relying on the models' predictions without careful consideration can lead to significant losses.

Overall, the study highlights the potential of machine learning models to improve our understanding of baseball game outcomes, but also emphasizes the importance of using these models thoughtfully and strategically."
cs.AI,"MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning","Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher",http://arxiv.org/abs/2511.02805v1,2025-11-04T18:27:39Z,"Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han","**Improving AI Search Agents: A New Approach**

Imagine you're chatting with a virtual assistant, asking it a series of questions on a topic. Current AI systems can struggle with this type of conversation, as they either use too much memory and computing power or forget important information. Researchers have proposed a new solution called MemSearcher, which aims to balance information retention with efficiency.

**The Problem with Current AI Search Agents**

Current AI search agents have two main limitations:

1. **Information overload**: They store entire conversation histories, which can be long and noisy, making it difficult to process and retrieve relevant information.
2. **Information loss**: They only use the current question, discarding essential context and previous interactions.

**How MemSearcher Works**

MemSearcher addresses these limitations by:

1. Maintaining a compact memory of essential information.
2. Combining the current question with the memory to generate answers and perform search actions.
3. Updating the memory to retain only relevant information.

**Optimizing MemSearcher**

To optimize MemSearcher, researchers developed a new reinforcement learning framework called multi-context GRPO. This framework jointly optimizes three key aspects:

1. **Reasoning**: The ability to draw logical conclusions from the information.
2. **Search strategies**: The ability to effectively search for relevant information.
3. **Memory management**: The ability to manage the compact memory.

**Results and Implications**

MemSearcher achieved significant improvements over existing AI models on seven public benchmarks, with:

* 11% improvement on a 3B model (Qwen2.5-3B-Instruct)
* 12% improvement on a 7B model (Qwen2.5-7B-Instruct)

Notably, the 3B-based MemSearcher even outperformed 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead.

**The Future of AI Search Agents**

The MemSearcher approach has the potential to improve the performance and efficiency of AI search agents, enabling more effective and scalable conversational AI systems. The code and models used in this research will be publicly available, allowing other researchers to build upon this work."
cs.AI,TabTune: A Unified Library for Inference and Fine-Tuning Tabular   Foundation Models,"Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.",http://arxiv.org/abs/2511.02802v2,2025-11-04T18:25:17Z,"Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu","**Unlocking the Power of Tabular Data with TabTune**

Imagine having a single tool that can help you make sense of complex data, like spreadsheets or databases, and make accurate predictions or decisions. That's what TabTune is - a new library that simplifies working with tabular foundation models, a type of artificial intelligence (AI) that's trained on large amounts of data.

Tabular foundation models have shown great promise in analyzing structured data, but their use has been limited due to the complexity of working with them. TabTune changes that by providing a unified interface for accessing seven state-of-the-art models, making it easy to:

1. **Make predictions**: Use pre-trained models to make predictions on new data.
2. **Fine-tune models**: Adjust the models to fit your specific needs.
3. **Evaluate performance**: Assess how well the models are doing, including metrics like accuracy, calibration, and fairness.

TabTune automates many of the tedious tasks involved in working with tabular foundation models, such as data preprocessing and model evaluation. This makes it easier for researchers and practitioners to compare different models and techniques, and to develop new ones.

**What does this mean?**

TabTune has the potential to unlock the full power of tabular data, enabling organizations to make better decisions, improve their operations, and drive innovation. By providing a standardized framework for working with tabular foundation models, TabTune can help to:

* **Improve accuracy**: By making it easier to access and compare different models, TabTune can help organizations choose the best model for their needs.
* **Increase efficiency**: By automating many tasks, TabTune can save time and resources.
* **Promote fairness and transparency**: By providing tools for evaluating model performance, including fairness and calibration, TabTune can help organizations ensure that their models are fair and transparent.

Overall, TabTune is an exciting development that can help to democratize access to AI and unlock the full potential of tabular data."
cs.AI,When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal   Reasoning,"Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing. A simple fusion mechanism aggregates these outputs, exposing contributors (modalities supporting correct outcomes) and saboteurs (modalities that mislead). Applying our diagnostic layer in a case study on multimodal emotion recognition benchmarks with foundation models revealed systematic reliability profiles, providing insight into whether failures may arise from dataset artifacts or model limitations. More broadly, our framework offers a diagnostic scaffold for multimodal reasoning, supporting principled auditing of fusion dynamics and informing possible interventions.",http://arxiv.org/abs/2511.02794v1,2025-11-04T18:20:13Z,"Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang","Here's a summary of the research paper for a general audience:

**The Problem with Multimodal AI Models**

Imagine you're trying to understand a joke that involves both a funny image and a caption. A multimodal AI model would analyze both the image and the caption to understand the joke. However, it's often unclear how the model combines these different types of information (or modalities) to make a decision. For instance, is the model relying more on the image or the caption?

**A New Diagnostic Tool**

Researchers have developed a new tool to help understand how multimodal AI models work. This tool, called a diagnostic layer, treats each type of information (e.g., image, caption) as a separate ""agent"" that provides its own opinion and confidence level. The agents' opinions are then combined to produce a final answer. The diagnostic layer also identifies which agents are contributing to correct answers and which ones are causing errors (or ""sabotaging"" the result).

**What the Researchers Found**

The researchers applied their diagnostic tool to a multimodal AI model that recognizes emotions from images and text. They found that sometimes, one type of information would dominate the others, leading to incorrect answers. For example, if the image is ambiguous but the caption is clear, the model might rely too heavily on the caption and ignore the image. This ""modality sabotage"" can lead to systematic errors, which can arise from either the dataset used to train the model or limitations in the model itself.

**The Impact**

The researchers' diagnostic tool provides a way to audit and understand how multimodal AI models combine different types of information. This can help identify potential errors and inform ways to improve the models. Ultimately, this research aims to make multimodal AI models more reliable and trustworthy."
cs.AI,Measuring AI Diffusion: A Population-Normalized Metric for Tracking   Global AI Usage,"Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion. We find wide variation in adoption, with a strong correlation between AI User Share and GDP. High uptake is concentrated in developed economies, though usage among internet-connected populations in lower-income countries reveals substantial latent demand. We also detect sharp increases in usage following major product launches, such as DeepSeek in early 2025. While the metric's reliance solely on Microsoft telemetry introduces potential biases related to this user base, it offers an important new lens into how AI is spreading globally. AI User Share enables timely benchmarking that can inform data-driven AI policy.",http://arxiv.org/abs/2511.02781v1,2025-11-04T18:03:51Z,"Amit Misra, Jane Wang, Scott McCullers, Kevin White, Juan Lavista Ferres","**Understanding AI Adoption Around the World**

Imagine being able to track how people are using artificial intelligence (AI) tools, like chatbots or virtual assistants, across the globe. Researchers have created a new way to measure this, called AI User Share. This metric estimates the percentage of a country's working-age population that actively uses AI tools.

The researchers used anonymized data from Microsoft to calculate AI User Share for 147 countries. They found that AI adoption varies widely, with richer countries leading the way. However, in lower-income countries, many people who have access to the internet are eager to use AI tools.

The study also discovered that AI usage surges when new AI products are launched. For example, after a major AI product called DeepSeek was released in early 2025, usage increased sharply.

While this new metric provides valuable insights into AI adoption, it has some limitations. It only uses data from Microsoft users, which might not represent the entire global population. Nevertheless, AI User Share offers a new perspective on how AI is spreading globally and can help inform policies related to AI.

**Key Takeaways:**

* A new metric, AI User Share, tracks AI adoption across 147 countries.
* Richer countries lead in AI adoption, but there's latent demand in lower-income countries.
* AI usage increases with new product launches.
* The metric provides insights for data-driven AI policy."
cs.AI,1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts,"Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce POCO, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. POCO autonomously generates PoC exploits in an agentic manner by interacting with a set of code-execution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate POCO on a dataset of 23 real-world vulnerability reports. POCO consistently outperforms the prompting and workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides readily actionable knowledge for the smart contract security community.",http://arxiv.org/abs/2511.02780v1,2025-11-04T18:03:12Z,"Vivi Andersson, Sofia Bobadilla, Harald Hobbelhagen, Martin Monperrus","Here's a summary of the research paper for a general audience:

**Title:** 1 PoCo: Automatically Generating Proof-of-Concept Exploits for Smart Contracts

**What it's about:** Smart contracts are digital agreements that execute automatically, but they can be vulnerable to hacking. To prevent this, security experts perform audits to identify potential weaknesses. A crucial part of these audits is creating ""proof-of-concept"" (PoC) exploits, which demonstrate that a vulnerability is real and can be exploited.

**The problem:** Creating PoCs manually is time-consuming, prone to errors, and often rushed due to tight deadlines. This can lead to incomplete or inaccurate PoCs, which can put smart contracts at risk.

**The solution:** Researchers have developed a new framework called POCO, which automatically generates PoC exploits from natural-language descriptions of vulnerabilities. POCO uses a combination of artificial intelligence and code-execution tools to produce fully executable exploits that can be easily integrated into audit reports.

**The results:** POCO was tested on 23 real-world vulnerability reports and outperformed existing methods, generating high-quality PoCs that were well-formed and logically correct. This new framework has the potential to significantly reduce the effort required to create PoCs, making smart contract audits more efficient and effective.

**Why it matters:** By automating the process of generating PoCs, POCO can help prevent financial losses due to smart contract vulnerabilities. This research provides a valuable tool for the smart contract security community, enabling them to identify and address potential weaknesses more effectively."
cs.AI,STAR-VAE: Latent Variable Transformers for Scalable and Controllable   Molecular Generation,"The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.",http://arxiv.org/abs/2511.02769v1,2025-11-04T17:56:00Z,"Bum Chul Kwon, Ben Shapira, Moshiko Raboh, Shreyans Sethi, Shruti Murarka, Joseph A Morrone, Jianying Hu, Parthasarathy Suryanarayanan","**Breakthrough in Molecular Generation: STAR-VAE Paves the Way for Efficient and Controllable Drug Discovery**

Researchers have developed a new artificial intelligence (AI) model called STAR-VAE, which can efficiently generate new molecules with specific properties, a crucial step in discovering new medicines. The model uses a type of machine learning called a variational autoencoder (VAE) to learn the patterns and structures of a vast library of molecules.

The STAR-VAE model has several key advantages:

1. **Scalability**: It can handle massive amounts of data, making it suitable for large-scale molecular generation tasks.
2. **Controllability**: The model allows researchers to specify desired properties, such as improved binding affinity, and generate molecules that meet those criteria.
3. **Efficiency**: STAR-VAE can adapt quickly to new data, making it faster and more efficient than existing models.

The researchers tested STAR-VAE on several benchmarks and found that it outperformed or matched existing models in generating high-quality molecules. The model's latent space, which represents the underlying patterns and structures of the molecules, was found to be smooth and semantically structured, enabling both unconditional exploration and property-aware generation.

The development of STAR-VAE has significant implications for the field of drug discovery, as it provides a powerful tool for generating new molecules with specific properties, which can accelerate the discovery of new medicines."
cs.AI,LLM-Supported Formal Knowledge Representation for Enhancing Control   Engineering Content with an Interactive Semantic Layer,"The rapid growth of research output in control engineering calls for new approaches to structure and formalize domain knowledge. This paper briefly describes an LLM-supported method for semi-automated generation of formal knowledge representations that combine human readability with machine interpretability and increased expressiveness. Based on the Imperative Representation of Knowledge (PyIRK) framework, we demonstrate how language models can assist in transforming natural-language descriptions and mathematical definitions (available as LaTeX source code) into a formalized knowledge graph. As a first application we present the generation of an ``interactive semantic layer'' to enhance the source documents in order to facilitate knowledge transfer. From our perspective this contributes to the vision of easily accessible, collaborative, and verifiable knowledge bases for the control engineering domain.",http://arxiv.org/abs/2511.02759v1,2025-11-04T17:36:57Z,"Julius Fiedler, Carsten Knoll, Klaus RÃ¶benack","Here's a summary of the research paper for a general audience:

**Making Control Engineering Knowledge More Accessible and Useful**

Control engineering is a field that involves designing and optimizing systems that control machines and processes. With so much new research being published, it's becoming increasingly difficult to keep track of all the knowledge in the field. This paper proposes a new way to organize and represent control engineering knowledge in a way that's easy for humans to understand and can also be interpreted by computers.

The researchers used a type of artificial intelligence called a language model to help convert natural language descriptions and mathematical equations into a formal, structured format. This format, called a knowledge graph, allows computers to understand the relationships between different concepts and makes it easier to search and retrieve information.

The goal of this work is to create a system that makes control engineering knowledge more accessible, collaborative, and reliable. The researchers envision a future where knowledge bases are easily searchable, verifiable, and can be contributed to by many people. This could lead to faster progress and innovation in the field of control engineering.

**What does this mean for non-experts?**

In simple terms, this research aims to make complex technical information more organized, understandable, and usable. This could have implications for many fields beyond control engineering, such as healthcare, finance, and education, where making complex information more accessible and usable could lead to significant benefits."
cs.AI,AI Diffusion in Low Resource Language Countries,"Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.",http://arxiv.org/abs/2511.02752v1,2025-11-04T17:31:39Z,"Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres","**Artificial Intelligence Adoption Lags in Countries with Limited Language Resources**

Artificial intelligence (AI) is becoming increasingly popular worldwide, but its adoption is not uniform across all countries. A recent study found that countries with limited language resources are lagging behind in AI adoption. The main reason for this disparity is that AI systems, specifically large language models, perform poorly on languages with limited data availability.

The study analyzed data from various countries and found that countries with limited language resources have about 20% fewer AI users compared to countries with more abundant language resources. This gap cannot be explained by differences in socioeconomic status or demographics alone.

The findings suggest that linguistic accessibility is a significant barrier to the equitable spread of AI technology. In other words, people who speak languages with limited online presence or resources are less likely to benefit from AI advancements. This highlights the need for more inclusive and diverse AI development that can cater to a broader range of languages and cultures."
cs.AI,Using Span Queries to Optimize for Cache and Attention Locality,"Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.",http://arxiv.org/abs/2511.02749v1,2025-11-04T17:22:49Z,"Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin","**Improving AI Performance with Span Queries**

Artificial intelligence (AI) models are becoming increasingly complex and are being used for a variety of tasks beyond simple chatbots. However, the systems that support these models are still optimized for chatbots, which can lead to inefficiencies. Researchers have proposed a new approach called ""span queries"" to improve the performance of AI models.

**What are Span Queries?**

Span queries are a way to express complex AI tasks as a series of interconnected queries. This allows AI models to be used for a wide range of tasks, including chatbots, reasoning, and decision-making. The key innovation of span queries is that they can be optimized to reduce the amount of data that needs to be retrieved from memory, which can significantly improve performance.

**Benefits of Span Queries**

The researchers demonstrated that span queries can lead to significant improvements in performance, including:

* 10-20x reductions in the time it takes to respond to user queries
* Improved accuracy, even with smaller models

**How it Works**

Span queries work by allowing AI models to be optimized for specific tasks, rather than being limited to a single use case. This is achieved by representing AI tasks as expression trees, which can be linked together with constraints that specify whether the order of inputs matters. By optimizing these expression trees, AI models can be made to run more efficiently and accurately.

**Real-World Impact**

The researchers showed that span queries can be applied to a range of AI tasks, including chatbots, reasoning, and decision-making. They also demonstrated that span queries can be used to improve the accuracy of AI models, even with smaller models. This has significant implications for the development of more efficient and effective AI systems."
cs.AI,CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in   Dynamic Environments for LLM Tool-Use Agents,"Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.",http://arxiv.org/abs/2511.02734v1,2025-11-04T16:58:29Z,"Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, Yi R. Fung","**Improving AI Agents' Ability to Plan and Adapt in a Changing World**

Imagine you're planning a trip and need to book flights, hotels, and transportation. You want to find the most cost-effective way to do it, but what if flight prices change or your preferred hotel is fully booked? Current AI agents, like those using Large Language Models (LLMs), are good at completing tasks, but they often don't consider the cost or adapt well to changes.

To address this, researchers have created a new benchmark called CostBench. It's a tool to evaluate AI agents' ability to plan and adapt in a cost-effective way, especially in dynamic environments where things can change unexpectedly. CostBench simulates real-world scenarios, like travel planning, and tests agents' ability to adjust plans in response to changes, such as tool failures or cost changes.

The results show that even top AI models, like GPT-5, struggle to find the most cost-effective solutions, especially in dynamic situations. For example, GPT-5 achieved less than 75% accuracy on the hardest tasks, and its performance dropped by around 40% when faced with dynamic conditions. These findings highlight the need for more research to develop AI agents that can plan, adapt, and make economically rational decisions in a changing world.

The introduction of CostBench marks an important step towards creating more sophisticated AI agents that can handle real-world complexities. By identifying the weaknesses of current AI agents, researchers can develop new agents that are both economically rational and robust, leading to more efficient and effective solutions for tasks like travel planning."
cs.AI,LLEXICORP: End-user Explainability of Convolutional Neural Networks,"Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.",http://arxiv.org/abs/2511.02720v1,2025-11-04T16:44:45Z,"VojtÄch KÅ¯r, Adam Bajger, Adam KukuÄka, Marek Hradil, VÃ­t Musil, TomÃ¡Å¡ BrÃ¡zdil","**Making AI More Transparent: A New Approach to Understanding Neural Networks**

Convolutional neural networks (CNNs) are a type of artificial intelligence (AI) that powers many modern computer vision systems, such as image recognition and object detection. However, these complex systems can be difficult to understand and trust, which is a major concern in areas like healthcare, finance, and transportation.

Recently, researchers have developed methods to explain how CNNs make decisions, known as Explainable AI (XAI). One approach, called Concept Relevance Propagation (CRP), tries to identify the key features or concepts that a CNN uses to classify images. However, this process is often manual and time-consuming, requiring experts to interpret and explain the results.

A new method, called LLEXICORP, aims to automate and simplify this process. By combining CRP with a large language model, LLEXICORP can automatically generate natural-language explanations of how a CNN makes decisions. This approach assigns descriptive names to the key concepts and creates intuitive narratives that can be tailored to different audiences.

In a test on a VGG16 model, LLEXICORP was able to provide clear and concise explanations of how the AI system classified various images from ImageNet. The results suggest that integrating concept-based attribution methods with large language models can make it easier to understand and interpret deep neural networks, leading to more transparent and trustworthy AI systems. This breakthrough has the potential to increase confidence in AI and make it more accessible to a wider range of users."
cs.AI,An unscented Kalman filter method for real time input-parameter-state   estimation,"The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.",http://arxiv.org/abs/2511.02717v1,2025-11-04T16:39:27Z,"Marios Impraimakis, Andrew W. Smyth","Here's a summary of the research paper for a general audience:

**Title:** A New Method for Real-Time Estimation of System Inputs and States

**Summary:** Researchers have developed a new method that uses a type of mathematical filter, called an unscented Kalman filter, to estimate the inputs, states, and parameters of a system in real-time. This method is useful for understanding complex systems, such as those found in engineering, physics, and other fields.

**Key Breakthrough:** The method can estimate not only the state of a system (e.g., its current condition) and its parameters (e.g., physical properties), but also the input or force acting on the system. This is a significant improvement over traditional methods that can only estimate some of these factors.

**How it works:** The method uses two stages to make estimates. First, it predicts the system's state and parameters based on past data. Then, it refines these estimates using new measurements. This approach allows for accurate and real-time estimation of the system's inputs, states, and parameters.

**Implications:** This new method has the potential to improve our understanding of complex systems, enabling better monitoring, control, and prediction of their behavior. It can be applied to a wide range of fields, from engineering and physics to finance and environmental science."
cs.AI,The Collaboration Gap,"The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a ""collaboration gap"": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a ""relay inference"" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.",http://arxiv.org/abs/2511.02687v1,2025-11-04T16:10:57Z,"Tim R. Davidson, Adam Fourney, Saleema Amershi, Robert West, Eric Horvitz, Ece Kamar","**The Collaboration Gap: A Challenge in AI Development**

As AI systems become more complex, they will rely on multiple independent agents working together to achieve common goals. However, a new study reveals that these agents often struggle to collaborate effectively, even when they are highly skilled on their own.

Researchers created a maze-solving benchmark to test the collaboration abilities of 32 leading AI models, both alone and in pairs. The results showed that models that performed well on their own often degraded significantly when required to work together. In some cases, smaller models that were highly skilled alone failed almost completely when paired with another model.

The study found that having the stronger model lead the collaboration, and then handing off to the weaker model, can improve outcomes. This ""relay inference"" approach can help close the collaboration gap.

The findings have important implications for AI development:

* **Collaboration-aware evaluation**: AI models should be tested on their ability to collaborate, not just their individual performance.
* **Training strategies for collaboration**: AI models should be trained to work effectively with others.
* **Designing interactions that bring out the best in agents**: AI systems should be designed to elicit the strengths of each agent, whether human or AI.

These insights are relevant not only to AI-AI collaboration but also to human-AI collaboration, highlighting the need for a more collaborative approach to AI development."
cs.CL,Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything,"Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.",http://arxiv.org/abs/2511.02834v2,2025-11-04T18:59:09Z,"Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh","Here's a summary of the research paper for a general audience:

**Introducing Agent-Omni: A Breakthrough in Multimodal Reasoning**

Imagine being able to ask a computer to explain a joke that includes both text and an image, or to describe a video with accompanying audio commentary. Current artificial intelligence (AI) models can handle some of these tasks, but they're limited to specific combinations of text, images, audio, and video, and require extensive training data.

Researchers have now developed a new framework called Agent-Omni, which enables AI models to reason across multiple modes of communication, such as text, images, audio, and video, without requiring extensive retraining. This framework works by coordinating multiple specialized AI models, each designed to handle a specific type of data, to work together to understand complex inputs.

The Agent-Omni system consists of a ""master agent"" that interprets user requests, assigns tasks to specialized ""agent"" models, and integrates their outputs into a coherent response. This approach allows the system to adapt to diverse inputs, maintain transparency, and be easily extensible as better models become available.

**Key Benefits and Achievements**

* Agent-Omni achieves state-of-the-art performance on a wide range of tasks, particularly those requiring complex cross-modal reasoning.
* The framework is modular, flexible, and can seamlessly integrate specialized models, making it easy to improve over time.
* Agent-Omni enables robust reasoning support across text, images, audio, video, and combinations thereof.

Overall, Agent-Omni represents a significant step forward in multimodal reasoning, enabling AI systems to better understand and interpret complex, real-world inputs."
cs.CL,In Good GRACEs: Principled Teacher Selection for Knowledge Distillation,"Knowledge distillation is an efficient strategy to use data generated by large ""teacher"" language models to train smaller capable ""student"" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.",http://arxiv.org/abs/2511.02833v1,2025-11-04T18:58:47Z,"Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Sham Kakade, Surbhi Goel","Here's a summary of the research paper for a general audience:

**The Problem:** Training small AI models to perform well on complex tasks often requires a lot of data and computational power. One way to overcome this is to use a large, pre-trained ""teacher"" model to guide the training of a smaller ""student"" model. However, finding the best teacher for a specific student and task can be a time-consuming and expensive process.

**The Solution:** Researchers have proposed a new method called GRACE, which helps select the optimal teacher for a given student and task. GRACE is a simple and efficient score that predicts how well a teacher will work with a student model. It does this by analyzing the student's behavior during training, without needing access to additional data or resources.

**The Results:** The researchers tested GRACE on several math problems and found that it can accurately predict which teachers will work best with a given student model. In fact, using GRACE to select a teacher improved the student's performance by up to 7.4% compared to randomly selecting a teacher. GRACE also provides guidance on other important aspects of the training process, such as how to generate data from the teacher and which teacher to choose given certain constraints.

**The Impact:** This research has the potential to make AI training more efficient and effective, which could lead to breakthroughs in areas such as natural language processing, computer vision, and more. By streamlining the process of selecting a teacher model, GRACE could help researchers and developers create more accurate and capable AI models."
cs.CL,Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities,"As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.",http://arxiv.org/abs/2511.02817v1,2025-11-04T18:42:12Z,"Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley","**Unlocking the Potential of Long Context Reasoning in AI Models**

As AI models become increasingly capable of processing longer and longer pieces of text, researchers are raising important questions about whether these models are truly making use of all the information they're given. To investigate this, a team of researchers has created a new benchmark called Oolong, which tests a model's ability to analyze and reason about large amounts of text.

Oolong is designed to push AI models to their limits by requiring them to analyze individual chunks of text, and then combine those analyses to answer complex questions. The benchmark consists of two sets of tasks: Oolong-synth, which uses synthetic data to test specific reasoning skills, and Oolong-real, which uses real-world conversational data to simulate real-world scenarios.

The results are surprising: even the most advanced AI models, such as GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro, struggle to perform well on Oolong, achieving accuracy rates of less than 50% on both sets of tasks, even when given a large context of 128,000 tokens.

The researchers behind Oolong hope that their work will help drive the development of more advanced AI models that can truly make use of long context, and they've made their data and evaluation tools available to the wider research community. By doing so, they aim to enable the creation of more powerful and capable AI models that can tackle complex tasks and provide more accurate results."
cs.CL,"MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning","Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher",http://arxiv.org/abs/2511.02805v1,2025-11-04T18:27:39Z,"Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han","**Breakthrough in AI-Powered Search: MemSearcher**

Imagine having a conversational AI assistant that can efficiently and accurately help you find information across multiple interactions. Researchers have made a significant step towards achieving this goal with the development of MemSearcher, a novel AI framework that enables large language models (LLMs) to reason, search, and manage memory more effectively.

The challenge with current search agents is that they either use too much information, making them slow and computationally expensive, or they discard important information, reducing their accuracy. MemSearcher addresses this trade-off by maintaining a compact memory that is iteratively updated to retain only essential information. This approach stabilizes the context length across interactions, improving efficiency without sacrificing accuracy.

The researchers also introduced a new end-to-end reinforcement learning framework, called multi-context GRPO, which optimizes the MemSearcher workflow. This framework allows the AI to learn how to reason, search, and manage memory more effectively.

The results are impressive: MemSearcher outperformed strong baselines on seven public benchmarks, achieving significant improvements in accuracy while reducing computational overhead. Notably, a smaller MemSearcher model (3B) even outperformed a larger baseline model (7B), demonstrating the efficiency and effectiveness of the MemSearcher approach.

The code and models will be publicly available, making it possible for others to build upon this research and develop more advanced AI-powered search systems. This breakthrough has the potential to enable more efficient and accurate conversational AI assistants, transforming the way we interact with information."
cs.CL,Can LLMs subtract numbers?,"We present a systematic study of subtraction in large language models (LLMs). While prior benchmarks emphasize addition and multiplication, subtraction has received comparatively little attention despite being structurally distinct as a non-commutative operation. We evaluate eight pretrained LLMs spanning four families on addition and subtraction problems. Our experiments reveal that subtraction accuracy lags behind addition by a wide margin. We find that the errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs frequently produce the correct magnitude but omit the negative sign. Probing analyses show that LLMs internally encode whether results should be negative, yet this information is often not reflected in generated outputs. We further test well-known techniques such as few-shot learning and instruction-tuning to see if they can improve the LLMs' performance. Our results suggest that while few-shot prompting yields modest gains, the instruction-tuned models achieve near-perfect accuracies in generating the negative sign. Together, these findings provide a clearer characterization of the limitations and recoverability of LLMs' arithmetic capabilities in subtraction.",http://arxiv.org/abs/2511.02795v1,2025-11-04T18:20:17Z,"Mayank Jobanputra, Nils Philipp Walter, Maitrey Mehta, Blerta Veseli, Evan Parker Kelly Chapple, Yifan Wang, Sneha Chetani, Ellie Pavlick, Antonio Vergari, Vera Demberg","**Can Large Language Models Subtract Numbers?**

Large language models (LLMs) have shown impressive abilities in performing mathematical operations, but researchers have found that they struggle with subtraction. A recent study tested eight LLMs on addition and subtraction problems and discovered that while they perform well on addition, their accuracy drops significantly when it comes to subtraction.

The study found that LLMs often get subtraction wrong when the number being subtracted is larger than the number it's being subtracted from (e.g., 5 - 7). In these cases, the models frequently produce the correct magnitude (e.g., 2), but forget to include the negative sign (e.g., -2).

However, the researchers also found that LLMs internally understand when a result should be negative, but this information isn't always reflected in their outputs. To improve performance, the researchers tested techniques like few-shot learning (providing examples) and instruction-tuning (fine-tuning the model). They found that these methods can modestly improve subtraction accuracy, with instruction-tuned models performing particularly well.

Overall, the study provides new insights into the limitations of LLMs' arithmetic capabilities, particularly with subtraction. While LLMs have impressive abilities, they still have room for improvement when it comes to performing mathematical operations."
cs.CL,VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual   Representation,"Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.",http://arxiv.org/abs/2511.02778v1,2025-11-04T18:00:18Z,"Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang","**Introducing VCode: A New Benchmark for Visual Coding**

Imagine being able to describe a picture using code, just like a computer program. Researchers have created a new benchmark called VCode, which tests the ability of artificial intelligence (AI) models to generate code that accurately represents visual information. This code uses a format called SVG (Scalable Vector Graphics), which is like a digital drawing that can be edited and manipulated.

**The Challenge: From Images to Code**

The goal of VCode is to take an image and produce SVG code that captures its meaning. For example, if you show a picture of a cat, the AI model should generate code that describes the cat's shape, size, and features. This code can then be used for further processing, like answering questions about the image.

**Evaluating AI Models**

To assess how well AI models perform, researchers have developed a new evaluation protocol called CodeVQA. This involves rendering the generated SVG code as an image and asking questions about it. If the model answers correctly, it means the SVG code accurately represents the original image.

**The Results: A Gap in AI Capabilities**

The researchers found that even the best AI models struggle to generate accurate SVG code, revealing a gap between language-based and visual-based coding abilities. To address this gap, they developed VCoder, a new framework that helps AI models improve their visual coding skills.

**VCoder: A New Framework for Visual Coding**

VCoder uses two key strategies:

1. **Thinking with Revision**: The model iteratively analyzes and refines its SVG code to ensure accuracy.
2. **Acting with Visual Tools**: The model uses detectors and parsers to gather structured information about the image, such as objects, shapes, and text.

**The Impact: Improving AI Capabilities**

The researchers found that VCoder significantly improves the performance of AI models on the VCode benchmark, outperforming the top-performing model by 12.3 points. Human studies also showed that both humans and AI models perform better on images than on rendered SVG code, highlighting the potential of symbolic visual representation.

**What's Next?**

The VCode benchmark and VCoder framework are publicly available, providing a new standard for evaluating and improving AI models' visual coding abilities. This research has the potential to enable more accurate and efficient processing of visual information, with applications in areas like computer vision, robotics, and human-computer interaction."
cs.CL,Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query   Retrieval,"Most text retrievers generate \emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.",http://arxiv.org/abs/2511.02770v1,2025-11-04T17:57:20Z,"Hung-Ting Chen, Xiang Liu, Shauli Ravfogel, Eunsol Choi","**Improving Search Results with Multiple Query Vectors**

Imagine you're searching for a term like ""bank"" online. You might be looking for a financial institution or the side of a river. Traditional search engines use a single ""query vector"" to find relevant results, but this approach can struggle when there are multiple possible interpretations of the search term.

Researchers have found that existing search systems perform poorly when the relevant results are diverse and far apart in terms of their digital representations. To address this limitation, a team developed a new search system called Autoregressive Multi-Embedding Retriever (AMER).

AMER generates multiple query vectors for a single search term, allowing it to capture different possible meanings and retrieve a wider range of relevant results. In tests, AMER outperformed traditional single-vector search systems, especially when the relevant results were diverse and dissimilar.

On synthetic data, AMER showed four times better performance than single-vector models. On real-world datasets, AMER achieved relative gains of 4% and 21% over single-vector baselines. The benefits of AMER were even more pronounced when the relevant results had diverse digital representations.

This research opens up new possibilities for improving search engines and information retrieval systems, highlighting the potential of using multiple query vectors to capture the complexity of human language and search queries."
cs.CL,Controlling Performance and Budget of a Centralized Multi-agent LLM   System with Reinforcement Learning,"Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems.",http://arxiv.org/abs/2511.02755v1,2025-11-04T17:35:17Z,"Bowen Jin, TJ Collins, Donghan Yu, Mert Cemri, Shenao Zhang, Mengyu Li, Jay Tang, Tian Qin, Zhiyang Xu, Jiarui Lu, Guoli Yin, Jiawei Han, Zirui Wang","Here's a summary of the research paper for a general audience:

**Title:** Smarter AI Systems on a Budget

**Goal:** Large language models (LLMs) are powerful tools, but they can be expensive to run and have varying levels of expertise. Researchers wanted to create a system that combines multiple LLMs to work together efficiently, while controlling costs.

**Approach:** They designed a centralized system where a ""controller"" LLM decides which expert models to use for a given task, and when. This approach allows for more cost-effective and flexible use of LLMs.

**Innovation:** The researchers used a technique called reinforcement learning to train the system to balance two goals: achieving high performance and minimizing costs. They also wanted the system to adapt to different budget conditions, so they developed a framework called CoRL.

**Results:** Experiments showed that CoRL enables a single system to outperform the best individual LLM when budget is not a concern, while still performing well even with limited budgets. This approach has the potential to make AI systems more efficient, scalable, and cost-effective.

**In simple terms:** Imagine having a team of experts with different skills, and a manager who decides who to involve in a project based on the task and budget. This research creates a similar system for AI models, allowing them to work together more efficiently and cost-effectively."
cs.CL,AI Diffusion in Low Resource Language Countries,"Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.",http://arxiv.org/abs/2511.02752v1,2025-11-04T17:31:39Z,"Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres","**Artificial Intelligence Adoption Lags in Countries with Limited Language Resources**

Artificial intelligence (AI) is becoming increasingly popular around the world, but its adoption is not happening at the same rate everywhere. A new study found that countries with limited language resources are falling behind. The problem lies in the way AI systems are trained: they rely on vast amounts of data, which is often scarce for languages spoken by smaller populations.

The study discovered that countries with limited language resources have about 20% fewer AI users compared to other countries with similar economic and demographic profiles. This suggests that the lack of language accessibility is a significant obstacle to the fair spread of AI technology.

In essence, the study highlights the need for more inclusive AI development that takes into account the languages and needs of diverse populations. By addressing this issue, we can work towards a more equitable distribution of AI benefits and opportunities worldwide."
cs.CL,CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in   Dynamic Environments for LLM Tool-Use Agents,"Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.",http://arxiv.org/abs/2511.02734v1,2025-11-04T16:58:29Z,"Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, Yi R. Fung","**Improving AI Agents' Ability to Plan and Adapt in a Changing World**

Imagine you're planning a trip and need to book flights, hotels, and transportation. You want to find the most cost-effective way to do it, but what if flight prices change or your preferred hotel is fully booked? Current AI agents, like those using Large Language Models (LLMs), are good at completing tasks, but they often don't consider the cost or adapt well to changing situations.

To address this limitation, researchers have created a new benchmark called CostBench. It's a tool to evaluate AI agents' ability to plan and adapt in a cost-effective way, especially in dynamic environments where things don't always go as planned. CostBench simulates real-world scenarios, such as travel planning, and tests agents' ability to adjust their plans in response to unexpected events, like tool failures or cost changes.

The results show that even the best AI models struggle to find the most cost-effective solutions, especially in dynamic situations. For example, GPT-5, a leading AI model, achieved less than 75% accuracy on the hardest tasks, and its performance dropped by around 40% when faced with dynamic conditions. This highlights the need for further research and development to create AI agents that are not only effective but also economically rational and robust.

The introduction of CostBench marks an important step towards creating more advanced AI agents that can plan, adapt, and make cost-effective decisions in a changing world."
cs.CL,PragExTra: A Multilingual Corpus of Pragmatic Explicitation in   Translation,"Translators often enrich texts with background details that make implicit cultural meanings explicit for new audiences. This phenomenon, known as pragmatic explicitation, has been widely discussed in translation theory but rarely modeled computationally. We introduce PragExTra, the first multilingual corpus and detection framework for pragmatic explicitation. The corpus covers eight language pairs from TED-Multi and Europarl and includes additions such as entity descriptions, measurement conversions, and translator remarks. We identify candidate explicitation cases through null alignments and refined using active learning with human annotation. Our results show that entity and system-level explicitations are most frequent, and that active learning improves classifier accuracy by 7-8 percentage points, achieving up to 0.88 accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic explicitation as a measurable, cross-linguistic phenomenon and takes a step towards building culturally aware machine translation. Keywords: translation, multilingualism, explicitation",http://arxiv.org/abs/2511.02721v1,2025-11-04T16:44:57Z,"Doreen Osmelak, Koel Dutta Chowdhury, Uliana Sentsova, Cristina EspaÃ±a-Bonet, Josef van Genabith","**Unlocking Cultural Context in Translation: Introducing PragExTra**

When translating texts from one language to another, translators often add details to help readers understand cultural references that might be unfamiliar to them. This process, called ""pragmatic explicitation,"" makes implicit meanings explicit, ensuring the translated text is clear and accessible to a new audience.

Researchers have now created a groundbreaking multilingual corpus, called PragExTra, to study and model this phenomenon computationally. The corpus covers eight language pairs and includes examples of added explanations, such as descriptions of entities, conversions of measurements, and translator notes.

The study found that the most common types of explicitation involve adding information about entities and systems. By using a technique called active learning, which involves human annotation and machine learning, the researchers achieved high accuracy in detecting explicitation cases, with accuracy rates of up to 0.88 and F1 scores of up to 0.82 across languages.

The creation of PragExTra marks a significant step towards developing more culturally aware machine translation systems. By better understanding how translators add context to texts, researchers can improve machine translation and make it more effective in conveying cultural nuances."
cs.CL,The Collaboration Gap,"The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a ""collaboration gap"": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a ""relay inference"" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.",http://arxiv.org/abs/2511.02687v1,2025-11-04T16:10:57Z,"Tim R. Davidson, Adam Fourney, Saleema Amershi, Robert West, Eric Horvitz, Ece Kamar","**The Collaboration Gap: A Challenge in AI Development**

As AI technology advances, it's becoming increasingly important for different AI systems to work together effectively. However, a new study reveals that even the best AI models struggle to collaborate with each other. Researchers created a benchmark to test the collaborative abilities of 32 leading AI models, pairing them in different combinations to solve maze problems.

The study found that AI models that perform well on their own often degrade significantly when required to collaborate. In some cases, smaller models that are good at solving mazes alone may fail almost completely when paired with another model. The researchers also discovered that letting the stronger model lead and then handing off to the weaker one can improve collaboration.

The study's findings have important implications for the development of AI systems. They highlight the need for:

1. **Collaboration-aware evaluation**: Testing AI models not just on their individual performance, but also on their ability to work with others.
2. **Training strategies to enhance collaboration**: Developing AI models that are specifically designed to work effectively with other models.
3. **Interaction design that brings out the best in AI models**: Designing systems that allow AI models to interact with each other in a way that leverages their strengths.

These insights are relevant not only to AI-AI collaboration but also to human-AI collaboration, and will be crucial for the development of effective and efficient AI systems in the future."
cs.CL,Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes,"Large language models (LLMs) are increasingly prevalent across diverse applications. However, their enormous size limits storage and processing capabilities to a few well-resourced stakeholders. As a result, most applications rely on pre-trained LLMs, fine-tuned for specific tasks. However, even storing the fine-tuned versions of these models remains a significant challenge due to the wide range of tasks they address. Recently, studies show that fine-tuning these models primarily affects a small fraction of parameters, highlighting the need for more efficient storage of fine-tuned models. This paper focuses on efficient storage of parameter updates in pre-trained models after fine-tuning. To address this challenge, we leverage the observation that fine-tuning updates are both low-rank and sparse, which can be utilized for storage efficiency. However, using only low-rank approximation or sparsification may discard critical singular components that enhance model expressivity. We first observe that given the same memory budget, sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks. Building on this, we propose our method, optimal singular damage, that selectively sparsifies low-rank approximated updates by leveraging the interleaved importance of singular vectors, ensuring that the most impactful components are retained. We demonstrate through extensive experiments that our proposed methods lead to significant storage efficiency and superior accuracy within the same memory budget compared to employing the low-rank approximation or sparsification individually.",http://arxiv.org/abs/2511.02681v1,2025-11-04T16:05:25Z,"Mohammadsajad Alipour, Mohammad Mohammadi Amiri","Here's a summary of the research paper ""Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes"" for a general audience:

**The Problem:** Large language models (LLMs) are powerful tools used in many applications, but they're huge and require a lot of storage space. This limits who can use them and how they're used.

**The Goal:** Researchers want to find a way to store these models more efficiently, especially when they're customized for specific tasks.

**The Breakthrough:** The researchers discovered that when LLMs are customized (or ""fine-tuned"") for a task, only a small part of the model's parameters change. They also found that these changes can be stored in a more compact way.

**The Solution:** The researchers developed a method called ""optimal singular damage"" that selectively stores the most important changes to the model, while discarding less important ones. This approach combines two techniques: reducing the complexity of the changes (using ""low-rank approximation"") and removing unnecessary data (using ""sparsification"").

**The Result:** The new method allows for significant storage efficiency while maintaining the model's accuracy. In other words, it enables more people to use customized LLMs, even with limited storage resources.

**In Simple Terms:** Imagine you have a huge library with millions of books, but you only need to store a few specific books. The researchers found a way to store just the changes to those books, rather than the entire library, making it much more efficient. This can help make powerful language models more accessible to more people."
cs.CL,"Understanding New-Knowledge-Induced Factual Hallucinations in LLMs:   Analysis, Solution, and Interpretation","Previous studies show that introducing new knowledge during large language models (LLMs) fine-tuning can lead to the generation of erroneous output when tested on known information, thereby triggering factual hallucinations. However, existing studies have not deeply investigated the specific manifestations and underlying mechanisms of these hallucinations. Our work addresses this gap by designing a controlled dataset Biography-Reasoning, and conducting a fine-grained analysis across multiple knowledge types and two task types, including knowledge question answering (QA) and knowledge reasoning tasks. We find that when fine-tuned on a dataset in which a specific knowledge type consists entirely of new knowledge, LLMs exhibit significantly increased hallucination tendencies. This suggests that the high unfamiliarity of a particular knowledge type, rather than the overall proportion of new knowledge, is a stronger driver of hallucinations, and these tendencies can even affect other knowledge types in QA tasks. To mitigate such factual hallucinations, we propose KnownPatch, which patches a small number of known knowledge samples in the later stages of training, effectively alleviating new-knowledge-induced hallucinations. Through attention analysis, we find that learning new knowledge reduces the model's attention to key entities in the question, thus causing excessive focus on the surrounding context, which may increase the risk of hallucination. Moreover, the attention pattern can propagate to similar contexts, facilitating the spread of hallucinations to textually similar questions. Our method effectively mitigates the disruption of new knowledge learning to the model's attention on key entities, accompanied by improved performance.",http://arxiv.org/abs/2511.02626v1,2025-11-04T14:55:24Z,"Renfei Dang, Peng Hu, Changjiang Gao, Shujian Huang","**The Dark Side of Learning New Things: How AI Models Can Get Facts Wrong**

Imagine you're teaching a student new information, but in the process, they start getting facts wrong that they previously knew. This phenomenon has been observed in large language models (LLMs), which are a type of artificial intelligence (AI) designed to understand and generate human-like text. Researchers have found that when LLMs learn new information, they can sometimes generate incorrect outputs, known as ""factual hallucinations,"" when tested on facts they already knew.

In a recent study, researchers investigated this issue by creating a special dataset called Biography-Reasoning. They found that when LLMs are fine-tuned on a dataset with entirely new information, they are more likely to generate incorrect outputs. This suggests that the unfamiliarity of new information, rather than the amount of new information, is a major driver of factual hallucinations.

The researchers also discovered that this problem can spread to similar questions and contexts, making it a significant challenge for LLMs. To address this issue, they proposed a solution called KnownPatch, which involves adding a small number of known facts to the training data. This approach was found to effectively reduce factual hallucinations and improve the model's performance.

The study also shed light on why this happens. When LLMs learn new information, they tend to focus less on key entities in the question and more on the surrounding context, which can lead to incorrect outputs. The researchers found that their method, KnownPatch, can help mitigate this issue by restoring the model's attention to key entities.

Overall, this study highlights the importance of carefully designing training data and methods to prevent factual hallucinations in LLMs. By understanding the causes of this problem and developing effective solutions, researchers can improve the accuracy and reliability of AI models."
cs.CL,The Realignment Problem: When Right becomes Wrong in LLMs,"The alignment of Large Language Models (LLMs) with human values is central to their safe deployment, yet current practice produces static, brittle, and costly-to-maintain models that fail to keep pace with evolving norms and policies. This misalignment, which we term the Alignment-Reality Gap, poses a growing challenge for reliable long-term use. Existing remedies are inadequate: large-scale re-annotation is economically prohibitive, and standard unlearning methods act as blunt instruments that erode utility rather than enable precise policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict Evaluation), a framework for principled unlearning that reconceives re-alignment as a programmatic policy application problem. TRACE programmatically triages existing preference data against a new policy, identifies high-impact conflicts via a alignment impact score, and applies a hybrid optimization that cleanly inverts, discards, or preserves preferences while safeguarding model performance. Empirical results show that TRACE achieves robust re-alignment across diverse model families (Qwen2.5-7B, Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF dataset under complex policy shift, TRACE enforces new principles without degrading general capabilities. Our work establishes a scalable, dynamic, and cost-effective paradigm for maintaining LLM alignment, providing a foundation for sustainable and responsible AI deployment.",http://arxiv.org/abs/2511.02623v1,2025-11-04T14:52:58Z,"Aakash Sen Sharma, Debdeep Sanyal, Vivek Srivastava, Shirish Karande, Murari Mandal","**The Challenge of Keeping AI Models Aligned with Human Values**

Large Language Models (LLMs) are powerful tools that can process and generate vast amounts of text. However, as societal norms and policies evolve, these models can become outdated and misaligned with human values. This misalignment can lead to models producing content that is no longer acceptable or safe.

**The Problem: Static Models Can't Keep Up**

Currently, updating LLMs to reflect changing values is a time-consuming and expensive process. Existing methods for updating models can be overly broad, causing them to lose their usefulness. This creates a significant challenge for the long-term reliability and safety of LLMs.

**A New Solution: TRACE**

Researchers have introduced a new framework called TRACE, which enables LLMs to be updated in a more precise and efficient way. TRACE works by:

1. Identifying conflicts between existing model data and new policies
2. Prioritizing high-impact conflicts
3. Applying targeted updates to the model

**Promising Results**

Tests of TRACE have shown promising results, with the framework successfully updating LLMs across different model families and datasets. Notably, TRACE was able to enforce new principles without degrading the model's general capabilities. This breakthrough has the potential to enable more sustainable and responsible AI deployment.

**Implications**

The development of TRACE marks an important step towards creating more dynamic and adaptable LLMs that can keep pace with evolving human values. By providing a scalable, cost-effective, and precise method for updating models, TRACE can help ensure that LLMs remain safe and reliable over time."
cs.CL,UniChange: Unifying Change Detection with Multimodal Large Language   Model,"Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.",http://arxiv.org/abs/2511.02607v1,2025-11-04T14:31:06Z,"Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li","**Breakthrough in Land Cover Change Detection: UniChange**

Imagine being able to track changes in the Earth's surface, such as deforestation, urbanization, or natural disasters, with unprecedented accuracy. A new research paper introduces UniChange, a revolutionary model that unifies change detection using a multimodal large language model.

**The Problem:**
Current change detection models are limited by their reliance on single-type data and can't effectively leverage diverse datasets. This leads to poor performance and limited versatility.

**The Solution:**
UniChange overcomes these limitations by integrating language capabilities with specialized change detection functionalities. This allows the model to learn from multiple datasets, including those with conflicting class definitions.

**Key Innovations:**

* UniChange uses three special tokens to identify changes between two images.
* The model utilizes text prompts to guide the identification of change categories, eliminating the need for predefined classification heads.

**Results:**
Experiments on four public benchmarks demonstrate state-of-the-art performance, surpassing all previous methods. UniChange achieved impressive IoU scores, ranging from 53.04 to 90.41, indicating a significant improvement in change detection accuracy.

**Impact:**
The UniChange model has the potential to transform the field of land cover change detection, enabling more accurate monitoring and analysis of environmental changes. The code is available open-source, allowing researchers and practitioners to build upon this innovation."
cs.CL,CGES: Confidence-Guided Early Stopping for Efficient and Accurate   Self-Consistency,"Large language models (LLMs) are often queried multiple times at test time, with predictions aggregated by majority vote. While effective, this self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls and can fail when the correct answer is rare. We introduce Confidence-Guided Early Stopping (CGES), a Bayesian framework that forms posteriors over candidate answers using scalar confidence signals derived from token probabilities or reward models. CGES adaptively halts sampling once the posterior mass of a candidate exceeds a threshold. We provide theoretical guarantees for both perfectly calibrated confidences and realistic noisy confidence signals. Across five reasoning benchmarks, CGES reduces the average number of model calls by about 69 percent (for example, from 16.0 to 4.9) while matching the accuracy of self-consistency within 0.06 percentage points.",http://arxiv.org/abs/2511.02603v1,2025-11-04T14:25:54Z,"Ehsan Aghazadeh, Ahmad Ghasemi, Hedyeh Beyhaghi, Hossein Pishro-Nik","**Improving Efficiency in AI Models with Confidence-Guided Early Stopping**

Large language models, like those used in chatbots and virtual assistants, often make predictions by querying the model multiple times and taking a majority vote. However, this approach can be inefficient and may not work well when the correct answer is not a popular choice.

Researchers have developed a new method called Confidence-Guided Early Stopping (CGES), which uses a Bayesian framework to make predictions more efficiently and accurately. CGES works by tracking the confidence of the model's predictions and stopping the querying process when the model is confident enough in its answer.

In tests across five reasoning benchmarks, CGES was able to reduce the number of model calls by about 69%, while still achieving similar accuracy to the traditional self-consistency method. This means that CGES can make predictions more quickly and efficiently, without sacrificing accuracy.

The benefits of CGES include:

* Reduced computational costs: By stopping the querying process early, CGES can save computing resources and reduce the time it takes to make predictions.
* Improved efficiency: CGES can make predictions more quickly, making it more suitable for real-time applications.
* Maintained accuracy: Despite reducing the number of model calls, CGES is able to maintain similar accuracy to traditional methods.

Overall, CGES has the potential to improve the efficiency and accuracy of large language models, making them more practical and effective in a wide range of applications."
cs.CL,Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations   to Decode Student Behaviour,"Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.",http://arxiv.org/abs/2511.02599v1,2025-11-04T14:20:56Z,"Max Norris, Kobi Gal, Sahan Bulathwela","Here's a summary of the research paper for a general audience:

**Improving AI in Education: A New Approach to Understanding Student Learning**

Researchers have made a breakthrough in using artificial intelligence (AI) to personalize learning for students. The challenge is to model how students learn and respond to questions in educational settings. Current AI models use data such as student responses and timestamps, but often ignore the actual text of the questions.

A team of researchers has developed a new approach called Next Token Knowledge Tracing (NTKT). This method uses large language models, like those used in chatbots, to analyze both the student's past responses and the text of the questions. By treating student learning as a ""next token prediction"" task, similar to how language models predict the next word in a sentence, NTKT can better understand student behavior and learning patterns.

The results are promising: NTKT outperforms current state-of-the-art models in predicting student responses and is more effective for new questions and students. This research highlights the importance of considering the content of questions in understanding student learning and demonstrates the potential of using pre-trained language models to improve AI in education. Ultimately, this could lead to more effective personalized learning tools for students."
cs.CL,The Analysis of Lexical Errors in Machine Translation from English into   Romanian,"The research explores error analysis in the performance of translating by Machine Translation from English into Romanian, and it focuses on lexical errors found in texts which include official information, provided by the World Health Organization (WHO), the Gavi Organization, by the patient information leaflet (the information about the active ingredients of the vaccines or the medication, the indications, the dosage instructions, the storage instructions, the side effects and warning, etc.). All of these texts are related to Covid-19 and have been translated by Google Translate, a multilingual Machine Translation that was created by Google. In the last decades, Google has actively worked to develop a more accurate and fluent automatic translation system. This research, specifically focused on improving Google Translate, aims to enhance the overall quality of Machine Translation by achieving better lexical selection and by reducing errors. The investigation involves a comprehensive analysis of 230 texts that have been translated from English into Romanian.",http://arxiv.org/abs/2511.02587v1,2025-11-04T14:07:21Z,Angela Stamatie,"Here's a summary of the research paper for a general audience:

**Improving Machine Translation: A Study on English to Romanian Translations**

Researchers analyzed the performance of Google Translate, a popular online translation tool, in translating texts from English to Romanian. Specifically, they looked at texts related to COVID-19 from reputable sources like the World Health Organization and vaccine information leaflets. The goal was to identify and understand lexical errors, which occur when the wrong words or phrases are used in a translation.

The researchers examined 230 translated texts and found that Google Translate sometimes struggled to accurately translate words and phrases from English to Romanian. This study aims to help improve the quality of machine translation by reducing errors and selecting more accurate words. The findings can be used to enhance Google Translate and other machine translation systems, ultimately making them more reliable and helpful for people who need to communicate across languages."
cs.CL,Smart-Hiring: An Explainable end-to-end Pipeline for CV Information   Extraction and Job Matching,"Hiring processes often involve the manual screening of hundreds of resumes for each job, a task that is time and effort consuming, error-prone, and subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural Language Processing (NLP) pipeline de- signed to automatically extract structured information from unstructured resumes and to semantically match candidates with job descriptions. The proposed system combines document parsing, named-entity recognition, and contextual text embedding techniques to capture skills, experience, and qualifications. Using advanced NLP technics, Smart-Hiring encodes both resumes and job descriptions in a shared vector space to compute similarity scores between candidates and job postings. The pipeline is modular and explainable, allowing users to inspect extracted entities and matching rationales. Experiments were conducted on a real-world dataset of resumes and job descriptions spanning multiple professional domains, demonstrating the robustness and feasibility of the proposed approach. The system achieves competitive matching accuracy while preserving a high degree of interpretability and transparency in its decision process. This work introduces a scalable and practical NLP frame- work for recruitment analytics and outlines promising directions for bias mitigation, fairness-aware modeling, and large-scale deployment of data-driven hiring solutions.",http://arxiv.org/abs/2511.02537v1,2025-11-04T12:44:54Z,"Kenza Khelkhal, Dihia Lanasri","**Introducing Smart-Hiring: A Game-Changer for Recruitment**

Hiring a new employee can be a tedious and time-consuming process, involving manually sifting through hundreds of resumes to find the best candidate. This process is not only laborious but also prone to errors and biases. To tackle this challenge, researchers have developed Smart-Hiring, an innovative pipeline that uses Natural Language Processing (NLP) to automatically extract relevant information from resumes and match candidates with job descriptions.

**How it works**

The Smart-Hiring pipeline consists of several modules that work together to analyze resumes and job postings. It uses advanced NLP techniques to:

1. Extract key information such as skills, experience, and qualifications from resumes.
2. Encode both resumes and job descriptions in a shared digital space to compute similarity scores between candidates and job postings.

**The benefits**

The Smart-Hiring system offers several benefits, including:

* **Efficiency**: Automating the resume screening process saves time and effort for hiring managers.
* **Accuracy**: The system reduces errors and biases associated with manual screening.
* **Transparency**: The pipeline is modular and explainable, allowing users to inspect extracted information and matching rationales.

**The results**

Experiments conducted on a real-world dataset of resumes and job descriptions demonstrated the effectiveness of Smart-Hiring, achieving competitive matching accuracy while maintaining transparency and interpretability.

**The future of recruitment**

The development of Smart-Hiring paves the way for more efficient, fair, and data-driven hiring practices. Future directions include exploring ways to mitigate biases, ensure fairness, and deploy large-scale recruitment analytics solutions. With Smart-Hiring, the recruitment process is poised to become more streamlined, accurate, and transparent."
stat.ML,ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free   Finetuning of Large Language Models,"Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy for finetuning large language models (LLMs) because it eliminates the memory overhead of backpropagation. However, it converges slowly due to the inherent curse of dimensionality when searching for descent directions in the high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a novel zeroth-order optimizer that accelerates convergence by adaptive directional sampling. Instead of drawing the direction uniformly at random, ConMeZO restricts the sampling to a cone centered around a momentum estimate. This concentrates the search in directions where the true gradient is more likely to lie and thus reduces the effect of high dimensions. We prove that ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically, when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than MeZO while retaining the low-memory footprint of zeroth-order methods.",http://arxiv.org/abs/2511.02757v1,2025-11-04T17:35:52Z,"Lejs Deen Behric, Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil","**Improving the Efficiency of Fine-Tuning Large Language Models**

Researchers have developed a new method called ConMeZO to fine-tune large language models (LLMs) more efficiently. Fine-tuning involves adjusting a pre-trained model's parameters to perform a specific task, but traditional methods require a lot of memory and computational power. ConMeZO uses a technique called zeroth-order optimization, which eliminates the need for backpropagation and reduces memory usage.

The challenge with zeroth-order optimization is that it can be slow, especially for large models with billions of parameters. ConMeZO addresses this issue by adaptively sampling descent directions, which helps to focus the search on more promising areas of the parameter space. This approach allows ConMeZO to converge faster than existing zeroth-order methods, with a speed improvement of up to 2X.

The benefits of ConMeZO are two-fold: it retains the low-memory footprint of zeroth-order methods, making it suitable for fine-tuning large LLMs, and it achieves faster convergence rates, making it a more efficient and practical solution for natural language processing tasks. Overall, ConMeZO has the potential to accelerate the development of more accurate and efficient language models."
stat.ML,Optimizing Kernel Discrepancies via Subset Selection,"Kernel discrepancies are a powerful tool for analyzing worst-case errors in quasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such discrepancy measures, we extend the subset selection problem to the setting of kernel discrepancies, selecting an m-element subset from a large population of size $n \gg m$. We introduce a novel subset selection algorithm applicable to general kernel discrepancies to efficiently generate low-discrepancy samples from both the uniform distribution on the unit hypercube, the traditional setting of classical QMC, and from more general distributions $F$ with known density functions by employing the kernel Stein discrepancy. We also explore the relationship between the classical $L_2$ star discrepancy and its $L_\infty$ counterpart.",http://arxiv.org/abs/2511.02706v1,2025-11-04T16:25:08Z,"Deyao Chen, FranÃ§ois ClÃ©ment, Carola Doerr, Nathan Kirk","**Improving Random Sampling with Math**

Researchers have made a breakthrough in optimizing a mathematical tool called kernel discrepancies, which helps evaluate the accuracy of quasi-Monte Carlo (QMC) methods. These methods are used to make predictions and estimates in various fields, such as finance, engineering, and computer science.

The study focuses on selecting a smaller, representative subset from a large dataset, which is a common challenge in data analysis. The researchers developed a new algorithm that efficiently chooses a subset of data points that are ""well-spread"" and representative of the entire dataset. This is particularly useful when working with large datasets and trying to make accurate predictions.

The algorithm works with a variety of probability distributions, including the uniform distribution (like flipping a coin) and more complex distributions. The researchers also explored the relationship between two different measures of discrepancy, which helps to better understand how to evaluate the accuracy of QMC methods.

**In simple terms:** Imagine you have a huge dataset and want to select a smaller group of data points that still accurately represent the whole dataset. This study developed a new method to do just that, which can improve the accuracy of predictions and estimates in various fields."
stat.ML,Learning CNF formulas from uniform random solutions in the local lemma   regime,"We study the problem of learning a $n$-variables $k$-CNF formula $\Phi$ from its i.i.d. uniform random solutions, which is equivalent to learning a Boolean Markov random field (MRF) with $k$-wise hard constraints. Revisiting Valiant's algorithm (Commun. ACM'84), we show that it can exactly learn (1) $k$-CNFs with bounded clause intersection size under Lov\'asz local lemma type conditions, from $O(\log n)$ samples; and (2) random $k$-CNFs near the satisfiability threshold, from $\widetilde{O}(n^{\exp(-\sqrt{k})})$ samples. These results significantly improve the previous $O(n^k)$ sample complexity. We further establish new information-theoretic lower bounds on sample complexity for both exact and approximate learning from i.i.d. uniform random solutions.",http://arxiv.org/abs/2511.02487v1,2025-11-04T11:22:05Z,"Weiming Feng, Xiongxin Yang, Yixiao Yu, Yiyao Zhang","**Unlocking the Secrets of Complex Formulas**

Imagine you have a complex puzzle with many pieces that must fit together in a specific way. A ""CNF formula"" is like a blueprint for solving such puzzles. It consists of many conditions (or ""clauses"") that the puzzle pieces must satisfy. But what if you only have examples of solved puzzles and not the blueprint itself? Can you figure out the blueprint from these examples?

Researchers have been working on this problem, and their new findings offer exciting insights. They have developed a way to learn the blueprint (or CNF formula) from a relatively small number of solved puzzle examples. Specifically, they showed that:

* For certain types of puzzles, you only need about 10-20 examples to accurately learn the blueprint (with a small number of conditions).
* For more complex puzzles, you need a larger number of examples, but still much fewer than previously thought (about 1,000 to 100,000 examples, depending on the puzzle size).

These results are significant because they improve upon previous methods that required an impractically large number of examples (about 1 billion times more). The researchers also established fundamental limits on how many examples are needed to learn the blueprint accurately.

These findings have implications for various fields, such as artificial intelligence, computer science, and statistics, where understanding complex relationships between variables is crucial. By learning from examples, we can unlock the secrets of complex formulas and improve our ability to model and solve real-world problems."
stat.ML,An Adaptive Sampling Framework for Detecting Localized Concept Drift   under Label Scarcity,"Concept drift and label scarcity are two critical challenges limiting the robustness of predictive models in dynamic industrial environments. Existing drift detection methods often assume global shifts and rely on dense supervision, making them ill-suited for regression tasks with local drifts and limited labels. This paper proposes an adaptive sampling framework that combines residual-based exploration and exploitation with EWMA monitoring to efficiently detect local concept drift under labeling budget constraints. Empirical results on synthetic benchmarks and a case study on electricity market demonstrate superior performance in label efficiency and drift detection accuracy.",http://arxiv.org/abs/2511.02452v1,2025-11-04T10:30:20Z,"Junghee Pyeon, Davide Cacciarelli, Kamran Paynabar","**Detecting Changes in Data Patterns with Limited Labels**

In today's fast-paced industrial world, machine learning models need to adapt quickly to changing data patterns to make accurate predictions. However, two major challenges hinder their performance: **concept drift** (when the underlying patterns in the data change over time) and **label scarcity** (when there are limited labeled examples to train the models).

Researchers have proposed a new framework that helps detect these changes in data patterns, even when there are limited labels available. This framework uses a smart sampling approach that balances exploring new data and exploiting what we already know. The results show that this approach outperforms existing methods in detecting local changes in data patterns while using fewer labels. This is particularly useful in applications like predicting electricity market trends, where data patterns can shift rapidly and labels are hard to come by."
stat.ML,"Efficient Solvers for SLOPE in R, Python, Julia, and C++","We present a suite of packages in R, Python, Julia, and C++ that efficiently solve the Sorted L-One Penalized Estimation (SLOPE) problem. The packages feature a highly efficient hybrid coordinate descent algorithm that fits generalized linear models (GLMs) and supports a variety of loss functions, including Gaussian, binomial, Poisson, and multinomial logistic regression. Our implementation is designed to be fast, memory-efficient, and flexible. The packages support a variety of data structures (dense, sparse, and out-of-memory matrices) and are designed to efficiently fit the full SLOPE path as well as handle cross-validation of SLOPE models, including the relaxed SLOPE. We present examples of how to use the packages and benchmarks that demonstrate the performance of the packages on both real and simulated data and show that our packages outperform existing implementations of SLOPE in terms of speed.",http://arxiv.org/abs/2511.02430v1,2025-11-04T10:03:15Z,"Johan Larsson, Malgorzata Bogdan, Krystyna Grzesiak, Mathurin Massias, Jonas Wallin","**Unlocking Efficient Data Analysis: A Breakthrough in SLOPE Solvers**

Imagine being able to analyze complex data sets quickly and accurately, without sacrificing performance. A team of researchers has made this a reality with the development of a suite of packages in R, Python, Julia, and C++ that efficiently solve the Sorted L-One Penalized Estimation (SLOPE) problem.

**What is SLOPE?**

SLOPE is a statistical technique used to analyze data and identify patterns. It's particularly useful for handling large data sets with many variables. However, traditional SLOPE solvers can be slow and memory-intensive, making them impractical for big data applications.

**The Breakthrough**

The researchers have created a new algorithm that solves the SLOPE problem much faster and more efficiently than existing methods. Their implementation supports various data structures, including dense, sparse, and out-of-memory matrices, making it versatile and adaptable to different data types.

**Key Benefits**

* **Speed**: The new packages outperform existing SLOPE implementations in terms of speed, making them ideal for large-scale data analysis.
* **Memory Efficiency**: The packages are designed to be memory-efficient, reducing the risk of data overload and making them suitable for big data applications.
* **Flexibility**: The packages support a range of loss functions, including Gaussian, binomial, Poisson, and multinomial logistic regression, making them suitable for various data analysis tasks.

**Real-World Impact**

The researchers have demonstrated the performance of their packages on both real and simulated data, showcasing their potential to accelerate data analysis in various fields, such as medicine, finance, and social sciences. With these efficient SLOPE solvers, data analysts and researchers can now focus on extracting insights from their data, rather than waiting for computations to complete."
stat.ML,Wasserstein Convergence of Critically Damped Langevin Diffusions,"Score-based Generative Models (SGMs) have achieved impressive performance in data generation across a wide range of applications and benefit from strong theoretical guarantees. Recently, methods inspired by statistical mechanics, in particular, Hamiltonian dynamics, have introduced Critically-damped Langevin Diffusions (CLDs), which define diffusion processes on extended spaces by coupling the data with auxiliary variables. These approaches, along with their associated score-matching and sampling procedures, have been shown to outperform standard diffusion-based samplers numerically. In this paper, we analyze a generalized dynamic that extends classical CLDs by introducing an additional hyperparameter controlling the noise applied to the data coordinate, thereby better exploiting the extended space. We further derive a novel upper bound on the sampling error of CLD-based generative models in the Wasserstein metric. This additional hyperparameter influences the smoothness of sample paths, and our discretization error analysis provides practical guidance for its tuning, leading to improved sampling performance.",http://arxiv.org/abs/2511.02419v1,2025-11-04T09:49:07Z,"Stanislas Strasman, Sobihan Surendran, Claire Boyer, Sylvain Le Corff, Vincent Lemaire, Antonio Ocello","Here's a summary of the research paper for a general audience:

**Improving Artificial Intelligence Models with Better Math**

Researchers have made progress in developing artificial intelligence (AI) models that can generate new data, such as images or text, that is similar to existing data. These models, called Score-based Generative Models, have shown impressive results and have strong mathematical guarantees.

The researchers in this study were interested in improving these models by using ideas from physics, specifically the behavior of particles in motion. They developed a new approach called Critically-damped Langevin Diffusions, which uses extra variables to help the model learn more effectively.

The researchers found that by adding a new ""hyperparameter"" - a adjustable setting that helps the model learn - they could improve the performance of their model. They also developed a new way to measure how well the model is working, which helps them understand how to adjust the hyperparameter for better results.

In simple terms, the researchers used math and physics-inspired ideas to create a more effective AI model that can generate new data. Their work provides guidance on how to fine-tune the model for better performance, which could lead to improvements in areas such as image and text generation."
stat.ML,A new class of Markov random fields enabling lightweight sampling,"This work addresses the problem of efficient sampling of Markov random fields (MRF). The sampling of Potts or Ising MRF is most often based on Gibbs sampling, and is thus computationally expensive. We consider in this work how to circumvent this bottleneck through a link with Gaussian Markov Random fields. The latter can be sampled in several cost-effective ways, and we introduce a mapping from real-valued GMRF to discrete-valued MRF. The resulting new class of MRF benefits from a few theoretical properties that validate the new model. Numerical results show the drastic performance gain in terms of computational efficiency, as we sample at least 35x faster than Gibbs sampling using at least 37x less energy, all the while exhibiting empirical properties close to classical MRFs.",http://arxiv.org/abs/2511.02373v1,2025-11-04T08:53:17Z,"Jean-Baptiste Courbot, Hugo Gangloff, Bruno Colicchio","Here's a summary of the research paper for a general audience:

**Breakthrough in Efficient Sampling of Complex Systems**

Researchers have made a significant advancement in the field of statistics and machine learning, enabling the efficient sampling of complex systems known as Markov random fields (MRFs). MRFs are mathematical models used to describe the relationships between variables in a system, but sampling from them can be computationally expensive.

The researchers have discovered a way to link MRFs to another type of model, called Gaussian Markov Random fields (GMRFs), which can be sampled more efficiently. By creating a mapping between the two, they have developed a new class of MRFs that can be sampled much faster than traditional methods.

The results show that this new approach is at least 35 times faster and uses 37 times less energy than traditional Gibbs sampling methods. Despite the significant speedup, the new approach still exhibits similar properties to classical MRFs, making it a reliable and efficient solution. This breakthrough has the potential to accelerate research and applications in various fields, such as computer vision, image processing, and machine learning."
stat.ML,Reducing normalizing flow complexity for MCMC preconditioning,"Preconditioning is a key component of MCMC algorithms that improves sampling efficiency by facilitating exploration of geometrically complex target distributions through an invertible map. While linear preconditioners are often sufficient for moderately complex target distributions, recent work has explored nonlinear preconditioning with invertible neural networks as components of normalizing flows (NFs). However, empirical and theoretical studies show that overparameterized NF preconditioners can degrade sampling efficiency and fit quality. Moreover, existing NF-based approaches do not adapt their architectures to the target distribution. Related work outside of MCMC similarly finds that suitably parameterized NFs can achieve comparable or superior performance with substantially less training time or data. We propose a factorized preconditioning architecture that reduces NF complexity by combining a linear component with a conditional NF, improving adaptability to target geometry. The linear preconditioner is applied to dimensions that are approximately Gaussian, as estimated from warmup samples, while the conditional NF models more complex dimensions. Our method yields significantly better tail samples on two complex synthetic distributions and consistently better performance on a sparse logistic regression posterior across varying likelihood and prior strengths. It also achieves higher effective sample sizes on hierarchical Bayesian model posteriors with weak likelihoods and strong funnel geometries. This approach is particularly relevant for hierarchical Bayesian model analyses with limited data and could inform current theoretical and software strides in neural MCMC design.",http://arxiv.org/abs/2511.02345v1,2025-11-04T08:08:00Z,"David Nabergoj, Erik Å trumbelj","**Improving MCMC Algorithms with Smarter Preconditioning**

Markov Chain Monte Carlo (MCMC) algorithms are statistical tools used to analyze complex data. However, they can struggle with certain types of data, leading to inefficient results. One way to improve MCMC algorithms is through preconditioning, which uses a mathematical transformation to make the data easier to analyze.

Recently, researchers have started using neural networks to create more flexible preconditioning transformations. However, these neural networks can become too complex and actually decrease the efficiency of the algorithm.

A new approach, proposed in this research paper, simplifies the neural network by combining a simple linear transformation with a more flexible, but less complex, neural network component. This hybrid approach adapts to the specific characteristics of the data, leading to more efficient and accurate results.

The researchers tested their method on several complex data sets and found that it outperformed existing approaches. Specifically, their method:

* Produced more accurate results for rare events (tail samples) in complex synthetic data sets
* Consistently performed better on a statistical model of sparse logistic regression data
* Provided more reliable results for hierarchical Bayesian models with weak data and complex geometries

This research has important implications for analyzing complex data, particularly in situations where data is limited. The proposed approach can lead to more efficient and accurate results, making it a valuable tool for researchers and practitioners."
stat.ML,A Stable Lasso,"The Lasso has been widely used as a method for variable selection, valued for its simplicity and empirical performance. However, Lasso's selection stability deteriorates in the presence of correlated predictors. Several approaches have been developed to mitigate this limitation. In this paper, we provide a brief review of existing approaches, highlighting their limitations. We then propose a simple technique to improve the selection stability of Lasso by integrating a weighting scheme into the Lasso penalty function, where the weights are defined as an increasing function of a correlation-adjusted ranking that reflects the predictive power of predictors. Empirical evaluations on both simulated and real-world datasets demonstrate the efficacy of the proposed method. Additional numerical results demonstrate the effectiveness of the proposed approach in stabilizing other regularization-based selection methods, indicating its potential as a general-purpose solution.",http://arxiv.org/abs/2511.02306v1,2025-11-04T06:41:47Z,"Mahdi Nouraie, Houying Zhu, Samuel Muller","**Improving the Reliability of a Popular Statistical Tool**

The Lasso, a widely used statistical method, helps identify the most important factors in a complex system. However, its performance can be compromised when the factors are highly correlated, leading to unstable and unreliable results. Researchers have proposed various solutions to address this limitation, but they often have their own drawbacks.

In a new study, researchers propose a simple yet effective technique to improve the Lasso's performance. By introducing a weighting scheme that takes into account the correlation between factors, their method enhances the stability of the Lasso in selecting the most relevant factors. The approach was tested on both simulated and real-world data, and the results demonstrate its effectiveness.

The proposed method not only improves the Lasso but also has the potential to stabilize other statistical methods that rely on regularization, making it a valuable tool for a wide range of applications. This innovation has the potential to increase the reliability of statistical analyses and lead to more accurate conclusions in various fields."
stat.ML,Probabilistic Graph Cuts,"Probabilistic relaxations of graph cuts offer a differentiable alternative to spectral clustering, enabling end-to-end and online learning without eigendecompositions, yet prior work centered on RatioCut and lacked general guarantees and principled gradients. We present a unified probabilistic framework that covers a wide class of cuts, including Normalized Cut. Our framework provides tight analytic upper bounds on expected discrete cuts via integral representations and Gauss hypergeometric functions with closed-form forward and backward. Together, these results deliver a rigorous, numerically stable foundation for scalable, differentiable graph partitioning covering a wide range of clustering and contrastive learning objectives.",http://arxiv.org/abs/2511.02272v2,2025-11-04T05:24:56Z,Ayoub Ghriss,"**Unlocking Efficient and Accurate Graph Clustering**

Imagine you have a large network of interconnected points, like a social media platform or a biological system. To understand the structure of this network, you need to group similar points together, a process known as clustering. Researchers have developed a technique called graph cuts to achieve this, but traditional methods had limitations.

A new study introduces a unified framework for probabilistic graph cuts, which provides a more efficient and accurate way to cluster complex networks. This approach allows for:

* **Faster and more flexible learning**: Unlike traditional methods, this new framework enables learning and clustering in real-time, without requiring complex calculations.
* **Improved accuracy**: The framework provides a rigorous foundation for graph partitioning, ensuring that the results are reliable and consistent.
* **Wider range of applications**: This unified framework covers a broad range of clustering objectives, making it a versatile tool for various fields, from social network analysis to biology and machine learning.

The study's findings have significant implications for large-scale network analysis, enabling researchers to tackle complex problems with greater ease and accuracy. By providing a more efficient and reliable way to cluster networks, this new framework has the potential to drive breakthroughs in various fields."
stat.ML,Limit Theorems for Stochastic Gradient Descent in High-Dimensional   Single-Layer Networks,"This paper studies the high-dimensional scaling limits of online stochastic gradient descent (SGD) for single-layer networks. Building on the seminal work of Saad and Solla, which analyzed the deterministic (ballistic) scaling limits of SGD corresponding to the gradient flow of the population loss, we focus on the critical scaling regime of the step size. Below this critical scale, the effective dynamics are governed by ballistic (ODE) limits, but at the critical scale, new correction term appears that changes the phase diagram. In this regime, near the fixed points, the corresponding diffusive (SDE) limits of the effective dynamics reduces to an Ornstein-Uhlenbeck process under certain conditions. These results highlight how the information exponent controls sample complexity and illustrates the limitations of deterministic scaling limit in capturing the stochastic fluctuations of high-dimensional learning dynamics.",http://arxiv.org/abs/2511.02258v1,2025-11-04T04:52:19Z,Parsa Rangriz,"**Unlocking the Secrets of Artificial Intelligence: A Breakthrough in Stochastic Gradient Descent**

Imagine you're trying to teach a computer to recognize pictures of cats and dogs. You show it many pictures, and it learns to identify them. But have you ever wondered how it learns so quickly? A team of researchers has made a significant discovery about a key algorithm used in artificial intelligence, called stochastic gradient descent (SGD).

SGD is like a game where the computer tries to guess the best way to recognize pictures by making small adjustments as it goes along. The researchers studied what happens to this game when the number of pictures (or ""data points"") is very large. They found that when the computer makes small adjustments quickly, its learning process behaves like a random walk. But when it makes adjustments at a certain ""critical"" speed, something new happens.

At this critical speed, the computer's learning process becomes more stable and predictable. The researchers described this process using a mathematical model, called an Ornstein-Uhlenbeck process. This model helps us understand how the computer learns and makes predictions.

The study's findings have important implications for artificial intelligence. They show that the way we analyze and understand AI algorithms needs to take into account the random fluctuations that occur during learning. This can help us design more efficient and effective AI systems.

**In simple terms:**

* Researchers studied a key algorithm in AI called stochastic gradient descent (SGD).
* They found that SGD's learning process changes when the number of data points is large.
* At a certain ""critical"" speed, the learning process becomes more stable and predictable.
* The study's findings can help us design better AI systems."
stat.ML,DoFlow: Causal Generative Flows for Interventional and Counterfactual   Time-Series Prediction,"Time-series forecasting increasingly demands not only accurate observational predictions but also causal forecasting under interventional and counterfactual queries in multivariate systems. We present DoFlow, a flow based generative model defined over a causal DAG that delivers coherent observational and interventional predictions, as well as counterfactuals through the natural encoding and decoding mechanism of continuous normalizing flows (CNFs). We also provide a supporting counterfactual recovery result under certain assumptions. Beyond forecasting, DoFlow provides explicit likelihoods of future trajectories, enabling principled anomaly detection. Experiments on synthetic datasets with various causal DAG and real world hydropower and cancer treatment time series show that DoFlow achieves accurate system-wide observational forecasting, enables causal forecasting over interventional and counterfactual queries, and effectively detects anomalies. This work contributes to the broader goal of unifying causal reasoning and generative modeling for complex dynamical systems.",http://arxiv.org/abs/2511.02137v1,2025-11-04T00:01:25Z,"Dongze Wu, Feng Qiu, Yao Xie","**Predicting the Future with Cause and Effect**

Imagine being able to predict not just what will happen next, but also what would happen if you intervened in a complex system. For example, how would a change in water flow affect a hydroelectric power plant, or how would a new cancer treatment affect a patient's health over time?

Researchers have developed a new artificial intelligence model called DoFlow, which can make these kinds of predictions. DoFlow uses a type of machine learning called generative flows to model complex systems, like power grids or biological systems, and predict how they will behave over time.

What makes DoFlow unique is that it takes into account the causal relationships between different parts of the system. In other words, it understands how different variables affect each other. This allows it to make predictions not just about what will happen next, but also about what would happen if you changed something in the system.

The researchers tested DoFlow on synthetic datasets and real-world data from hydroelectric power plants and cancer treatment. They found that DoFlow was able to make accurate predictions about future events, detect anomalies, and even make predictions about what would happen under hypothetical scenarios.

This work has the potential to revolutionize fields like energy management, healthcare, and finance, where understanding complex systems and making predictions about future events is crucial. By combining causal reasoning and generative modeling, DoFlow provides a powerful tool for making sense of complex systems and making informed decisions."
stat.ML,Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits,"Variance-dependent regret bounds have received increasing attention in recent studies on contextual bandits. However, most of these studies are focused on upper confidence bound (UCB)-based bandit algorithms, while sampling based bandit algorithms such as Thompson sampling are still understudied. The only exception is the LinVDTS algorithm (Xu et al., 2023), which is limited to linear reward function and its regret bound is not optimal with respect to the model dimension. In this paper, we present FGTSVA, a variance-aware Thompson Sampling algorithm for contextual bandits with general reward function with optimal regret bound. At the core of our analysis is an extension of the decoupling coefficient, a technique commonly used in the analysis of Feel-good Thompson sampling (FGTS) that reflects the complexity of the model space. With the new decoupling coefficient denoted by $\mathrm{dc}$, FGTS-VA achieves the regret of $\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$, where $|\mathcal{F}|$ is the size of the model space, $T$ is the total number of rounds, and $\sigma_t^2$ is the subgaussian norm of the noise (e.g., variance when the noise is Gaussian) at round $t$. In the setting of contextual linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms using weighted linear regression (Zhou and Gu, 2022).",http://arxiv.org/abs/2511.02123v1,2025-11-03T23:25:41Z,"Xuheng Li, Quanquan Gu","**Improving Decision-Making with Variance-Aware Thompson Sampling**

Imagine you're trying to choose the best option from a set of possibilities, but you're not sure which one will work best. This is a common problem in many fields, such as medicine, finance, and marketing. Researchers have developed algorithms to help make better decisions in these situations, known as ""contextual bandits.""

One popular approach is called Thompson sampling, which works by randomly sampling possible solutions and choosing the best one based on those samples. However, most existing Thompson sampling algorithms don't take into account the uncertainty or ""variance"" of the outcomes.

A new algorithm, called FGTS-VA, addresses this limitation by incorporating variance-aware techniques into Thompson sampling. This allows FGTS-VA to make more informed decisions by considering not only the expected outcome but also the uncertainty of that outcome.

The researchers behind FGTS-VA showed that their algorithm achieves optimal performance, matching the best existing algorithms in certain situations. Specifically, FGTS-VA achieved a regret bound of $\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$, which takes into account the complexity of the model space, the total number of rounds, and the uncertainty of the outcomes.

The key benefits of FGTS-VA are:

* **Improved performance**: FGTS-VA achieves optimal regret bounds, making it a competitive algorithm for contextual bandit problems.
* **Flexibility**: FGTS-VA can handle general reward functions, making it applicable to a wide range of situations.
* **Variance-awareness**: By considering the uncertainty of outcomes, FGTS-VA can make more informed decisions.

In simple terms, FGTS-VA is a new algorithm that helps make better decisions in complex situations by considering both the expected outcome and the uncertainty of that outcome. Its performance is competitive with the best existing algorithms, and it has the potential to be applied in a wide range of fields."
stat.ML,Enhancing Phenotype Discovery in Electronic Health Records through Prior   Knowledge-Guided Unsupervised Learning,"Objectives: Unsupervised learning with electronic health record (EHR) data has shown promise for phenotype discovery, but approaches typically disregard existing clinical information, limiting interpretability. We operationalize a Bayesian latent class framework for phenotyping that incorporates domain-specific knowledge to improve clinical meaningfulness of EHR-derived phenotypes and illustrate its utility by identifying an asthma sub-phenotype informed by features of Type 2 (T2) inflammation.   Materials and methods: We illustrate a framework for incorporating clinical knowledge into a Bayesian latent class model via informative priors to guide unsupervised clustering toward clinically relevant subgroups. This approach models missingness, accounting for potential missing-not-at-random patterns, and provides patient-level probabilities for phenotype assignment with uncertainty. Using reusable and flexible code, we applied the model to a large asthma EHR cohort, specifying informative priors for T2 inflammation-related features and weakly informative priors for other clinical variables, allowing the data to inform posterior distributions.   Results and Conclusion: Using encounter data from January 2017 to February 2024 for 44,642 adult asthma patients, we found a bimodal posterior distribution of phenotype assignment, indicating clear class separation. The T2 inflammation-informed class (38.7%) was characterized by elevated eosinophil levels and allergy markers, plus high healthcare utilization and medication use, despite weakly informative priors on the latter variables. These patterns suggest an ""uncontrolled T2-high"" sub-phenotype. This demonstrates how our Bayesian latent class modeling approach supports hypothesis generation and cohort identification in EHR-based studies of heterogeneous diseases without well-established phenotype definitions.",http://arxiv.org/abs/2511.02102v1,2025-11-03T22:25:07Z,"Melanie Mayer, Kimberly Lactaoen, Gary E. Weissman, Blanca E. Himes, Rebecca A. Hubbard","**Unlocking Hidden Patterns in Health Records to Improve Patient Care**

Researchers have developed a new method to analyze electronic health records (EHRs) and uncover hidden patterns in patient data. This approach uses a type of artificial intelligence called unsupervised learning, which helps identify groups of patients with similar characteristics. However, unlike previous methods, this new approach incorporates existing medical knowledge to make the discovered patterns more meaningful and clinically relevant.

The researchers applied their method to a large dataset of patients with asthma, a condition that can manifest differently in different people. By incorporating prior knowledge about a specific type of inflammation (Type 2 or T2 inflammation), they were able to identify a distinct subgroup of patients with asthma who had elevated levels of certain markers, such as eosinophils, and allergy symptoms. This subgroup, dubbed the ""uncontrolled T2-high"" sub-phenotype, was also characterized by high healthcare utilization and medication use.

The study demonstrates the potential of this approach to enhance the discovery of clinically meaningful phenotypes from EHR data, which can ultimately lead to better patient care and more targeted treatments. By leveraging existing medical knowledge and accounting for uncertainty in the data, this method can help researchers and clinicians identify specific patient subgroups and develop more effective treatment strategies."
stat.ML,Data-driven Learning of Interaction Laws in Multispecies Particle   Systems with Gaussian Processes: Convergence Theory and Applications,"We develop a Gaussian process framework for learning interaction kernels in multi-species interacting particle systems from trajectory data. Such systems provide a canonical setting for multiscale modeling, where simple microscopic interaction rules generate complex macroscopic behaviors. While our earlier work established a Gaussian process approach and convergence theory for single-species systems, and later extended to second-order models with alignment and energy-type interactions, the multi-species setting introduces new challenges: heterogeneous populations interact both within and across species, the number of unknown kernels grows, and asymmetric interactions such as predator-prey dynamics must be accommodated. We formulate the learning problem in a nonparametric Bayesian setting and establish rigorous statistical guarantees. Our analysis shows recoverability of the interaction kernels, provides quantitative error bounds, and proves statistical optimality of posterior estimators, thereby unifying and generalizing previous single-species theory. Numerical experiments confirm the theoretical predictions and demonstrate the effectiveness of the proposed approach, highlighting its advantages over existing kernel-based methods. This work contributes a complete statistical framework for data-driven inference of interaction laws in multi-species systems, advancing the broader multiscale modeling program of connecting microscopic particle dynamics with emergent macroscopic behavior.",http://arxiv.org/abs/2511.02053v1,2025-11-03T20:38:38Z,"Jinchao Feng, Charles Kulick, Sui Tang","**Unlocking the Secrets of Complex Systems: A New Approach to Understanding Interactions**

Imagine a flock of birds flying together, a school of fish swimming in unison, or a group of animals interacting in a complex ecosystem. These systems are made up of many individual parts that interact with each other in complex ways, giving rise to fascinating patterns and behaviors. But what if we could learn the underlying rules that govern these interactions, and use that knowledge to better understand and predict the behavior of these systems?

Researchers have developed a new approach to doing just that, using a type of machine learning called Gaussian processes. This approach allows scientists to learn the interaction rules between different species in a system, using data from observations or simulations. The researchers have shown that their approach is not only effective, but also statistically rigorous, meaning that it provides a high degree of accuracy and reliability.

The breakthrough comes from the ability to handle complex systems with many different species interacting with each other, including systems where some species may prey on others. The researchers have developed a framework that can learn the interaction rules between different species, and have shown that it works well in practice.

This work has important implications for fields such as biology, ecology, and physics, where understanding complex systems is crucial. By providing a new tool for learning the underlying rules of complex systems, researchers can gain a deeper understanding of how these systems work, and make more accurate predictions about their behavior. Ultimately, this could lead to better management of ecosystems, more effective conservation strategies, and a deeper understanding of the natural world."
stat.ML,Bridging Lifelong and Multi-Task Representation Learning via Algorithm   and Complexity Measure,"In lifelong learning, a learner faces a sequence of tasks with shared structure and aims to identify and leverage it to accelerate learning. We study the setting where such structure is captured by a common representation of data. Unlike multi-task learning or learning-to-learn, where tasks are available upfront to learn the representation, lifelong learning requires the learner to make use of its existing knowledge while continually gathering partial information in an online fashion. In this paper, we consider a generalized framework of lifelong representation learning. We propose a simple algorithm that uses multi-task empirical risk minimization as a subroutine and establish a sample complexity bound based on a new notion we introduce--the task-eluder dimension. Our result applies to a wide range of learning problems involving general function classes. As concrete examples, we instantiate our result on classification and regression tasks under noise.",http://arxiv.org/abs/2511.01847v1,2025-11-03T18:54:56Z,"Zhi Wang, Chicheng Zhang, Ramya Korlakai Vinayak","Here's a summary of the research paper for a general audience:

**Lifelong Learning: A New Approach to Machine Learning**

Imagine you're trying to learn a new skill, like playing a musical instrument. At first, it's hard, but as you practice, you start to recognize patterns and get better. Now, imagine you're trying to learn multiple instruments over time. A new approach to machine learning, called lifelong learning, aims to help computers learn in a similar way.

In lifelong learning, a computer faces a series of tasks, one after another, and tries to use what it learned from previous tasks to learn new ones more quickly. For example, if a computer is learning to recognize pictures of cats and dogs, and then later it's asked to recognize pictures of birds and fish, it can use what it learned from the first task to help with the second.

The researchers in this study developed a new algorithm that helps computers learn in this way. Their algorithm uses a technique called multi-task learning, which involves learning to do several tasks at once. They also introduced a new measure, called the task-eluder dimension, which helps them understand how well their algorithm works.

The good news is that this approach can be applied to a wide range of machine learning problems, including classification (like recognizing pictures) and regression (like predicting continuous values). The researchers showed that their algorithm works well even when there's noise or uncertainty in the data.

Overall, this study takes a step forward in developing more efficient and effective machine learning algorithms that can learn over time, much like humans do."
stat.ML,Fractional Diffusion Bridge Models,"We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr\""{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr\'echet Inception Distance (FID) in unpaired image translation.",http://arxiv.org/abs/2511.01795v1,2025-11-03T17:51:10Z,"Gabriel Nobis, Maximilian Springenberg, Arina Belova, Rembert Daems, Christoph Knochenhauer, Manfred Opper, Tolga Birdal, Wojciech Samek","**Introducing Fractional Diffusion Bridge Models: A New Approach to Understanding Complex Systems**

Researchers have developed a novel framework called Fractional Diffusion Bridge Models (FDBM) to better understand and predict complex systems that exhibit unique behaviors, such as memory effects, long-range dependencies, and anomalous diffusion. These phenomena are common in many fields, including biology and image processing, but are not well-captured by traditional modeling approaches.

**The Problem with Traditional Models**

Traditional models rely on Brownian motion (BM), which assumes that the movement of particles or systems is random and memoryless. However, real-world systems often exhibit complex behaviors that cannot be explained by BM. For example, in biology, protein conformations can change over time in a way that depends on their past behavior.

**A New Approach: Fractional Diffusion Bridge Models**

FDBM uses a more realistic and flexible approach, called fractional Brownian motion (fBM), which can capture these complex behaviors. However, fBM is challenging to work with because it is non-Markovian, meaning that its future behavior depends on its entire past history. To overcome this challenge, the researchers developed a Markovian approximation of fBM, which allows for efficient inference and prediction.

**Applications and Results**

The researchers applied FDBM to two tasks: predicting future protein conformations from aligned data and unpaired image translation. In both cases, FDBM outperformed traditional Brownian-based models, achieving lower errors in protein structure prediction and image translation.

**Implications and Future Directions**

The development of FDBM has significant implications for understanding and predicting complex systems in various fields. By capturing complex behaviors and dependencies, FDBM can lead to more accurate predictions and better decision-making. Future research can build on this framework to tackle even more challenging problems in science and engineering."
stat.ML,A Proof of Learning Rate Transfer under $Î¼$P,"We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\mu P$, the optimal learning rate converges to a \emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.",http://arxiv.org/abs/2511.01734v1,2025-11-03T16:45:47Z,Soufiane Hayou,"**Unlocking the Secret to Learning Rate Transfer in Artificial Intelligence**

Imagine you're trying to teach a computer to recognize pictures of cats and dogs. You'd want the computer to learn quickly and accurately, right? One key factor in achieving this is the ""learning rate,"" which controls how fast the computer learns from its mistakes.

Researchers have been puzzled by a phenomenon called ""learning rate transfer,"" where the optimal learning rate for a small computer model (called a neural network) can be applied to a much larger model, leading to faster learning. Now, a new study provides a mathematical proof that explains why this works for a specific type of neural network called a linear multi-layer perceptron (MLP).

The study introduces a special way of setting up the neural network, called $\mu$P (mu-P), which helps the computer learn more effectively. The researchers found that when using $\mu$P, the optimal learning rate for a small neural network can indeed be transferred to a much larger network, without needing to be adjusted. This is a significant breakthrough, as it provides a theoretical explanation for learning rate transfer.

In contrast, other ways of setting up the neural network, such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP), do not exhibit this property. The study's findings are supported by extensive experiments, which demonstrate the effectiveness of $\mu$P in achieving learning rate transfer.

This research has important implications for the development of more efficient and effective artificial intelligence systems. By understanding how to transfer learning rates, researchers can build larger and more complex models that learn faster and more accurately, leading to breakthroughs in areas like computer vision, natural language processing, and more."
stat.ML,"Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued   Causal Inference via Dynamic Neural Masking","Counterfactual causal inference faces significant challenges when extended to multi-category, multi-valued treatments, where complex cross-effects between heterogeneous interventions are difficult to model. Existing methodologies remain constrained to binary or single-type treatments and suffer from restrictive assumptions, limited scalability, and inadequate evaluation frameworks for complex intervention scenarios.   We present XTNet, a novel network architecture for multi-category, multi-valued treatment effect estimation. Our approach introduces a cross-effect estimation module with dynamic masking mechanisms to capture treatment interactions without restrictive structural assumptions. The architecture employs a decomposition strategy separating basic effects from cross-treatment interactions, enabling efficient modeling of combinatorial treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that accounts for treatment costs and interaction effects. Extensive experiments on synthetic and real-world datasets demonstrate that XTNet consistently outperforms state-of-the-art baselines in both ranking accuracy and effect estimation quality. The results of the real-world A/B test further confirm its effectiveness.",http://arxiv.org/abs/2511.01641v1,2025-11-03T14:50:02Z,"Xiaopeng Ke, Yihan Yu, Ruyue Zhang, Zhishuo Zhou, Fangzhou Shi, Chang Men, Zhengdan Zhu","**Unlocking the Power of Complex Treatments: A Breakthrough in Causal Inference**

Imagine you're a doctor trying to decide which treatment to prescribe to a patient. You have multiple options, and each treatment can have different effects depending on the patient's characteristics and other factors. But how do you predict the outcome of different treatments and choose the best one?

Researchers have made a significant step forward in solving this problem by developing a new method called XTNet. This method can estimate the effects of multiple treatments, each with multiple possible values, and predict how they interact with each other.

The challenge with complex treatments is that their effects can be intertwined, making it hard to model and predict outcomes. XTNet addresses this challenge by using a novel architecture that:

1. **Captures treatment interactions**: XTNet introduces a dynamic masking mechanism that helps identify how different treatments interact with each other.
2. **Decomposes complex effects**: The method breaks down the effects of treatments into basic effects and cross-treatment interactions, making it easier to model and analyze complex scenarios.

The researchers tested XTNet on both simulated and real-world data and found that it outperforms existing methods in predicting treatment effects and ranking accuracy. They also proposed a new evaluation metric, MCMV-AUCC, which takes into account treatment costs and interaction effects.

The implications of this research are significant, as it can be applied to various fields, including medicine, economics, and social sciences, where understanding the effects of complex treatments is crucial. With XTNet, researchers and practitioners can make more informed decisions and develop more effective interventions."
stat.ML,Partial Trace-Class Bayesian Neural Networks,"Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in deep learning, but often come at a prohibitive computational cost. We propose three different innovative architectures of partial trace-class Bayesian neural networks (PaTraC BNNs) that enable uncertainty quantification comparable to standard BNNs but use significantly fewer Bayesian parameters. These PaTraC BNNs have computational and statistical advantages over standard Bayesian neural networks in terms of speed and memory requirements. Our proposed methodology therefore facilitates reliable, robust, and scalable uncertainty quantification in neural networks. The three architectures build on trace-class neural network priors which induce an ordering of the neural network parameters, and are thus a natural choice in our framework. In a numerical simulation study, we verify the claimed benefits, and further illustrate the performance of our proposed methodology on a real-world dataset.",http://arxiv.org/abs/2511.01628v1,2025-11-03T14:38:35Z,"Arran Carter, Torben Sell","Here's a summary of the research paper for a general audience:

**Making Deep Learning More Reliable and Efficient**

Deep learning models are powerful tools for making predictions, but they often lack a crucial aspect: uncertainty. Bayesian neural networks (BNNs) can provide a way to quantify uncertainty, but they can be computationally expensive. A team of researchers has proposed a new approach called partial trace-class Bayesian neural networks (PaTraC BNNs) that makes uncertainty quantification more efficient and scalable.

The innovation lies in designing neural networks that use fewer ""Bayesian parameters"" while still providing reliable uncertainty estimates. This leads to significant improvements in speed and memory requirements, making it possible to apply BNNs to larger and more complex problems.

The researchers tested their approach through simulations and real-world data, and the results show that PaTraC BNNs perform well while using fewer resources. This breakthrough has the potential to make deep learning models more reliable, robust, and practical for a wide range of applications."
