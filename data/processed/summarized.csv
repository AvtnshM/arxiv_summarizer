category,title,summary,link,published,authors,summary_short
cs.LG,Design Rule Checking with a CNN Based Feature Extractor,"Design rule checking (DRC) is getting increasingly complex in advanced nodes technologies. It would be highly desirable to have a fast interactive DRC engine that could be used during layout. In this work, we establish the proof of feasibility for such an engine. The proposed model consists of a convolutional neural network (CNN) trained to detect DRC violations. The model was trained with artificial data that was derived from a set of $50$ SRAM designs. The focus in this demonstration was metal 1 rules. Using this solution, we can detect multiple DRC violations 32x faster than Boolean checkers with an accuracy of up to 92. The proposed solution can be easily expanded to a complete rule set.",https://arxiv.org/pdf/2012.11510v1,2020-12-21T17:26:31Z,"Luis Francisco, Tanmay Lagare, Arpit Jain, Somal Chaudhary, Madhura Kulkarni, Divya Sardana, W. Rhett Davis, Paul Franzon","Here's a summary of the research paper for a general audience:

**Faster and Smarter Chip Design: A New Approach to Design Rule Checking**

As technology advances, designing and manufacturing computer chips becomes increasingly complex. One crucial step in chip design is ensuring that the design follows a set of strict rules, known as Design Rule Checking (DRC). However, traditional DRC methods can be slow and time-consuming.

Researchers have proposed a new approach that uses artificial intelligence (AI) to speed up DRC. They trained a convolutional neural network (CNN), a type of machine learning algorithm, to detect potential design rule violations. The CNN was trained on a dataset of 50 sample chip designs and was able to detect multiple violations 32 times faster than traditional methods, with an accuracy of up to 92%.

This breakthrough has the potential to revolutionize chip design, enabling designers to quickly identify and fix errors during the design process. The researchers focused on a specific set of rules related to metal layers in chips, but the approach can be easily expanded to cover a wider range of design rules. This innovation could lead to faster and more efficient chip design, which could have significant impacts on the development of new technologies, from smartphones to medical devices."
cs.LG,Unsupervised in-distribution anomaly detection of new physics through conditional density estimation,"Anomaly detection is a key application of machine learning, but is generally focused on the detection of outlying samples in the low probability density regions of data. Here we instead present and motivate a method for unsupervised in-distribution anomaly detection using a conditional density estimator, designed to find unique, yet completely unknown, sets of samples residing in high probability density regions. We apply this method towards the detection of new physics in simulated Large Hadron Collider (LHC) particle collisions as part of the 2020 LHC Olympics blind challenge, and show how we detected a new particle appearing in only 0.08% of 1 million collision events. The results we present are our original blind submission to the 2020 LHC Olympics, where it achieved the state-of-the-art performance.",https://arxiv.org/pdf/2012.11638v1,2020-12-21T19:05:22Z,"George Stein, Uros Seljak, Biwei Dai","Here's a summary of the research paper for a general audience:

**Detecting Rare Events in Particle Collisions**

Scientists at the Large Hadron Collider (LHC) use powerful machines to smash particles together to understand the fundamental nature of the universe. In these collisions, rare events can occur that might reveal new and exciting phenomena, such as the existence of new particles. However, these events are often hidden among a vast number of more common collisions.

A team of researchers has developed a new machine learning technique to detect these rare events, even if they are not easily distinguishable from more common ones. Unlike traditional methods that focus on detecting outliers, this technique looks for unusual patterns within the most common types of collisions.

The researchers tested their method in a challenge called the 2020 LHC Olympics, where they were able to detect a new particle that appeared in only 0.08% of 1 million collision events. This achievement set a new standard for detecting rare events in particle collisions and has the potential to lead to breakthroughs in our understanding of the universe.

**In simple terms:** Imagine trying to find a specific grain of sand on a beach. Traditional methods would look for grains that are clearly different from the rest, like a grain of a different color. The new technique looks for specific patterns within the common grains, allowing it to detect the desired grain even if it's not easily noticeable. This technique can help scientists discover new and exciting phenomena in particle collisions."
cs.LG,Detecting Botnet Attacks in IoT Environments: An Optimized Machine Learning Approach,"The increased reliance on the Internet and the corresponding surge in connectivity demand has led to a significant growth in Internet-of-Things (IoT) devices. The continued deployment of IoT devices has in turn led to an increase in network attacks due to the larger number of potential attack surfaces as illustrated by the recent reports that IoT malware attacks increased by 215.7% from 10.3 million in 2017 to 32.7 million in 2018. This illustrates the increased vulnerability and susceptibility of IoT devices and networks. Therefore, there is a need for proper effective and efficient attack detection and mitigation techniques in such environments. Machine learning (ML) has emerged as one potential solution due to the abundance of data generated and available for IoT devices and networks. Hence, they have significant potential to be adopted for intrusion detection for IoT environments. To that end, this paper proposes an optimized ML-based framework consisting of a combination of Bayesian optimization Gaussian Process (BO-GP) algorithm and decision tree (DT) classification model to detect attacks on IoT devices in an effective and efficient manner. The performance of the proposed framework is evaluated using the Bot-IoT-2018 dataset. Experimental results show that the proposed optimized framework has a high detection accuracy, precision, recall, and F-score, highlighting its effectiveness and robustness for the detection of botnet attacks in IoT environments.",https://arxiv.org/pdf/2012.11325v1,2020-12-16T16:39:55Z,"MohammadNoor Injadat, Abdallah Moubayed, Abdallah Shami","**Protecting IoT Devices from Cyber Attacks: A New Machine Learning Approach**

The growing number of Internet-connected devices, known as the Internet of Things (IoT), has created new vulnerabilities to cyber attacks. In 2018, IoT malware attacks increased by 215.7% to 32.7 million, highlighting the need for effective security measures. Researchers have proposed a new machine learning-based framework to detect and prevent botnet attacks on IoT devices.

This framework combines two techniques: Bayesian optimization and decision tree classification. It was tested using a dataset of known botnet attacks (Bot-IoT-2018) and showed high accuracy, precision, and recall in detecting attacks. The results suggest that this optimized machine learning approach is effective and robust in identifying botnet attacks in IoT environments.

In simple terms, this research offers a promising solution to protect IoT devices from cyber threats by using machine learning to detect and prevent attacks. This approach could help safeguard the growing number of connected devices and prevent them from being used for malicious activities."
cs.LG,Collaborative residual learners for automatic icd10 prediction using prescribed medications,"Clinical coding is an administrative process that involves the translation of diagnostic data from episodes of care into a standard code format such as ICD10. It has many critical applications such as billing and aetiology research. The automation of clinical coding is very challenging due to data sparsity, low interoperability of digital health systems, complexity of real-life diagnosis coupled with the huge size of ICD10 code space. Related work suffer from low applicability due to reliance on many data sources, inefficient modelling and less generalizable solutions. We propose a novel collaborative residual learning based model to automatically predict ICD10 codes employing only prescriptions data. Extensive experiments were performed on two real-world clinical datasets (outpatient & inpatient) from Maharaj Nakorn Chiang Mai Hospital with real case-mix distributions. We obtain multi-label classification accuracy of 0.71 and 0.57 of average precision, 0.57 and 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",https://arxiv.org/pdf/2012.11327v1,2020-12-16T07:07:27Z,"Yassien Shaalan, Alexander Dokumentov, Piyapong Khumrin, Krit Khwanngern, Anawat Wisetborisu, Thanakom Hatsadeang, Nattapat Karaket, Witthawin Achariyaviriya, Sansanee Auephanwiriyakul, Nipon Theera-Umpon, Terence Siganakis","Here's a summary of the research paper for a general audience:

**Automating Medical Coding with AI**

Medical coding is the process of translating patient diagnosis information into standardized codes, such as ICD10, for administrative and research purposes. However, this process is time-consuming, prone to errors, and challenging to automate due to the complexity of medical diagnoses and the vast number of possible codes.

Researchers have proposed a new artificial intelligence (AI) model that uses prescribed medication data to automatically predict ICD10 codes. This model, called collaborative residual learning, is designed to learn from patterns in medication data to make accurate predictions.

In tests using real-world clinical data from a hospital in Thailand, the model achieved promising results, with accuracy rates of 71% and 57% for inpatient and outpatient data, respectively. This means that the model was able to correctly predict the main diagnosis code about 7 out of 10 times for inpatient data and about 5 out of 10 times for outpatient data.

The development of this AI model has the potential to streamline medical coding, reduce errors, and free up healthcare professionals to focus on more critical tasks. While further testing and refinement are needed, this research represents an important step towards automating medical coding and improving healthcare efficiency."
cs.LG,Ensemble model for pre-discharge icd10 coding prediction,"The translation of medical diagnosis to clinical coding has wide range of applications in billing, aetiology analysis, and auditing. Currently, coding is a manual effort while the automation of such task is not straight forward. Among the challenges are the messy and noisy clinical records, case complexities, along with the huge ICD10 code space. Previous work mainly relied on discharge notes for prediction and was applied to a very limited data scale. We propose an ensemble model incorporating multiple clinical data sources for accurate code predictions. We further propose an assessment mechanism to provide confidence rates in predicted outcomes. Extensive experiments were performed on two new real-world clinical datasets (inpatient & outpatient) with unaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We obtain multi-label classification accuracies of 0.73 and 0.58 for average precision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",https://arxiv.org/pdf/2012.11333v1,2020-12-16T07:02:56Z,"Yassien Shaalan, Alexander Dokumentov, Piyapong Khumrin, Krit Khwanngern, Anawat Wisetborisu, Thanakom Hatsadeang, Nattapat Karaket, Witthawin Achariyaviriya, Sansanee Auephanwiriyakul, Nipon Theera-Umpon, Terence Siganakis","Here's a summary of the research paper for a general audience:

**Automating Medical Coding: A New Approach**

Medical coding is the process of translating a patient's diagnosis into a specific code, which is used for billing, research, and quality improvement. Currently, this process is done manually by healthcare professionals, which can be time-consuming and prone to errors.

Researchers have proposed a new approach to automate medical coding using a type of artificial intelligence called an ""ensemble model"". This model combines data from multiple sources, such as electronic health records, to predict the correct medical codes.

The researchers tested their model on two large datasets of patient records from a hospital in Thailand. They found that their model was able to accurately predict the medical codes with a high degree of accuracy, achieving scores of 0.73 and 0.58 for inpatient and outpatient data, respectively.

The innovation of this approach lies in its ability to handle complex and noisy clinical data, and to provide a confidence rate for each predicted code. This can help healthcare professionals to review and validate the codes more efficiently.

The potential benefits of this technology include improved accuracy, reduced manual workload, and enhanced data quality for research and quality improvement initiatives."
cs.LG,Single-level Optimization For Differential Architecture Search,"In this paper, we point out that differential architecture search (DARTS) makes gradient of architecture parameters biased for network weights and architecture parameters are updated in different datasets alternatively in the bi-level optimization framework. The bias causes the architecture parameters of non-learnable operations to surpass that of learnable operations. Moreover, using softmax as architecture parameters' activation function and inappropriate learning rate would exacerbate the bias. As a result, it's frequently observed that non-learnable operations are dominated in the search phase. To reduce the bias, we propose to use single-level to replace bi-level optimization and non-competitive activation function like sigmoid to replace softmax. As a result, we could search high-performance architectures steadily. Experiments on NAS Benchmark 201 validate our hypothesis and stably find out nearly the optimal architecture. On DARTS space, we search the state-of-the-art architecture with 77.0% top1 accuracy (training setting follows PDARTS and without any additional module) on ImageNet-1K and steadily search architectures up-to 76.5% top1 accuracy (but not select the best from the searched architectures) which is comparable with current reported best result.",https://arxiv.org/pdf/2012.11337v1,2020-12-15T18:40:33Z,"Pengfei Hou, Ying Jin","Here's a summary of the research paper for a general audience:

**Improving the Search for Better Artificial Intelligence Models**

Researchers have identified a problem with a popular method for automatically designing artificial intelligence (AI) models, called Differential Architecture Search (DARTS). The issue arises from the way the method optimizes the model's architecture, which can lead to biased results. Specifically, the method tends to favor ""non-learnable"" operations over ""learnable"" ones, which can result in suboptimal models.

To address this problem, the researchers propose a new approach that uses a single-level optimization method and a different activation function. This approach helps to reduce the bias and leads to more stable and better-performing models.

**Key Findings and Results**

The researchers tested their approach on a benchmark dataset and found that it consistently discovered high-performance models. On a large image classification dataset (ImageNet-1K), they found a model with a state-of-the-art accuracy of 77.0%. This is comparable to, or even surpasses, the best results reported by other methods.

**What does this mean?**

The researchers' work has the potential to improve the design of AI models, which are used in a wide range of applications, from image and speech recognition to natural language processing. By developing more efficient and effective methods for designing AI models, we can accelerate progress in AI research and its applications."
cs.LG,Unifying Homophily and Heterophily Network Transformation via Motifs,"Higher-order proximity (HOP) is fundamental for most network embedding methods due to its significant effects on the quality of node embedding and performance on downstream network analysis tasks. Most existing HOP definitions are based on either homophily to place close and highly interconnected nodes tightly in embedding space or heterophily to place distant but structurally similar nodes together after embedding. In real-world networks, both can co-exist, and thus considering only one could limit the prediction performance and interpretability. However, there is no general and universal solution that takes both into consideration. In this paper, we propose such a simple yet powerful framework called homophily and heterophliy preserving network transformation (H2NT) to capture HOP that flexibly unifies homophily and heterophily. Specifically, H2NT utilises motif representations to transform a network into a new network with a hybrid assumption via micro-level and macro-level walk paths. H2NT can be used as an enhancer to be integrated with any existing network embedding methods without requiring any changes to latter methods. Because H2NT can sparsify networks with motif structures, it can also improve the computational efficiency of existing network embedding methods when integrated. We conduct experiments on node classification, structural role classification and motif prediction to show the superior prediction performance and computational efficiency over state-of-the-art methods. In particular, DeepWalk-based H2 NT achieves 24% improvement in terms of precision on motif prediction, while reducing 46% computational time compared to the original DeepWalk.",https://arxiv.org/pdf/2012.11400v2,2020-12-21T15:03:18Z,"Yan Ge, Jun Ma, Li Zhang, Haiping Lu","**Unlocking the Secrets of Complex Networks**

Imagine a social network where friends are connected to each other, and a network of web pages where related but distant pages are linked. Researchers have long struggled to capture the nuances of these complex networks, where relationships between nodes (like people or web pages) can be either similar (homophily) or dissimilar (heterophily). A new framework, called Homophily and Heterophily Preserving Network Transformation (H2NT), has been developed to unify these two concepts.

**The Problem: Balancing Similarity and Dissimilarity**

In network analysis, understanding the relationships between nodes is crucial for tasks like predicting node behavior or identifying clusters. Traditional methods focus on either homophily (nodes with similar connections) or heterophily (nodes with dissimilar connections), but real-world networks often exhibit both. H2NT addresses this limitation by transforming a network into a new representation that captures both homophily and heterophily.

**The Solution: H2NT**

H2NT uses ""motifs"" (small patterns of connections) to create a hybrid network that balances similarity and dissimilarity. This framework can be integrated with existing network analysis methods, improving their performance and efficiency. By sparsifying networks, H2NT also reduces computational time.

**The Results: Improved Performance and Efficiency**

Tests on various network analysis tasks, such as node classification and motif prediction, demonstrate that H2NT outperforms state-of-the-art methods. For example, when combined with DeepWalk, a popular network analysis method, H2NT achieves a 24% improvement in precision on motif prediction while reducing computational time by 46%.

**The Impact: A New Era in Network Analysis**

The H2NT framework has the potential to revolutionize the field of network analysis, enabling researchers to better understand complex relationships in various domains, from social networks to biological systems. By providing a unified approach to capturing homophily and heterophily, H2NT paves the way for more accurate and efficient network analysis."
cs.LG,Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals,"Spurious correlations threaten the validity of statistical classifiers. While model accuracy may appear high when the test data is from the same distribution as the training data, it can quickly degrade when the test distribution changes. For example, it has been shown that classifiers perform poorly when humans make minor modifications to change the label of an example. One solution to increase model reliability and generalizability is to identify causal associations between features and classes. In this paper, we propose to train a robust text classifier by augmenting the training data with automatically generated counterfactual data. We first identify likely causal features using a statistical matching approach. Next, we generate counterfactual samples for the original training data by substituting causal features with their antonyms and then assigning opposite labels to the counterfactual samples. Finally, we combine the original data and counterfactual data to train a robust classifier. Experiments on two classification tasks show that a traditional classifier trained on the original data does very poorly on human-generated counterfactual samples (e.g., 10%-37% drop in accuracy). However, the classifier trained on the combined data is more robust and performs well on both the original test data and the counterfactual test data (e.g., 12%-25% increase in accuracy compared with the traditional classifier). Detailed analysis shows that the robust classifier makes meaningful and trustworthy predictions by emphasizing causal features and de-emphasizing non-causal features.",https://arxiv.org/pdf/2012.10040v1,2020-12-18T03:57:32Z,"Zhao Wang, Aron Culotta","**Improving Text Classification with Counterfactual Data**

Imagine you're trying to train a computer to classify text into different categories, such as spam vs. non-spam emails. A common problem is that the computer may pick up on superficial patterns in the data, like certain words or phrases that are often associated with a particular category, rather than understanding the underlying meaning. These superficial patterns are called ""spurious correlations.""

Researchers have found that when the computer is tested on new data that is slightly different from the original training data, its performance can degrade significantly. For example, if the computer was trained on a dataset of spam emails that often contain the word ""free,"" it may not perform well on a new dataset of spam emails that use the word ""gratis"" instead.

To address this issue, researchers propose a new approach that involves generating ""counterfactual"" data, which are examples that are similar to the original data but with key features changed. For instance, if the computer was trained on a dataset of spam emails that often contain the word ""free,"" the researchers might generate counterfactual examples by replacing the word ""free"" with ""expensive"" and changing the label to ""non-spam.""

By training the computer on both the original data and the counterfactual data, the researchers found that it becomes more robust and better able to generalize to new, unseen data. In experiments, the computer's accuracy improved by 12-25% on both the original test data and the counterfactual test data. This approach helps the computer to focus on the underlying meaning of the text, rather than superficial patterns, and makes its predictions more trustworthy."
cs.LG,Transfer Learning Based Automatic Model Creation Tool For Resource Constraint Devices,"With the enhancement of Machine Learning, many tools are being designed to assist developers to easily create their Machine Learning models. In this paper, we propose a novel method for auto creation of such custom models for constraint devices using transfer learning without the need to write any machine learning code. We share the architecture of our automatic model creation tool and the CNN Model created by it using pretrained models such as YAMNet and MobileNetV2 as feature extractors. Finally, we demonstrate accuracy and memory footprint of the model created from the tool by creating an Automatic Image and Audio classifier and report the results of our experiments using Stanford Cars and ESC-50 dataset.",https://arxiv.org/pdf/2012.10056v1,2020-12-18T05:38:58Z,"Karthik Bhat, Manan Bhandari, ChangSeok Oh, Sujin Kim, Jeeho Yoo","Here's a summary of the research paper for a general audience:

**Making Machine Learning Accessible to All**

Machine learning is a powerful technology that enables computers to learn from data and make predictions. However, creating machine learning models can be a complex and time-consuming task, requiring specialized coding skills. This can be a barrier for developers who want to use machine learning on devices with limited computing resources, such as smartphones or smart home devices.

To address this challenge, researchers have developed a new tool that makes it easy to create custom machine learning models without needing to write code. The tool uses a technique called transfer learning, which allows it to leverage pre-trained models and adapt them to new tasks.

The researchers tested their tool by creating an automatic image and audio classifier, which can identify objects in images and sounds in audio recordings. They used publicly available datasets, such as Stanford Cars and ESC-50, to evaluate the performance of their model. The results showed that the model created by the tool achieved high accuracy and used a relatively small amount of memory, making it suitable for use on devices with limited resources.

Overall, this research aims to make machine learning more accessible and user-friendly, enabling a wider range of developers to harness the power of AI on a variety of devices."
cs.LG,Data Assimilation in the Latent Space of a Neural Network,"There is an urgent need to build models to tackle Indoor Air Quality issue. Since the model should be accurate and fast, Reduced Order Modelling technique is used to reduce the dimensionality of the problem. The accuracy of the model, that represent a dynamic system, is improved integrating real data coming from sensors using Data Assimilation techniques. In this paper, we formulate a new methodology called Latent Assimilation that combines Data Assimilation and Machine Learning. We use a Convolutional neural network to reduce the dimensionality of the problem, a Long-Short-Term-Memory to build a surrogate model of the dynamic system and an Optimal Interpolated Kalman Filter to incorporate real data. Experimental results are provided for CO2 concentration within an indoor space. This methodology can be used for example to predict in real-time the load of virus, such as the SARS-COV-2, in the air by linking it to the concentration of CO2.",https://arxiv.org/pdf/2012.12056v1,2020-12-22T14:43:50Z,"Maddalena Amendola, Rossella Arcucci, Laetitia Mottet, Cesar Quilodran Casas, Shiwei Fan, Christopher Pain, Paul Linden, Yi-Ke Guo","**Improving Indoor Air Quality with AI-Powered Modeling**

Indoor air quality is a pressing concern, especially with the risk of airborne viruses like SARS-COV-2. Researchers have developed a new method called Latent Assimilation, which combines machine learning and data assimilation techniques to create more accurate and faster models of indoor air quality.

The method uses a neural network to reduce the complexity of the problem, a type of recurrent neural network (LSTM) to simulate the dynamic behavior of the air quality, and a Kalman filter to incorporate real-time data from sensors. The approach was tested on CO2 concentration data within an indoor space and showed promising results.

This innovative methodology has the potential to predict indoor air quality in real-time, including the load of viruses like SARS-COV-2, by linking it to CO2 concentration. This could enable more effective measures to prevent the spread of airborne diseases and improve overall indoor air quality."
cs.LG,Prediction of Chronic Kidney Disease Using Deep Neural Network,"Deep neural Network (DNN) is becoming a focal point in Machine Learning research. Its application is penetrating into different fields and solving intricate and complex problems. DNN is now been applied in health image processing to detect various ailment such as cancer and diabetes. Another disease that is causing threat to our health is the kidney disease. This disease is becoming prevalent due to substances and elements we intake. Death is imminent and inevitable within few days without at least one functioning kidney. Ignoring the kidney malfunction can cause chronic kidney disease leading to death. Frequently, Chronic Kidney Disease (CKD) and its symptoms are mild and gradual, often go unnoticed for years only to be realized lately. Bade, a Local Government of Yobe state in Nigeria has been a center of attention by medical practitioners due to the prevalence of CKD. Unfortunately, a technical approach in culminating the disease is yet to be attained. We obtained a record of 400 patients with 10 attributes as our dataset from Bade General Hospital. We used DNN model to predict the absence or presence of CKD in the patients. The model produced an accuracy of 98%. Furthermore, we identified and highlighted the Features importance to provide the ranking of the features used in the prediction of the CKD. The outcome revealed that two attributes; Creatinine and Bicarbonate have the highest influence on the CKD prediction.",https://arxiv.org/pdf/2012.12089v1,2020-12-22T15:31:14Z,"Iliyas Ibrahim Iliyas, Isah Rambo Saidu, Ali Baba Dauda, Suleiman Tasiu","**Breakthrough in Predicting Chronic Kidney Disease using Artificial Intelligence**

Chronic Kidney Disease (CKD) is a serious health condition that can lead to death if left untreated. Researchers have made a significant breakthrough in predicting CKD using a type of artificial intelligence called Deep Neural Network (DNN). In a study conducted in Nigeria, a DNN model was trained on data from 400 patients to predict the presence or absence of CKD. The model achieved an impressive accuracy of 98%, outperforming traditional methods.

The study also identified the most important factors that contribute to CKD prediction. The results showed that two key attributes, Creatinine and Bicarbonate, have the highest influence on predicting CKD. This finding can help doctors and healthcare professionals focus on these critical factors when diagnosing and treating patients.

The use of DNN in predicting CKD has the potential to revolutionize healthcare, especially in areas where access to medical expertise is limited. Early detection and treatment of CKD can significantly improve patient outcomes, and this study demonstrates the power of AI in making a positive impact on human health."
cs.LG,Machine Learning Algorithm for NLOS Millimeter Wave in 5G V2X Communication,"The 5G vehicle-to-everything (V2X) communication for autonomous and semi-autonomous driving utilizes the wireless technology for communication and the Millimeter Wave bands are widely implemented in this kind of vehicular network application. The main purpose of this paper is to broadcast the messages from the mmWave Base Station to vehicles at LOS (Line-of-sight) and NLOS (Non-LOS). Relay using Machine Learning (RML) algorithm is formulated to train the mmBS for identifying the blockages within its coverage area and broadcast the messages to the vehicles at NLOS using a LOS nodes as a relay. The transmission of information is faster with higher throughput and it covers a wider bandwidth which is reused, therefore when performing machine learning within the coverage area of mmBS most of the vehicles in NLOS can be benefited. A unique method of relay mechanism combined with machine learning is proposed to communicate with mobile nodes at NLOS.",https://arxiv.org/pdf/2012.12123v1,2020-12-16T11:41:09Z,"Deepika Mohan, G. G. Md. Nawaz Ali, Peter Han Joo Chong","**Improving 5G Connectivity for Self-Driving Cars**

Imagine a future where self-driving cars can communicate with each other and their surroundings seamlessly, ensuring a safe and efficient driving experience. A recent research paper proposes a solution to enhance this communication, particularly in situations where direct line-of-sight (LOS) between the car's communication device and the base station is blocked.

The researchers developed a machine learning algorithm that enables a relay system to help cars that are out of direct sight (Non-LOS or NLOS) receive important messages from the base station. This is achieved by using nearby cars that have a clear line-of-sight to act as relays, forwarding messages to cars that are blocked from direct communication.

The proposed system offers several benefits, including:

* **Faster information transmission**: Messages are delivered quickly, which is critical for self-driving cars that rely on real-time data to make decisions.
* **Higher data capacity**: The system can handle a large amount of data, ensuring that cars receive the information they need to operate safely.
* **Wider coverage**: The relay system allows more cars to receive messages, even if they are not in direct line-of-sight with the base station.

By combining machine learning with a unique relay mechanism, this research has the potential to improve the reliability and efficiency of 5G communication for autonomous vehicles, paving the way for safer and more efficient transportation systems."
cs.LG,Projected Stochastic Gradient Langevin Algorithms for Constrained Sampling and Non-Convex Learning,"Langevin algorithms are gradient descent methods with additive noise. They have been used for decades in Markov chain Monte Carlo (MCMC) sampling, optimization, and learning. Their convergence properties for unconstrained non-convex optimization and learning problems have been studied widely in the last few years. Other work has examined projected Langevin algorithms for sampling from log-concave distributions restricted to convex compact sets. For learning and optimization, log-concave distributions correspond to convex losses. In this paper, we analyze the case of non-convex losses with compact convex constraint sets and IID external data variables. We term the resulting method the projected stochastic gradient Langevin algorithm (PSGLA). We show the algorithm achieves a deviation of $O(T^{-1/4}(\log T)^{1/2})$ from its target distribution in 1-Wasserstein distance. For optimization and learning, we show that the algorithm achieves $ε$-suboptimal solutions, on average, provided that it is run for a time that is polynomial in $ε^{-1}$ and slightly super-exponential in the problem dimension.",https://arxiv.org/pdf/2012.12137v1,2020-12-22T16:19:20Z,Andrew Lamperski,"**Unlocking Efficient Machine Learning with Projected Stochastic Gradient Langevin Algorithms**

Imagine you're trying to optimize a complex system, like a self-driving car's navigation system, to make the best decisions possible. This involves minimizing a ""loss function"" that measures how well the system performs. However, the system has constraints, like staying within the road boundaries, and the data used to train it is noisy and uncertain.

Researchers have developed a new algorithm, called the Projected Stochastic Gradient Langevin Algorithm (PSGLA), to tackle these challenges. This algorithm combines the strengths of two popular methods: gradient descent, which iteratively adjusts the system's parameters to minimize the loss function, and Langevin algorithms, which add a random ""noise"" term to the updates to explore the solution space.

The PSGLA algorithm is designed to work with complex, non-convex loss functions, which are common in machine learning problems. It also incorporates constraints, ensuring that the solution stays within a specified region. For example, in the self-driving car scenario, the algorithm would ensure that the navigation system stays within the road boundaries.

The researchers analyzed the performance of PSGLA and found that it achieves impressive results. Specifically, they showed that:

* The algorithm converges to the target distribution (i.e., the optimal solution) at a rate of $O(T^{-1/4}(\log T)^{1/2})$, which is a measure of how quickly the algorithm approaches the optimal solution.
* For optimization and learning problems, PSGLA can find solutions that are within a certain tolerance (ε) of the optimal solution, on average, in a time that grows polynomially with the tolerance and slightly super-exponentially with the problem dimension.

To put it simply, the PSGLA algorithm is a powerful tool for optimizing complex systems with constraints. Its ability to efficiently explore the solution space and converge to the optimal solution makes it a promising approach for a wide range of applications, from machine learning to robotics.

**What does this mean for real-world applications?**

The PSGLA algorithm has the potential to improve the performance of many machine learning systems, such as:

* **Robotics**: PSGLA can be used to optimize control systems for robots, ensuring that they stay within specified constraints and perform tasks efficiently.
* **Computer vision**: PSGLA can be used to optimize image recognition systems, ensuring that they accurately identify objects and stay within specified constraints.
* **Natural language processing**: PSGLA can be used to optimize language models, ensuring that they accurately predict text and stay within specified constraints.

By providing a more efficient and effective way to optimize complex systems, the PSGLA algorithm can help drive innovation in a wide range of fields."
cs.LG,Image to Bengali Caption Generation Using Deep CNN and Bidirectional Gated Recurrent Unit,"There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.",https://arxiv.org/pdf/2012.12139v1,2020-12-22T16:22:02Z,"Al Momin Faruk, Hasan Al Faraby, Md. Muzahidul Azad, Md. Riduyan Fedous, Md. Kishor Morol","Here's a summary of the research paper for a general audience:

**Breaking the Language Barrier: Generating Bengali Captions from Images**

Imagine being able to understand and describe an image in your native language, Bengali, which is spoken by over 243 million people worldwide. Researchers have made a significant step towards making this possible by developing a new artificial intelligence (AI) model that generates Bengali captions from images.

The model uses a combination of two powerful AI techniques: a deep convolutional neural network (CNN) to analyze the image, and a bidirectional gated recurrent unit (GRU) to generate a caption in Bengali. This technology has the potential to help Bengali speakers understand and share information more easily, and could also assist visually impaired individuals in their daily lives.

In a test using a new dataset of 8000 images, the model achieved impressive results, generating captions that were highly accurate and natural-sounding. This breakthrough could pave the way for more research and applications in Bengali language processing, ultimately helping to bridge the language gap and promote greater understanding and communication."
cs.LG,High-Speed Robot Navigation using Predicted Occupancy Maps,"Safe and high-speed navigation is a key enabling capability for real world deployment of robotic systems. A significant limitation of existing approaches is the computational bottleneck associated with explicit mapping and the limited field of view (FOV) of existing sensor technologies. In this paper, we study algorithmic approaches that allow the robot to predict spaces extending beyond the sensor horizon for robust planning at high speeds. We accomplish this using a generative neural network trained from real-world data without requiring human annotated labels. Further, we extend our existing control algorithms to support leveraging the predicted spaces to improve collision-free planning and navigation at high speeds. Our experiments are conducted on a physical robot based on the MIT race car using an RGBD sensor where were able to demonstrate improved performance at 4 m/s compared to a controller not operating on predicted regions of the map.",https://arxiv.org/pdf/2012.12142v1,2020-12-22T16:25:12Z,"Kapil D. Katyal, Adam Polevoy, Joseph Moore, Craig Knuth, Katie M. Popek","**Breakthrough in Fast and Safe Robot Navigation**

Imagine robots that can navigate through spaces quickly and safely, without bumping into obstacles. Researchers have made a significant progress in developing a new approach to enable high-speed robot navigation. The challenge lies in the limited view of sensors and the time it takes to process information. To overcome this, the team trained a neural network to predict what lies beyond the sensor's view, allowing the robot to anticipate and avoid potential obstacles.

The researchers tested their approach on a robot car and achieved impressive results, demonstrating improved navigation performance at speeds of up to 4 meters per second (about 9 miles per hour). This development has the potential to enable robots to move quickly and safely in real-world environments, such as warehouses, hospitals, and homes. The innovation could pave the way for more efficient and autonomous robotic systems."
cs.LG,General Domain Adaptation Through Proportional Progressive Pseudo Labeling,"Domain adaptation helps transfer the knowledge gained from a labeled source domain to an unlabeled target domain. During the past few years, different domain adaptation techniques have been published. One common flaw of these approaches is that while they might work well on one input type, such as images, their performance drops when applied to others, such as text or time-series. In this paper, we introduce Proportional Progressive Pseudo Labeling (PPPL), a simple, yet effective technique that can be implemented in a few lines of code to build a more general domain adaptation technique that can be applied on several different input types. At the beginning of the training phase, PPPL progressively reduces target domain classification error, by training the model directly with pseudo-labeled target domain samples, while excluding samples with more likely wrong pseudo-labels from the training set and also postponing training on such samples. Experiments on 6 different datasets that include tasks such as anomaly detection, text sentiment analysis and image classification demonstrate that PPPL can beat other baselines and generalize better.",https://arxiv.org/pdf/2012.13028v1,2020-12-23T23:57:00Z,"Mohammad J. Hashemi, Eric Keller","**Improving Domain Adaptation: A Simple yet Effective Technique**

Domain adaptation is a technique that helps machines learn from one type of data and apply that knowledge to another, different type of data. For example, a machine learning model trained on images might not work well on text data. Researchers have proposed various domain adaptation techniques, but most of them have limitations and only work well on specific types of data.

A new technique called Proportional Progressive Pseudo Labeling (PPPL) has been developed to overcome these limitations. PPPL is a simple and effective method that can be applied to various types of data, including images, text, and time-series data.

Here's how PPPL works: during the training phase, the model is initially trained on a subset of the target data with pseudo-labels (guessed labels). The model gradually learns from more data, excluding samples that are likely to have incorrect pseudo-labels. This approach helps reduce errors and improves the model's performance.

The researchers tested PPPL on six different datasets, including tasks such as anomaly detection, text sentiment analysis, and image classification. The results showed that PPPL outperformed other techniques and generalized better across different types of data. This means that PPPL has the potential to be a more versatile and effective domain adaptation technique, applicable to a wide range of machine learning tasks."
cs.LG,Wheel-Rail Interface Condition Estimation (W-RICE),"The surface roughness between the wheel and rail has a huge influence on rolling noise level. The presence of the third body such as frost or grease at wheel-rail interface contributes towards change in adhesion coefficient resulting in the generation of noise at various levels. Therefore, it is possible to estimate adhesion conditions between the wheel and rail from the analysis of noise patterns originating from wheel-rail interaction. In this study, a new approach to estimate adhesion condition is proposed which takes rolling noise as input.",https://arxiv.org/pdf/2012.13096v1,2020-12-24T04:40:27Z,"Sundar Shrestha, Anand Koirala, Maksym Spiryagin, Qing Wu","**New Method Estimates Wheel-Rail Condition Using Rolling Noise**

Researchers have developed a novel approach called Wheel-Rail Interface Condition Estimation (W-RICE) to assess the condition of the interface between train wheels and rails. The condition of this interface, including factors like surface roughness and presence of substances like frost or grease, significantly affects the noise level generated by trains. By analyzing the noise patterns produced by the interaction between wheels and rails, W-RICE can estimate the adhesion (or grip) between the two surfaces. This breakthrough could lead to improved maintenance and safety of rail networks by enabling early detection of potential issues."
cs.LG,Upper Confidence Bounds for Combining Stochastic Bandits,"We provide a simple method to combine stochastic bandit algorithms. Our approach is based on a ""meta-UCB"" procedure that treats each of $N$ individual bandit algorithms as arms in a higher-level $N$-armed bandit problem that we solve with a variant of the classic UCB algorithm. Our final regret depends only on the regret of the base algorithm with the best regret in hindsight. This approach provides an easy and intuitive alternative strategy to the CORRAL algorithm for adversarial bandits, without requiring the stability conditions imposed by CORRAL on the base algorithms. Our results match lower bounds in several settings, and we provide empirical validation of our algorithm on misspecified linear bandit and model selection problems.",https://arxiv.org/pdf/2012.13115v1,2020-12-24T05:36:29Z,"Ashok Cutkosky, Abhimanyu Das, Manish Purohit","**Improving Decision-Making with a Simple and Effective Method**

Imagine you're trying to find the best option among several choices, but you don't know which one is the best. This is a classic problem in decision-making, known as a ""bandit problem"". Researchers have developed algorithms to help solve this problem, but choosing the best algorithm can be tricky.

A new study proposes a simple and intuitive method to combine multiple algorithms, called ""meta-UCB"". This approach treats each algorithm as an option and uses a modified version of the classic UCB algorithm to select the best one. The result is a method that performs as well as the best individual algorithm, without requiring complex conditions or assumptions.

The study shows that this method works well in various situations, including when the algorithms are not perfectly accurate or when there are many options to choose from. The researchers also tested their method on real-world problems, such as selecting the best model for predicting outcomes, and found that it performs well.

Overall, the meta-UCB method provides a practical and effective way to combine multiple algorithms and make better decisions in uncertain situations."
cs.LG,Randomized RX for target detection,"This work tackles the target detection problem through the well-known global RX method. The RX method models the clutter as a multivariate Gaussian distribution, and has been extended to nonlinear distributions using kernel methods. While the kernel RX can cope with complex clutters, it requires a considerable amount of computational resources as the number of clutter pixels gets larger. Here we propose random Fourier features to approximate the Gaussian kernel in kernel RX and consequently our development keep the accuracy of the nonlinearity while reducing the computational cost which is now controlled by an hyperparameter. Results over both synthetic and real-world image target detection problems show space and time efficiency of the proposed method while providing high detection performance.",https://arxiv.org/pdf/2012.12308v1,2020-12-08T19:18:49Z,"Fatih Nar, Adrián Pérez-Suay, José Antonio Padrón, Gustau Camps-Valls","**Improving Target Detection with a Faster and More Efficient Method**

Researchers have developed a new approach to detect targets in images, such as objects or anomalies, using a technique called the Random Fourier Features (RX) method. The traditional RX method assumes that the background (or ""clutter"") follows a simple statistical pattern, but this can be limiting when dealing with complex scenes. A previous extension of the RX method, called kernel RX, can handle more complex patterns, but it requires a lot of computational power.

The new method uses a mathematical trick to approximate the complex patterns, allowing for faster and more efficient target detection. The approach reduces the computational cost while maintaining high accuracy, making it suitable for large-scale image analysis. Tests on both simulated and real-world images demonstrate the effectiveness of the new method, which offers improved speed and efficiency without sacrificing detection performance.

**In simpler terms:** Imagine trying to spot a specific object in a busy image. This new method helps computers do that more quickly and accurately, even when the background is complex or cluttered. It's an important advancement for applications such as surveillance, environmental monitoring, and medical imaging."
cs.LG,"The Life and Death of SSDs and HDDs: Similarities, Differences, and Prediction Models","Data center downtime typically centers around IT equipment failure. Storage devices are the most frequently failing components in data centers. We present a comparative study of hard disk drives (HDDs) and solid state drives (SSDs) that constitute the typical storage in data centers. Using a six-year field data of 100,000 HDDs of different models from the same manufacturer from the BackBlaze dataset and a six-year field data of 30,000 SSDs of three models from a Google data center, we characterize the workload conditions that lead to failures and illustrate that their root causes differ from common expectation but remain difficult to discern. For the case of HDDs we observe that young and old drives do not present many differences in their failures. Instead, failures may be distinguished by discriminating drives based on the time spent for head positioning. For SSDs, we observe high levels of infant mortality and characterize the differences between infant and non-infant failures. We develop several machine learning failure prediction models that are shown to be surprisingly accurate, achieving high recall and low false positive rates. These models are used beyond simple prediction as they aid us to untangle the complex interaction of workload characteristics that lead to failures and identify failure root causes from monitored symptoms.",https://arxiv.org/pdf/2012.12373v1,2020-12-22T21:50:32Z,"Riccardo Pinciroli, Lishan Yang, Jacob Alter, Evgenia Smirni","**The Life and Death of Computer Storage Devices: A Study Reveals Surprising Insights**

Computer data centers, which store and manage vast amounts of digital information, are vulnerable to equipment failures that can cause downtime. One of the most common causes of failure is storage devices, such as hard disk drives (HDDs) and solid state drives (SSDs). A recent study analyzed data from over 100,000 HDDs and 30,000 SSDs to understand what leads to their failures.

The study found that HDDs and SSDs fail in different ways. Contrary to expectations, young and old HDDs are equally likely to fail, but drives that spend more time positioning their heads are more prone to failure. For SSDs, many failures occur early in their lifespan, a phenomenon known as ""infant mortality."" The study also identified distinct patterns of failure for SSDs that occur later in their lifespan.

The researchers developed machine learning models that can predict when these storage devices are likely to fail. These models are surprisingly accurate and can help data center operators anticipate and prevent failures. By understanding the complex interactions between workload characteristics and device failures, the study provides valuable insights for improving the reliability and lifespan of computer storage devices.

**Key Takeaways:**

* Storage devices are a common cause of data center downtime
* HDDs and SSDs fail in different ways, with distinct patterns of failure
* Machine learning models can accurately predict device failures
* Understanding device failures can help improve reliability and lifespan of storage devices

This study has important implications for data center operators, IT professionals, and manufacturers of storage devices, highlighting the need for more robust and reliable storage solutions."
cs.CV,"Leaf Segmentation and Counting with Deep Learning: on Model Certainty, Test-Time Augmentation, Trade-Offs","Plant phenotyping tasks such as leaf segmentation and counting are fundamental to the study of phenotypic traits. Since it is well-suited for these tasks, deep supervised learning has been prevalent in recent works proposing better performing models at segmenting and counting leaves. Despite good efforts from research groups, one of the main challenges for proposing better methods is still the limitation of labelled data availability. The main efforts of the field seem to be augmenting existing limited data sets, and some aspects of the modelling process have been under-discussed. This paper explores such topics and present experiments that led to the development of the best-performing method in the Leaf Segmentation Challenge and in another external data set of Komatsuna plants. The model has competitive performance while been arguably simpler than other recently proposed ones. The experiments also brought insights such as the fact that model cardinality and test-time augmentation may have strong applications in object segmentation of single class and high occlusion, and regarding the data distribution of recently proposed data sets for benchmarking.",https://arxiv.org/pdf/2012.11486v1,2020-12-21T17:00:05Z,"Douglas Pinto Sampaio Gomes, Lihong Zheng","Here's a summary of the research paper for a general audience:

**Accurately Counting Leaves with Artificial Intelligence**

Scientists are using artificial intelligence (AI) to analyze plants and study their characteristics. One important task is to accurately count the leaves of a plant, which can help researchers understand how plants grow and develop. In this study, researchers explored ways to improve AI models that segment (separate) and count leaves from images.

The researchers found that even with limited data, AI models can be trained to accurately count leaves. They developed a simple yet effective model that outperformed other models in a competition and on a separate dataset of a different plant species. The study also highlighted the importance of techniques such as ""test-time augmentation,"" which involves generating multiple versions of an image to help the model make more accurate predictions.

The findings have implications for plant research and could lead to better understanding of plant growth and development. The study's results also suggest that simpler AI models can be just as effective as more complex ones, which could make it easier for researchers to develop and use these models."
cs.CV,PointINet: Point Cloud Frame Interpolation Network,"LiDAR point cloud streams are usually sparse in time dimension, which is limited by hardware performance. Generally, the frame rates of mechanical LiDAR sensors are 10 to 20 Hz, which is much lower than other commonly used sensors like cameras. To overcome the temporal limitations of LiDAR sensors, a novel task named Point Cloud Frame Interpolation is studied in this paper. Given two consecutive point cloud frames, Point Cloud Frame Interpolation aims to generate intermediate frame(s) between them. To achieve that, we propose a novel framework, namely Point Cloud Frame Interpolation Network (PointINet). Based on the proposed method, the low frame rate point cloud streams can be upsampled to higher frame rates. We start by estimating bi-directional 3D scene flow between the two point clouds and then warp them to the given time step based on the 3D scene flow. To fuse the two warped frames and generate intermediate point cloud(s), we propose a novel learning-based points fusion module, which simultaneously takes two warped point clouds into consideration. We design both quantitative and qualitative experiments to evaluate the performance of the point cloud frame interpolation method and extensive experiments on two large scale outdoor LiDAR datasets demonstrate the effectiveness of the proposed PointINet. Our code is available at https://github.com/ispc-lab/PointINet.git.",https://arxiv.org/pdf/2012.10066v1,2020-12-18T06:15:01Z,"Fan Lu, Guang Chen, Sanqing Qu, Zhijun Li, Yinlong Liu, Alois Knoll","**Enhancing LiDAR Technology with PointINet**

LiDAR (Light Detection and Ranging) sensors are commonly used in various applications, including self-driving cars and robotics, to create 3D models of the environment. However, these sensors have a limitation: they can only capture a certain number of frames per second, which can be as low as 10-20 frames per second. This can make it difficult to accurately track and predict the movement of objects.

To overcome this limitation, researchers have developed a new technology called PointINet, a Point Cloud Frame Interpolation Network. This network takes two consecutive 3D point cloud frames captured by a LiDAR sensor and generates one or more intermediate frames between them. This process, known as point cloud frame interpolation, effectively increases the frame rate of the LiDAR sensor, allowing for more accurate and detailed tracking of objects.

The PointINet system works by first estimating the 3D movement of objects between the two input frames. It then uses this information to ""warp"" the frames to a specific intermediate time step. Finally, it combines the warped frames to create a new, intermediate point cloud frame.

The researchers tested PointINet on two large-scale outdoor LiDAR datasets and demonstrated its effectiveness in generating high-quality intermediate frames. This technology has the potential to enhance the performance of LiDAR sensors in various applications, including autonomous vehicles, robotics, and surveillance. The code for PointINet is now available open-source, allowing other researchers to build upon this innovation."
cs.CV,Prediction of Chronic Kidney Disease Using Deep Neural Network,"Deep neural Network (DNN) is becoming a focal point in Machine Learning research. Its application is penetrating into different fields and solving intricate and complex problems. DNN is now been applied in health image processing to detect various ailment such as cancer and diabetes. Another disease that is causing threat to our health is the kidney disease. This disease is becoming prevalent due to substances and elements we intake. Death is imminent and inevitable within few days without at least one functioning kidney. Ignoring the kidney malfunction can cause chronic kidney disease leading to death. Frequently, Chronic Kidney Disease (CKD) and its symptoms are mild and gradual, often go unnoticed for years only to be realized lately. Bade, a Local Government of Yobe state in Nigeria has been a center of attention by medical practitioners due to the prevalence of CKD. Unfortunately, a technical approach in culminating the disease is yet to be attained. We obtained a record of 400 patients with 10 attributes as our dataset from Bade General Hospital. We used DNN model to predict the absence or presence of CKD in the patients. The model produced an accuracy of 98%. Furthermore, we identified and highlighted the Features importance to provide the ranking of the features used in the prediction of the CKD. The outcome revealed that two attributes; Creatinine and Bicarbonate have the highest influence on the CKD prediction.",https://arxiv.org/pdf/2012.12089v1,2020-12-22T15:31:14Z,"Iliyas Ibrahim Iliyas, Isah Rambo Saidu, Ali Baba Dauda, Suleiman Tasiu","**Breakthrough in Predicting Chronic Kidney Disease using Artificial Intelligence**

Researchers have made a significant breakthrough in predicting Chronic Kidney Disease (CKD) using a type of artificial intelligence called Deep Neural Network (DNN). CKD is a serious health condition that can lead to death if left untreated. The disease often develops gradually, with mild symptoms that can go unnoticed for years.

In a study conducted in Nigeria, researchers collected data from 400 patients and used DNN to predict the presence or absence of CKD. The results were impressive, with the model achieving an accuracy of 98%. This means that the DNN model was able to correctly predict whether a patient had CKD or not in almost all cases.

The study also identified the most important factors that contribute to the prediction of CKD. The two most influential factors were found to be Creatinine and Bicarbonate levels in the blood. These findings can help doctors and healthcare professionals to better diagnose and treat CKD.

This study demonstrates the potential of artificial intelligence in healthcare, particularly in predicting and diagnosing complex diseases like CKD. The use of DNN can help doctors to detect CKD earlier, allowing for timely interventions and potentially saving lives."
cs.CV,A Deep Reinforcement Learning Approach for Ramp Metering Based on Traffic Video Data,"Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",https://arxiv.org/pdf/2012.12104v1,2020-12-09T05:08:41Z,"Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchuan Du","**Improving Traffic Flow with AI-Powered Ramp Metering**

Researchers have developed a new approach to managing traffic flow on highways using artificial intelligence (AI) and video data. The approach, called deep reinforcement learning, uses traffic cameras to monitor and control the flow of vehicles onto highways through on-ramps. By analyzing video footage, the AI system learns to optimize traffic signal timings to reduce congestion and travel times.

In a real-world test, the AI-powered system outperformed traditional methods, resulting in:

* Lower travel times on the main highway
* Shorter queues of vehicles waiting to merge onto the highway
* Higher traffic flows downstream of the merging area

This innovative approach has the potential to improve traffic efficiency and reduce congestion on highways by leveraging the wealth of data from traffic cameras. By using AI to analyze video data, traffic managers can make more informed decisions to optimize traffic flow and reduce travel times."
cs.CV,Image to Bengali Caption Generation Using Deep CNN and Bidirectional Gated Recurrent Unit,"There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.",https://arxiv.org/pdf/2012.12139v1,2020-12-22T16:22:02Z,"Al Momin Faruk, Hasan Al Faraby, Md. Muzahidul Azad, Md. Riduyan Fedous, Md. Kishor Morol","Here's a summary of the research paper for a general audience:

**Breaking the Language Barrier: Generating Bengali Captions from Images**

Imagine being able to understand and describe an image in your native language, even if you're visually impaired. Researchers have made a significant step towards making this possible for Bengali speakers, a language spoken by over 243 million people worldwide.

The team developed a new artificial intelligence (AI) model that can generate natural-sounding Bengali captions from images. This technology uses a combination of two powerful AI techniques: a deep convolutional neural network (CNN) to analyze the image, and a bidirectional gated recurrent unit (GRU) to generate the caption.

The researchers tested their model on a new dataset of 8,000 images with five captions each, called BNATURE. The results show that their model can generate high-quality Bengali captions that are similar to those written by humans. This technology has the potential to help Bengali speakers better understand and communicate with each other, and also assist visually impaired individuals in their daily lives.

The implications of this research are significant, as it can help bridge the language gap and make visual information more accessible to a wider audience. With further development, this technology could be used in various applications, such as image recognition, language translation, and assistive technologies for the visually impaired."
cs.CV,General Domain Adaptation Through Proportional Progressive Pseudo Labeling,"Domain adaptation helps transfer the knowledge gained from a labeled source domain to an unlabeled target domain. During the past few years, different domain adaptation techniques have been published. One common flaw of these approaches is that while they might work well on one input type, such as images, their performance drops when applied to others, such as text or time-series. In this paper, we introduce Proportional Progressive Pseudo Labeling (PPPL), a simple, yet effective technique that can be implemented in a few lines of code to build a more general domain adaptation technique that can be applied on several different input types. At the beginning of the training phase, PPPL progressively reduces target domain classification error, by training the model directly with pseudo-labeled target domain samples, while excluding samples with more likely wrong pseudo-labels from the training set and also postponing training on such samples. Experiments on 6 different datasets that include tasks such as anomaly detection, text sentiment analysis and image classification demonstrate that PPPL can beat other baselines and generalize better.",https://arxiv.org/pdf/2012.13028v1,2020-12-23T23:57:00Z,"Mohammad J. Hashemi, Eric Keller","**Improving Domain Adaptation: A Simple yet Effective Technique**

Domain adaptation is a technique that helps machines learn from one type of data and apply that knowledge to another, different type of data. For example, a machine learning model trained on images might not work well on text data. Researchers have proposed various domain adaptation techniques, but most of them have limitations and work well only on specific types of data.

A new technique called Proportional Progressive Pseudo Labeling (PPPL) has been introduced, which can be applied to various types of data, including images, text, and time-series data. PPPL works by gradually adding pseudo-labeled target data to the training set, while excluding samples that are likely to have incorrect labels. This approach helps to reduce errors and improve the model's performance on the target data.

**Key Benefits:**

* Simple to implement
* Effective across multiple data types
* Outperforms other domain adaptation techniques

**Real-World Applications:**

* Anomaly detection
* Text sentiment analysis
* Image classification

Overall, PPPL offers a promising solution for domain adaptation, enabling machines to learn from one type of data and apply that knowledge to another, with improved accuracy and generalizability."
cs.CV,Appearance-Invariant 6-DoF Visual Localization using Generative Adversarial Networks,"We propose a novel visual localization network when outside environment has changed such as different illumination, weather and season. The visual localization network is composed of a feature extraction network and pose regression network. The feature extraction network is made up of an encoder network based on the Generative Adversarial Network CycleGAN, which can capture intrinsic appearance-invariant feature maps from unpaired samples of different weathers and seasons. With such an invariant feature, we use a 6-DoF pose regression network to tackle long-term visual localization in the presence of outdoor illumination, weather and season changes. A variety of challenging datasets for place recognition and localization are used to prove our visual localization network, and the results show that our method outperforms state-of-the-art methods in the scenarios with various environment changes.",https://arxiv.org/pdf/2012.13191v1,2020-12-24T10:43:43Z,"Yimin Lin, Jianfeng Huang, Shiguo Lian","**Advances in Visual Localization: A Breakthrough in Navigation Technology**

Imagine being able to navigate through familiar places, even when the environment looks drastically different due to changes in lighting, weather, or season. Researchers have made a significant breakthrough in developing a visual localization system that can accurately determine a device's position and orientation in 3D space, despite these changes.

The innovative system uses a type of artificial intelligence called Generative Adversarial Networks (GANs) to extract features from images that remain consistent across different environmental conditions. This allows the system to recognize and localize a device's position with six degrees of freedom (6-DoF), even when the scene looks very different.

The researchers tested their system on several challenging datasets and found that it outperformed existing state-of-the-art methods in various scenarios, including changes in illumination, weather, and season. This technology has the potential to improve navigation systems in various applications, such as self-driving cars, drones, and augmented reality devices.

**In simple terms:** This new technology helps devices understand where they are and how they're oriented in 3D space, even when the environment looks different due to changes in lighting, weather, or season. It's a significant step forward in developing more accurate and reliable navigation systems."
cs.CV,"Underwater image filtering: methods, datasets and evaluation","Underwater images are degraded by the selective attenuation of light that distorts colours and reduces contrast. The degradation extent depends on the water type, the distance between an object and the camera, and the depth under the water surface the object is at. Underwater image filtering aims to restore or to enhance the appearance of objects captured in an underwater image. Restoration methods compensate for the actual degradation, whereas enhancement methods improve either the perceived image quality or the performance of computer vision algorithms. The growing interest in underwater image filtering methods--including learning-based approaches used for both restoration and enhancement--and the associated challenges call for a comprehensive review of the state of the art. In this paper, we review the design principles of filtering methods and revisit the oceanology background that is fundamental to identify the degradation causes. We discuss image formation models and the results of restoration methods in various water types. Furthermore, we present task-dependent enhancement methods and categorise datasets for training neural networks and for method evaluation. Finally, we discuss evaluation strategies, including subjective tests and quality assessment measures. We complement this survey with a platform ( https://puiqe.eecs.qmul.ac.uk/ ), which hosts state-of-the-art underwater filtering methods and facilitates comparisons.",https://arxiv.org/pdf/2012.12258v1,2020-12-22T18:56:39Z,"Chau Yi Li, Riccardo Mazzon, Andrea Cavallaro","**Improving Underwater Images: A Review of Techniques and Challenges**

Have you ever seen a blurry or discolored underwater photo? That's because water affects light in a way that degrades image quality. Researchers have been working on developing methods to improve underwater images, known as underwater image filtering. The goal is to restore or enhance the appearance of objects captured in underwater images.

**Why is it hard?**

Underwater images are affected by the type of water, distance from the object to the camera, and depth. This makes it challenging to develop effective filtering methods. Researchers have been exploring different approaches, including machine learning-based techniques, to address these challenges.

**What's being done?**

Researchers have been working on two main types of methods:

1. **Restoration methods**: These aim to compensate for the actual degradation caused by water, restoring the original image.
2. **Enhancement methods**: These improve the perceived image quality or help computer vision algorithms work better with underwater images.

**How are they evaluated?**

To assess the effectiveness of these methods, researchers use various evaluation strategies, including:

1. **Subjective tests**: Human evaluators rate the quality of the filtered images.
2. **Quality assessment measures**: Objective metrics are used to evaluate the improvement in image quality.

**A comprehensive platform**

To facilitate comparisons and advancements in underwater image filtering, researchers have created a platform (https://puiqe.eecs.qmul.ac.uk/) that hosts state-of-the-art filtering methods and allows for evaluations.

Overall, this review highlights the importance of underwater image filtering and the need for continued research in this area to improve our ability to capture high-quality underwater images."
cs.CV,Flexible deep transfer learning by separate feature embeddings and manifold alignment,"Object recognition is a key enabler across industry and defense. As technology changes, algorithms must keep pace with new requirements and data. New modalities and higher resolution sensors should allow for increased algorithm robustness. Unfortunately, algorithms trained on existing labeled datasets do not directly generalize to new data because the data distributions do not match. Transfer learning (TL) or domain adaptation (DA) methods have established the groundwork for transferring knowledge from existing labeled source data to new unlabeled target datasets. However, current DA approaches assume similar source and target feature spaces and suffer in the case of massive domain shifts or changes in the feature space. Existing methods assume the data are either the same modality, or can be aligned to a common feature space. Therefore, most methods are not designed to support a fundamental domain change such as visual to auditory data. We propose a novel deep learning framework that overcomes this limitation by learning separate feature extractions for each domain while minimizing the distance between the domains in a latent lower-dimensional space. The alignment is achieved by considering the data manifold along with an adversarial training procedure. We demonstrate the effectiveness of the approach versus traditional methods with several ablation experiments on synthetic, measured, and satellite image datasets. We also provide practical guidelines for training the network while overcoming vanishing gradients which inhibit learning in some adversarial training settings.",https://arxiv.org/pdf/2012.12302v1,2020-12-22T19:24:44Z,"Samuel Rivera, Joel Klipfel, Deborah Weeks","**Advancing Object Recognition: A Breakthrough in Deep Learning**

Imagine being able to recognize objects in images or videos, even if they're taken with a different camera or sensor, or if they're from a completely different environment. This is a crucial task in various industries, such as defense, healthcare, and self-driving cars. However, current computer algorithms struggle to adapt to new data, especially when it's significantly different from what they've been trained on.

Researchers have made progress in developing transfer learning methods, which allow algorithms to learn from existing data and apply that knowledge to new, unseen data. However, these methods have limitations, particularly when dealing with drastic changes in data, such as switching from images to audio signals.

To overcome this challenge, a team of researchers has proposed a novel deep learning framework that enables flexible transfer learning. Their approach learns separate feature extractions for each domain (e.g., images and audio) and aligns them in a lower-dimensional space. This allows the algorithm to recognize objects even when the data is fundamentally different.

The researchers tested their approach on various datasets, including synthetic, measured, and satellite images, and demonstrated its effectiveness compared to traditional methods. They also provided practical guidelines for training the network, overcoming common issues that can hinder learning.

This breakthrough has the potential to improve object recognition in various applications, enabling algorithms to adapt to new data and environments, and ultimately leading to more robust and accurate results."
cs.CV,Randomized RX for target detection,"This work tackles the target detection problem through the well-known global RX method. The RX method models the clutter as a multivariate Gaussian distribution, and has been extended to nonlinear distributions using kernel methods. While the kernel RX can cope with complex clutters, it requires a considerable amount of computational resources as the number of clutter pixels gets larger. Here we propose random Fourier features to approximate the Gaussian kernel in kernel RX and consequently our development keep the accuracy of the nonlinearity while reducing the computational cost which is now controlled by an hyperparameter. Results over both synthetic and real-world image target detection problems show space and time efficiency of the proposed method while providing high detection performance.",https://arxiv.org/pdf/2012.12308v1,2020-12-08T19:18:49Z,"Fatih Nar, Adrián Pérez-Suay, José Antonio Padrón, Gustau Camps-Valls","**Improving Target Detection with a Faster and More Efficient Method**

Researchers have developed a new approach to detect targets in images, such as objects or anomalies, using a technique called the ""RX method"". This method models the background (or ""clutter"") as a complex statistical distribution, allowing it to identify targets that stand out from the background.

The challenge with current methods is that they can be computationally expensive, especially when dealing with large images. To address this, the researchers proposed using a mathematical trick called ""random Fourier features"" to approximate a complex mathematical function, known as a Gaussian kernel. This trick allows the method to maintain its accuracy while significantly reducing the computational cost.

The results show that the new method is not only faster and more efficient, but also provides high detection performance on both simulated and real-world images. This improvement has the potential to enable faster and more accurate target detection in a variety of applications, such as surveillance, environmental monitoring, and search and rescue operations."
cs.CV,Multi-Task Multi-Sensor Fusion for 3D Object Detection,"In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and BEV object detection, while being real time.",https://arxiv.org/pdf/2012.12397v1,2020-12-22T22:49:15Z,"Ming Liang, Bin Yang, Yun Chen, Rui Hu, Raquel Urtasun","**Advances in 3D Object Detection: A Breakthrough in Autonomous Driving**

Imagine a world where self-driving cars can accurately detect and respond to their surroundings in real-time. A recent research paper has made significant strides towards making this vision a reality. The study proposes a new approach to 3D object detection, which enables computers to better understand their environment by combining data from multiple sensors.

The researchers developed a sophisticated system that can process information from various sources, such as cameras and lidar sensors, to detect objects in 3D space. What's innovative about this approach is that it simultaneously performs multiple related tasks, including:

* Detecting objects in 2D and 3D
* Estimating the ground plane
* Filling in gaps in depth information

By fusing information from these tasks, the system learns to create more accurate representations of its surroundings. The results are impressive: the system outperforms existing methods on a benchmark test (KITTI) and can operate in real-time.

This breakthrough has significant implications for autonomous driving, robotics, and other applications that require accurate 3D object detection. The researchers' approach has the potential to enhance the safety and efficiency of self-driving cars, making them more reliable and practical for everyday use."
cs.CV,Multi-Contrast Computed Tomography Healthy Kidney Atlas,"The construction of three-dimensional multi-modal tissue maps provides an opportunity to spur interdisciplinary innovations across temporal and spatial scales through information integration. While the preponderance of effort is allocated to the cellular level and explore the changes in cell interactions and organizations, contextualizing findings within organs and systems is essential to visualize and interpret higher resolution linkage across scales. There is a substantial normal variation of kidney morphometry and appearance across body size, sex, and imaging protocols in abdominal computed tomography (CT). A volumetric atlas framework is needed to integrate and visualize the variability across scales. However, there is no abdominal and retroperitoneal organs atlas framework for multi-contrast CT. Hence, we proposed a high-resolution CT retroperitoneal atlas specifically optimized for the kidney across non-contrast CT and early arterial, late arterial, venous and delayed contrast enhanced CT. Briefly, we introduce a deep learning-based volume of interest extraction method and an automated two-stage hierarchal registration pipeline to register abdominal volumes to a high-resolution CT atlas template. To generate and evaluate the atlas, multi-contrast modality CT scans of 500 subjects (without reported history of renal disease, age: 15-50 years, 250 males & 250 females) were processed. We demonstrate a stable generalizability of the atlas template for integrating the normal kidney variation from small to large, across contrast modalities and populations with great variability of demographics. The linkage of atlas and demographics provided a better understanding of the variation of kidney anatomy across populations.",https://arxiv.org/pdf/2012.12432v2,2020-12-23T01:03:16Z,"Ho Hin Lee, Yucheng Tang, Kaiwen Xu, Shunxing Bao, Agnes B. Fogo, Raymond Harris, Mark P. de Caestecker, Mattias Heinrich, Jeffrey M. Spraggins, Yuankai Huo, Bennett A. Landman","**Unlocking the Secrets of the Healthy Kidney: A Groundbreaking Atlas**

Imagine having a detailed map of a healthy kidney that can help doctors and researchers better understand how kidneys work and how they can be affected by different conditions. A team of researchers has created just that - a 3D atlas of the healthy kidney using advanced imaging technology called computed tomography (CT).

The atlas is special because it combines multiple images of the kidney taken at different times and with different levels of contrast, providing a more complete picture of this vital organ. The researchers used CT scans from 500 healthy individuals, ranging in age from 15 to 50, to create the atlas. They developed a sophisticated computer program to analyze and combine the images, allowing them to create a detailed and accurate map of the kidney.

The atlas reveals the natural variation in kidney shape and size that exists among healthy individuals, and how it relates to factors such as age, sex, and body size. This information can help doctors better understand what a healthy kidney looks like and identify potential problems earlier. The atlas can also serve as a valuable tool for researchers, enabling them to study the kidney in greater detail and gain new insights into how it functions.

The creation of this atlas marks an important step forward in the field of kidney research and could have significant implications for the diagnosis and treatment of kidney diseases. By providing a detailed and accurate map of the healthy kidney, this atlas has the potential to improve our understanding of kidney health and disease, and ultimately lead to better patient outcomes."
cs.CV,"Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast","Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well - typically requiring T1 scans (e.g., MP-RAGE). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing (""thick slice"") in clinical settings every year. The inability to quantitatively analyze these scans hinders the adoption of quantitative neuroimaging in healthcare, and precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in CNNs are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols - even within sites. Here we present SynthSR, a method to train a CNN that receives one or more thick-slice scans with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, e.g., skull stripping or bias field correction. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution training data. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at github.com/BBillot/SynthSR.",https://arxiv.org/pdf/2012.13340v1,2020-12-24T17:29:53Z,"Juan Eugenio Iglesias, Benjamin Billot, Yael Balbastre, Azadeh Tabari, John Conklin, Daniel C. Alexander, Polina Golland, Brian L. Edlow, Bruce Fischl","**Breakthrough in Brain Imaging Analysis**

Researchers have developed a new method called SynthSR, which enables the analysis of a large number of brain MRI scans that were previously difficult to study. These scans, taken in clinical settings, have limitations such as thick slices, varying orientations, and different contrasts, making it hard to analyze them quantitatively.

The SynthSR method uses a type of artificial intelligence called a convolutional neural network (CNN) to ""upgrade"" these limited scans to a higher quality, allowing for more accurate analysis. This is achieved by training the CNN on synthetic images generated from 3D brain segmentations, rather than requiring high-quality training data.

The results show that SynthSR can reliably generate high-quality images from low-quality scans, which can then be used for various downstream analyses, such as:

* Measuring the volume of brain structures
* Registering images for further analysis
* Even measuring the thickness of the brain's cortex

This breakthrough has the potential to unlock the analysis of millions of existing MRI scans, leading to a better understanding of the human brain and potentially improving healthcare outcomes. The source code for SynthSR is publicly available, making it accessible to researchers and clinicians worldwide."
cs.CV,Spatio-temporal Multi-task Learning for Cardiac MRI Left Ventricle Quantification,"Quantitative assessment of cardiac left ventricle (LV) morphology is essential to assess cardiac function and improve the diagnosis of different cardiovascular diseases. In current clinical practice, LV quantification depends on the measurement of myocardial shape indices, which is usually achieved by manual contouring of the endo- and epicardial. However, this process subjected to inter and intra-observer variability, and it is a time-consuming and tedious task. In this paper, we propose a spatio-temporal multi-task learning approach to obtain a complete set of measurements quantifying cardiac LV morphology, regional-wall thickness (RWT), and additionally detecting the cardiac phase cycle (systole and diastole) for a given 3D Cine-magnetic resonance (MR) image sequence. We first segment cardiac LVs using an encoder-decoder network and then introduce a multitask framework to regress 11 LV indices and classify the cardiac phase, as parallel tasks during model optimization. The proposed deep learning model is based on the 3D spatio-temporal convolutions, which extract spatial and temporal features from MR images. We demonstrate the efficacy of the proposed method using cine-MR sequences of 145 subjects and comparing the performance with other state-of-the-art quantification methods. The proposed method obtained high prediction accuracy, with an average mean absolute error (MAE) of 129 $mm^2$, 1.23 $mm$, 1.76 $mm$, Pearson correlation coefficient (PCC) of 96.4%, 87.2%, and 97.5% for LV and myocardium (Myo) cavity regions, 6 RWTs, 3 LV dimensions, and an error rate of 9.0\% for phase classification. The experimental results highlight the robustness of the proposed method, despite varying degrees of cardiac morphology, image appearance, and low contrast in the cardiac MR sequences.",https://arxiv.org/pdf/2012.13364v1,2020-12-24T17:48:35Z,"Sulaiman Vesal, Mingxuan Gu, Andreas Maier, Nishant Ravikumar","**Breakthrough in Cardiac MRI Analysis: AI-Powered Left Ventricle Quantification**

Researchers have developed a cutting-edge artificial intelligence (AI) approach to analyze cardiac magnetic resonance imaging (MRI) scans, which could revolutionize the diagnosis and treatment of cardiovascular diseases. The left ventricle, a crucial part of the heart, plays a vital role in pumping blood throughout the body. Accurate assessment of its shape and function is essential for detecting heart conditions.

The current manual method of measuring left ventricle morphology is time-consuming, tedious, and prone to errors. In contrast, the proposed AI-powered method uses a deep learning model to automatically segment the left ventricle, quantify its shape, and detect the cardiac phase cycle (systole and diastole) from 3D Cine-MRI image sequences.

**Key Findings:**

* The AI model achieved high prediction accuracy, with an average error of 129 mm² for left ventricle and myocardium cavity regions.
* The model demonstrated a strong correlation (96.4%, 87.2%, and 97.5%) with manual measurements for left ventricle and myocardium cavity regions, wall thickness, and dimensions.
* The AI approach also accurately classified the cardiac phase cycle with an error rate of 9.0%.

**Implications:**

* The proposed method has the potential to reduce the variability and time associated with manual contouring, enabling clinicians to make more accurate diagnoses and develop effective treatment plans.
* This AI-powered approach could lead to improved patient outcomes, reduced healthcare costs, and enhanced clinical decision-making.

Overall, this research presents a significant advancement in cardiac MRI analysis, offering a robust and accurate method for left ventricle quantification."
cs.CV,Skeleton-based Approaches based on Machine Vision: A Survey,"Recently, skeleton-based approaches have achieved rapid progress on the basis of great success in skeleton representation. Plenty of researches focus on solving specific problems according to skeleton features. Some skeleton-based approaches have been mentioned in several overviews on object detection as a non-essential part. Nevertheless, there has not been any thorough analysis of skeleton-based approaches attentively. Instead of describing these techniques in terms of theoretical constructs, we devote to summarizing skeleton-based approaches with regard to application fields and given tasks as comprehensively as possible. This paper is conducive to further understanding of skeleton-based application and dealing with particular issues.",https://arxiv.org/pdf/2012.12447v1,2020-12-23T02:03:37Z,"Jie Li, Binglin Li, Min Gao","Here's a summary of the research paper for a general audience:

**Unlocking the Power of Skeletons in Computer Vision**

Imagine being able to detect and analyze the shape and movement of objects, like humans or animals, using just their skeletal structure. This is the idea behind skeleton-based approaches in machine vision, a field that has seen rapid progress in recent years.

Researchers have been exploring ways to use skeletons to solve specific problems, such as object detection, tracking, and recognition. While some of these approaches have been mentioned in broader reviews of computer vision, there hasn't been a comprehensive analysis of skeleton-based methods.

This survey aims to fill that gap by summarizing the various skeleton-based approaches and their applications across different fields, such as robotics, healthcare, and security. By understanding how skeletons can be used to analyze visual data, researchers and developers can tackle specific challenges and create more accurate and efficient computer vision systems.

In simple terms, this paper provides a roadmap for understanding the exciting possibilities of skeleton-based approaches in machine vision and how they can be applied to real-world problems."
cs.CV,CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic Cholecystectomy Based on Cholec80,"Computer-assisted surgery has been developed to enhance surgery correctness and safety. However, researchers and engineers suffer from limited annotated data to develop and train better algorithms. Consequently, the development of fundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is limited. This article elaborates on the efforts of preparing the dataset for semantic segmentation, which is the foundation of many computer-assisted surgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080 laparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and annotated the images. The dataset is named CholecSeg8K and its total size is 3GB. Each of these images is annotated at pixel-level for thirteen classes, which are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k is released under the license CC BY- NC-SA 4.0.",https://arxiv.org/pdf/2012.12453v1,2020-12-23T02:23:15Z,"W. -Y. Hong, C. -L. Kao, Y. -H. Kuo, J. -R. Wang, W. -L. Chang, C. -S. Shih","Here's a summary of the research paper for a general audience:

**Advancing Computer-Assisted Surgery: A New Dataset for Laparoscopic Cholecystectomy**

Computer-assisted surgery is a rapidly growing field that aims to improve the accuracy and safety of surgical procedures. However, one major challenge is the lack of annotated data, which is necessary to train and develop better algorithms. To address this issue, researchers have created a new dataset called CholecSeg8k, which is specifically designed for laparoscopic cholecystectomy, a common surgical procedure to remove the gallbladder.

The dataset consists of 8,080 images extracted from 17 video clips, with each image annotated at the pixel level for 13 different classes, such as organs, tissues, and surgical instruments. This dataset, which is 3GB in size, provides a valuable resource for researchers and engineers to develop and train algorithms for computer-assisted surgery.

The creation of CholecSeg8k is an important step towards advancing computer-assisted surgery, as it lays the foundation for developing fundamental algorithms, such as Simultaneous Localization and Mapping (SLAM). By making this dataset publicly available under a Creative Commons license, researchers hope to facilitate collaboration and innovation in the field of computer-assisted surgery."
cs.CV,Unsupervised Domain Adaptation for Semantic Segmentation by Content Transfer,"In this paper, we tackle the unsupervised domain adaptation (UDA) for semantic segmentation, which aims to segment the unlabeled real data using labeled synthetic data. The main problem of UDA for semantic segmentation relies on reducing the domain gap between the real image and synthetic image. To solve this problem, we focused on separating information in an image into content and style. Here, only the content has cues for semantic segmentation, and the style makes the domain gap. Thus, precise separation of content and style in an image leads to effect as supervision of real data even when learning with synthetic data. To make the best of this effect, we propose a zero-style loss. Even though we perfectly extract content for semantic segmentation in the real domain, another main challenge, the class imbalance problem, still exists in UDA for semantic segmentation. We address this problem by transferring the contents of tail classes from synthetic to real domain. Experimental results show that the proposed method achieves the state-of-the-art performance in semantic segmentation on the major two UDA settings.",https://arxiv.org/pdf/2012.12545v1,2020-12-23T09:01:00Z,"Suhyeon Lee, Junhyuk Hyun, Hongje Seong, Euntai Kim","**Improving Computer Vision with Unsupervised Domain Adaptation**

Imagine you have a computer program that can accurately identify objects in pictures taken from a simulated environment, but struggles to do the same with real-world photos. This is a common problem in computer vision, known as domain adaptation. Researchers have proposed a new solution to address this challenge, specifically for a task called semantic segmentation, which involves identifying and labeling different objects within an image.

The researchers' approach focuses on separating the content and style of an image. The content refers to the objects and features that are relevant for semantic segmentation, while the style refers to the characteristics that distinguish one domain (e.g., synthetic images) from another (e.g., real-world images). By separating these two components, the researchers aim to leverage the labeled synthetic data to improve the performance of the model on unlabeled real-world data.

The proposed method, called ""content transfer,"" involves two key innovations:

1. **Zero-style loss**: This technique helps the model learn to ignore the style of the images and focus on the content, which is essential for semantic segmentation.
2. **Content transfer for tail classes**: The researchers also address the issue of class imbalance, where some objects are much rarer than others. They propose transferring the content of rare classes from synthetic to real-world images, which helps to improve the model's performance on these classes.

The results show that this approach achieves state-of-the-art performance on two common benchmarks for unsupervised domain adaptation in semantic segmentation. This research has the potential to improve computer vision applications, such as self-driving cars, medical imaging, and more, by enabling models to learn from simulated data and adapt to real-world scenarios."
cs.CV,Training DNNs in O(1) memory with MEM-DFA using Random Matrices,"This work presents a method for reducing memory consumption to a constant complexity when training deep neural networks. The algorithm is based on the more biologically plausible alternatives of the backpropagation (BP): direct feedback alignment (DFA) and feedback alignment (FA), which use random matrices to propagate error. The proposed method, memory-efficient direct feedback alignment (MEM-DFA), uses higher independence of layers in DFA and allows avoiding storing at once all activation vectors, unlike standard BP, FA, and DFA. Thus, our algorithm's memory usage is constant regardless of the number of layers in a neural network. The method increases the computational cost only by a constant factor of one extra forward pass.   The MEM-DFA, BP, FA, and DFA were evaluated along with their memory profiles on MNIST and CIFAR-10 datasets on various neural network models. Our experiments agree with our theoretical results and show a significant decrease in the memory cost of MEM-DFA compared to the other algorithms.",https://arxiv.org/pdf/2012.11745v1,2020-12-21T23:27:40Z,"Tien Chu, Kamil Mykitiuk, Miron Szewczyk, Adam Wiktor, Zbigniew Wojna","**Breakthrough in Training Artificial Intelligence: Reduced Memory Usage**

Imagine training a huge artificial brain, like those used in self-driving cars or voice assistants, without running out of computer memory. Researchers have made a significant breakthrough in this area by developing a new method called MEM-DFA. This method allows for training deep neural networks, a type of artificial intelligence, using a constant amount of memory, regardless of the network's size.

The innovation is based on a more biologically inspired way of teaching neural networks, called direct feedback alignment (DFA). This approach uses random matrices to help the network learn from its mistakes. The new method, MEM-DFA, takes this idea a step further by allowing the network to learn in a more independent and memory-efficient way.

The researchers tested MEM-DFA on several large datasets, including images of handwritten digits (MNIST) and street scenes (CIFAR-10). They found that MEM-DFA uses significantly less memory than other methods, while only requiring a small extra computational effort. This achievement has the potential to enable the training of more complex and powerful artificial intelligence models, even on devices with limited memory, such as smartphones or smart home devices.

**Key Takeaways:**

* A new method, MEM-DFA, reduces memory usage to a constant level when training deep neural networks.
* This approach uses a biologically inspired way of teaching neural networks, called direct feedback alignment (DFA).
* MEM-DFA uses significantly less memory than other methods, while only requiring a small extra computational effort.
* This breakthrough enables the training of more complex and powerful artificial intelligence models on devices with limited memory."
cs.CV,Deep Continuous Fusion for Multi-Sensor 3D Object Detection,"In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.",https://arxiv.org/pdf/2012.10992v1,2020-12-20T18:43:41Z,"Ming Liang, Bin Yang, Shenlong Wang, Raquel Urtasun","**Advancing 3D Object Detection with Multi-Sensor Technology**

Imagine a world where self-driving cars and robots can accurately detect and respond to their surroundings. A team of researchers has made significant progress towards this goal by developing a new system that combines data from cameras and LIDAR (Light Detection and Ranging) sensors to detect 3D objects.

The system, called Deep Continuous Fusion, uses a novel approach to fuse data from both cameras and LIDAR sensors, allowing it to accurately locate objects in 3D space. This approach enables the system to leverage the strengths of both sensors: cameras provide detailed visual information, while LIDAR sensors provide precise distance and spatial information.

The researchers tested their system on two large datasets and achieved state-of-the-art results, demonstrating significant improvements over existing methods. This breakthrough has the potential to enhance the safety and reliability of autonomous vehicles and robotics applications, and could pave the way for more efficient and effective 3D object detection systems."
cs.CV,Deep Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose Estimation,"In this work, we introduce Deep Bingham Networks (DBN), a generic framework that can naturally handle pose-related uncertainties and ambiguities arising in almost all real life applications concerning 3D data. While existing works strive to find a single solution to the pose estimation problem, we make peace with the ambiguities causing high uncertainty around which solutions to identify as the best. Instead, we report a family of poses which capture the nature of the solution space. DBN extends the state of the art direct pose regression networks by (i) a multi-hypotheses prediction head which can yield different distribution modes; and (ii) novel loss functions that benefit from Bingham distributions on rotations. This way, DBN can work both in unambiguous cases providing uncertainty information, and in ambiguous scenes where an uncertainty per mode is desired. On a technical front, our network regresses continuous Bingham mixture models and is applicable to both 2D data such as images and to 3D data such as point clouds. We proposed new training strategies so as to avoid mode or posterior collapse during training and to improve numerical stability. Our methods are thoroughly tested on two different applications exploiting two different modalities: (i) 6D camera relocalization from images; and (ii) object pose estimation from 3D point clouds, demonstrating decent advantages over the state of the art. For the former we contributed our own dataset composed of five indoor scenes where it is unavoidable to capture images corresponding to views that are hard to uniquely identify. For the latter we achieve the top results especially for symmetric objects of ModelNet dataset.",https://arxiv.org/pdf/2012.11002v1,2020-12-20T19:20:26Z,"Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, Tolga Birdal","**Understanding Uncertainty in 3D Pose Estimation**

Imagine trying to determine the exact position and orientation of an object in 3D space, like a camera or a chair. This task, called pose estimation, is crucial in various applications, such as robotics, computer vision, and augmented reality. However, in real-life scenarios, there are often uncertainties and ambiguities, making it challenging to find a single, precise solution.

**The Problem with Traditional Methods**

Traditional methods for pose estimation try to find one ""best"" solution, but they often struggle with situations where there are multiple possible solutions or where the data is noisy or unclear. For example, if you take a picture of a room from a certain angle, it might be hard to tell exactly where the camera is or how it's oriented.

**Introducing Deep Bingham Networks**

Researchers have developed a new approach called Deep Bingham Networks (DBN), which acknowledges and handles these uncertainties. Instead of providing a single solution, DBN reports a range of possible poses, capturing the complexity of the solution space. This approach is particularly useful in situations where there are multiple possible solutions or where the data is ambiguous.

**How DBN Works**

DBN extends existing pose estimation methods by:

1. Predicting multiple possible poses, rather than a single one.
2. Using a novel mathematical framework (Bingham distributions) to represent the uncertainties in 3D rotations.

**Advantages and Applications**

DBN has been tested on two applications:

1. **6D camera relocalization**: DBN outperforms existing methods in determining the position and orientation of a camera in a room, especially in situations where the view is ambiguous.
2. **Object pose estimation**: DBN achieves top results in estimating the pose of objects from 3D point clouds, particularly for symmetric objects.

**The Impact of DBN**

By handling uncertainty and ambiguity in pose estimation, DBN has the potential to improve various applications, such as:

* Robotics: more accurate and robust object manipulation and navigation.
* Computer vision: better understanding of 3D scenes and objects.
* Augmented reality: more precise and stable tracking of objects and environments.

Overall, DBN offers a more robust and accurate approach to pose estimation, acknowledging the complexities and uncertainties of real-life scenarios."
cs.AI,A Deep Reinforcement Learning Approach for Ramp Metering Based on Traffic Video Data,"Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",https://arxiv.org/pdf/2012.12104v1,2020-12-09T05:08:41Z,"Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchuan Du","**Improving Traffic Flow with AI-Powered Ramp Metering**

Researchers have developed a new approach to managing traffic flow on highways using artificial intelligence (AI) and video data. The approach, called deep reinforcement learning, uses traffic cameras to collect video data and optimize traffic signal timings to regulate the flow of vehicles onto the highway.

Traditionally, traffic signals are controlled using data from sensors that measure traffic volume and occupancy. However, this method has limitations, as sensors only provide information about a small section of the road. In contrast, traffic cameras can capture a wider area and provide more detailed information about traffic conditions.

The researchers tested their approach in a real-world setting and found that it resulted in:

* Lower travel times on the highway
* Shorter vehicle queues at on-ramps
* Higher traffic flows downstream of merging areas

These findings suggest that using AI and video data can improve the efficiency of traffic management and reduce congestion on highways. This approach has the potential to be widely adopted and could lead to smoother, faster commutes for drivers."
cs.AI,Rethink AI-based Power Grid Control: Diving Into Algorithm Design,"Recently, deep reinforcement learning (DRL)-based approach has shown promisein solving complex decision and control problems in power engineering domain.In this paper, we present an in-depth analysis of DRL-based voltage control fromaspects of algorithm selection, state space representation, and reward engineering.To resolve observed issues, we propose a novel imitation learning-based approachto directly map power grid operating points to effective actions without any interimreinforcement learning process. The performance results demonstrate that theproposed approach has strong generalization ability with much less training time.The agent trained by imitation learning is effective and robust to solve voltagecontrol problem and outperforms the former RL agents.",https://arxiv.org/pdf/2012.13026v1,2020-12-23T23:38:41Z,"Xiren Zhou, Siqi Wang, Ruisheng Diao, Desong Bian, Jiahui Duan, Di Shi","**Breakthrough in Power Grid Control: A New Approach to Artificial Intelligence**

Researchers have made a significant advancement in using artificial intelligence (AI) to control power grids. For years, AI has been used to solve complex problems in power engineering, but its application in power grid control has been limited. A new study presents a novel approach that uses imitation learning, a type of AI, to improve voltage control in power grids.

**The Problem: Voltage Control**

Voltage control is crucial to ensure the stability and efficiency of power grids. However, it is a complex task that requires swift and accurate decision-making. Traditional AI-based approaches, known as deep reinforcement learning (DRL), have shown promise but have limitations, including requiring extensive training time and struggling to generalize to new situations.

**The Solution: Imitation Learning**

The researchers propose a new approach using imitation learning, which enables the AI agent to learn from expert operators and directly map power grid operating points to effective actions. This approach eliminates the need for a lengthy reinforcement learning process.

**The Results: Improved Performance**

The results show that the imitation learning-based approach outperforms traditional DRL-based methods, demonstrating strong generalization ability and requiring much less training time. This means that the AI agent can adapt to new situations and make effective decisions quickly, making it a robust and effective solution for voltage control in power grids.

**Impact and Future Directions**

This breakthrough has significant implications for the future of power grid control. The proposed approach can lead to more efficient, stable, and reliable power grids, which is essential for modern society. As the demand for electricity continues to grow, innovative solutions like imitation learning will play a crucial role in shaping the future of power grid control."
cs.AI,Fuzzy Commitments Offer Insufficient Protection to Biometric Templates Produced by Deep Learning,"In this work, we study the protection that fuzzy commitments offer when they are applied to facial images, processed by the state of the art deep learning facial recognition systems. We show that while these systems are capable of producing great accuracy, they produce templates of too little entropy. As a result, we present a reconstruction attack that takes a protected template, and reconstructs a facial image. The reconstructed facial images greatly resemble the original ones. In the simplest attack scenario, more than 78% of these reconstructed templates succeed in unlocking an account (when the system is configured to 0.1% FAR). Even in the ""hardest"" settings (in which we take a reconstructed image from one system and use it in a different system, with different feature extraction process) the reconstructed image offers 50 to 120 times higher success rates than the system's FAR.",https://arxiv.org/pdf/2012.13293v1,2020-12-24T15:28:33Z,"Danny Keller, Margarita Osadchy, Orr Dunkelman","**Biometric Security Alert: Facial Recognition Systems May Not Be as Secure as Thought**

Researchers have raised concerns about the security of facial recognition systems that use a technique called ""fuzzy commitments"" to protect sensitive biometric data. Fuzzy commitments are designed to safeguard facial recognition templates, which are essentially digital representations of a person's face.

The study found that facial recognition systems powered by deep learning technology, which are highly accurate, produce templates with surprisingly low levels of randomness (or entropy). This makes them vulnerable to a type of attack called a ""reconstruction attack,"" where a hacker can take a protected template and use it to recreate a person's facial image.

In a simulated attack, the researchers were able to reconstruct facial images that closely resembled the originals. In some cases, these reconstructed images were able to ""unlock"" an account with a success rate of over 78%. Even in more challenging scenarios, the reconstructed images were 50 to 120 times more likely to succeed than the system's intended failure rate.

The findings suggest that fuzzy commitments may not provide sufficient protection for biometric templates produced by deep learning-based facial recognition systems. This raises important questions about the security and reliability of these systems, which are increasingly being used in various applications, including identity verification and authentication."
cs.AI,Generalization in portfolio-based algorithm selection,"Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector's average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learning-theoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm's performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting.",https://arxiv.org/pdf/2012.13315v1,2020-12-24T16:33:17Z,"Maria-Florina Balcan, Tuomas Sandholm, Ellen Vitercik","**Improving Algorithm Performance: A New Approach to Balancing Customization and Reliability**

Imagine you're trying to choose the best tool for a specific job from a collection of tools. In computer science, this is similar to selecting the best algorithm (a set of instructions) for a particular problem. A popular approach called portfolio-based algorithm selection has been successful in practice. It works by:

1. Creating a ""portfolio"" of different algorithm settings (like a collection of tools).
2. Training a ""selector"" to choose the best algorithm setting from the portfolio for a given problem.

However, a key challenge is ensuring that the selector performs well on new, unseen problems. Researchers have now developed a mathematical framework to analyze this challenge. They found that:

* If the portfolio is too large, it's difficult to avoid ""overfitting,"" where the selector becomes too specialized to the training data and performs poorly on new problems.
* The size of the training dataset (the set of problems used to train the selector) is crucial in ensuring that the selector's performance on the training data is similar to its performance on new problems.

The researchers' analysis reveals a trade-off: a larger portfolio offers more options for solving different problems, but it also increases the risk of overfitting. This work provides a foundation for developing more reliable and effective algorithm selection methods, which can lead to improved performance in various applications."
cs.AI,"I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling","To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.",https://arxiv.org/pdf/2012.13391v2,2020-12-24T18:47:49Z,"Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston","Here's a summary of the research paper for a general audience:

**Improving Chatbots' Consistency**

Imagine having a conversation with a chatbot and saying something like, ""I like fish, especially dolphins."" A chatbot that understands the conversation should recognize that dolphins are actually a type of fish, and therefore, there's a contradiction in your statement. But can chatbots really understand these kinds of nuances?

Researchers have created a new task called DECODE (DialoguE COntradiction DEtection) to test how well chatbots can detect contradictions in conversations. They collected a dataset of conversations, both between humans and between humans and chatbots, where contradictory statements were made.

The researchers found that a new approach to detecting contradictions, which breaks down conversations into individual statements, works better than traditional methods. This approach is more robust and can even handle conversations that are a bit unusual or unexpected.

The researchers also showed that their best model for detecting contradictions agrees well with human judgments, and can be used to automatically evaluate and improve the consistency of chatbots. This work has the potential to make chatbots more reliable and trustworthy in their conversations with humans."
cs.AI,Skeleton-based Approaches based on Machine Vision: A Survey,"Recently, skeleton-based approaches have achieved rapid progress on the basis of great success in skeleton representation. Plenty of researches focus on solving specific problems according to skeleton features. Some skeleton-based approaches have been mentioned in several overviews on object detection as a non-essential part. Nevertheless, there has not been any thorough analysis of skeleton-based approaches attentively. Instead of describing these techniques in terms of theoretical constructs, we devote to summarizing skeleton-based approaches with regard to application fields and given tasks as comprehensively as possible. This paper is conducive to further understanding of skeleton-based application and dealing with particular issues.",https://arxiv.org/pdf/2012.12447v1,2020-12-23T02:03:37Z,"Jie Li, Binglin Li, Min Gao","Here's a summary of the research paper for a general audience:

**Understanding Skeletons through Machine Vision**

Imagine being able to analyze and understand the shape and movement of objects, people, or animals using just their skeletal structure. This is the focus of a new survey on ""skeleton-based approaches"" in machine vision.

Researchers have made significant progress in representing skeletons, which are the internal frameworks of objects or living beings. By studying these skeletons, scientists can solve specific problems, such as detecting objects or tracking movements.

While some studies have touched on skeleton-based approaches, there hasn't been a comprehensive review of these techniques. This survey aims to change that by summarizing the various ways skeleton-based approaches are used in different fields and applications.

The goal of this research is to help scientists and engineers better understand how to apply skeleton-based approaches to tackle specific challenges. By providing a thorough overview of these techniques, the survey hopes to inspire new ideas and innovations in machine vision and related fields."
cs.AI,Overview of FPGA deep learning acceleration based on convolutional neural network,"In recent years, deep learning has become more and more mature, and as a commonly used algorithm in deep learning, convolutional neural networks have been widely used in various visual tasks. In the past, research based on deep learning algorithms mainly relied on hardware such as GPUs and CPUs. However, with the increasing development of FPGAs, both field programmable logic gate arrays, it has become the main implementation hardware platform that combines various neural network deep learning algorithms This article is a review article, which mainly introduces the related theories and algorithms of convolution. It summarizes the application scenarios of several existing FPGA technologies based on convolutional neural networks, and mainly introduces the application of accelerators. At the same time, it summarizes some accelerators' under-utilization of logic resources or under-utilization of memory bandwidth, so that they can't get the best performance.",https://arxiv.org/pdf/2012.12634v1,2020-12-23T12:44:24Z,Simin Liu,"Here's a summary of the research paper for a general audience:

**Unlocking the Power of Deep Learning with FPGAs**

Deep learning, a subset of artificial intelligence, has made tremendous progress in recent years. One key technique used in deep learning is the convolutional neural network (CNN), which is particularly effective for image and video analysis. To make CNNs run efficiently, researchers have traditionally relied on powerful computer chips like GPUs and CPUs.

However, with the advancement of Field-Programmable Gate Arrays (FPGAs), a new type of hardware, researchers are now exploring its potential to accelerate deep learning. FPGAs are integrated circuits that can be programmed and reprogrammed to perform specific tasks.

This review article provides an overview of how FPGAs can be used to speed up CNNs. It covers the basics of CNNs, summarizes existing FPGA-based technologies, and highlights the challenges of optimizing FPGA performance. The authors note that some current FPGA-based accelerators are not fully utilizing their resources, leading to suboptimal performance.

In essence, this paper explores the exciting possibilities of using FPGAs to unlock the full potential of deep learning, and identifies areas for improvement to make these technologies more efficient and effective."
cs.AI,Modelling Human Routines: Conceptualising Social Practice Theory for Agent-Based Simulation,"Our routines play an important role in a wide range of social challenges such as climate change, disease outbreaks and coordinating staff and patients in a hospital. To use agent-based simulations (ABS) to understand the role of routines in social challenges we need an agent framework that integrates routines. This paper provides the domain-independent Social Practice Agent (SoPrA) framework that satisfies requirements from the literature to simulate our routines. By choosing the appropriate concepts from the literature on agent theory, social psychology and social practice theory we ensure SoPrA correctly depicts current evidence on routines. By creating a consistent, modular and parsimonious framework suitable for multiple domains we enhance the usability of SoPrA. SoPrA provides ABS researchers with a conceptual, formal and computational framework to simulate routines and gain new insights into social systems.",https://arxiv.org/pdf/2012.11903v1,2020-12-22T10:06:47Z,"Rijk Mercuur, Virginia Dignum, Catholijn M. Jonker","**Understanding Human Routines through Computer Simulations**

Human routines play a significant role in various social challenges, such as addressing climate change, preventing disease outbreaks, and managing healthcare. Researchers are using computer simulations, specifically agent-based simulations (ABS), to study how routines affect these challenges. However, to accurately simulate human behavior, they need a framework that can model routines.

A new study introduces the Social Practice Agent (SoPrA) framework, which integrates theories from social psychology, social practice, and agent theory. SoPrA provides a comprehensive and flexible framework for simulating human routines in various contexts. By using SoPrA, researchers can create computer simulations that accurately depict how people behave in different situations, gaining new insights into complex social systems.

The SoPrA framework has the potential to enhance our understanding of social challenges and inform strategies to address them. For example, it could help researchers:

* Identify effective ways to promote environmentally friendly behaviors to mitigate climate change
* Develop targeted interventions to prevent disease outbreaks
* Optimize staff and patient coordination in hospitals

By providing a consistent, modular, and easy-to-use framework, SoPrA can help researchers simulate and analyze human routines, ultimately leading to better solutions for social challenges."
cs.AI,Dynamic-K Recommendation with Personalized Decision Boundary,"In this paper, we investigate the recommendation task in the most common scenario with implicit feedback (e.g., clicks, purchases). State-of-the-art methods in this direction usually cast the problem as to learn a personalized ranking on a set of items (e.g., webpages, products). The top-N results are then provided to users as recommendations, where the N is usually a fixed number pre-defined by the system according to some heuristic criteria (e.g., page size, screen size). There is one major assumption underlying this fixed-number recommendation scheme, i.e., there are always sufficient relevant items to users' preferences. Unfortunately, this assumption may not always hold in real-world scenarios. In some applications, there might be very limited candidate items to recommend, and some users may have very high relevance requirement in recommendation. In this way, even the top-1 ranked item may not be relevant to a user's preference. Therefore, we argue that it is critical to provide a dynamic-K recommendation, where the K should be different with respect to the candidate item set and the target user. We formulate this dynamic-K recommendation task as a joint learning problem with both ranking and classification objectives. The ranking objective is the same as existing methods, i.e., to create a ranking list of items according to users' interests. The classification objective is unique in this work, which aims to learn a personalized decision boundary to differentiate the relevant items from irrelevant items. Based on these ideas, we extend two state-of-the-art ranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding dynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two datasets show that the dynamic-K models are more effective than the original fixed-N recommendation methods.",https://arxiv.org/pdf/2012.13569v1,2020-12-25T13:02:57Z,"Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao","**Personalized Recommendations Just Got More Accurate**

Imagine you're browsing online for a new TV. You click on a few models, and then a recommendation system suggests more TVs based on your interests. But have you ever wondered why you're shown a fixed number of recommendations, say 5 or 10, regardless of how many options are available or how picky you are?

Researchers have found that traditional recommendation systems make an assumption that may not always be true: that there are always enough relevant options to show users. However, in some cases, there may be very few options available, or users may have very specific preferences. To address this issue, the researchers propose a new approach called ""dynamic-K"" recommendation.

**What is Dynamic-K Recommendation?**

Dynamic-K recommendation adjusts the number of recommendations shown to each user based on their individual preferences and the available options. For example, if you're a very picky shopper, you might only be shown 1 or 2 highly relevant options, while a less picky shopper might be shown 5 or 10 options.

The researchers developed new algorithms, DK-BPRMF and DK-HRM, which learn to rank items in order of relevance and also set a personalized decision boundary to determine when to stop recommending items. This approach allows the system to adapt to individual users' needs and provide more accurate recommendations.

**Key Benefits**

The dynamic-K approach has two main advantages:

1. **More accurate recommendations**: By adjusting the number of recommendations based on individual preferences and available options, the system can provide more relevant suggestions.
2. **Improved user experience**: Users are shown only the most relevant options, reducing the likelihood of being overwhelmed by too many choices.

**The Results**

The researchers tested their approach on two datasets and found that the dynamic-K models outperformed traditional fixed-number recommendation methods. This suggests that dynamic-K recommendation has the potential to improve online recommendation systems, making them more personalized and effective."
cs.AI,Compliance Generation for Privacy Documents under GDPR: A Roadmap for Implementing Automation and Machine Learning,"Most prominent research today addresses compliance with data protection laws through consumer-centric and public-regulatory approaches. We shift this perspective with the Privatech project to focus on corporations and law firms as agents of compliance. To comply with data protection laws, data processors must implement accountability measures to assess and document compliance in relation to both privacy documents and privacy practices. In this paper, we survey, on the one hand, current research on GDPR automation, and on the other hand, the operational challenges corporations face to comply with GDPR, and that may benefit from new forms of automation. We attempt to bridge the gap. We provide a roadmap for compliance assessment and generation by identifying compliance issues, breaking them down into tasks that can be addressed through machine learning and automation, and providing notes about related developments in the Privatech project.",https://arxiv.org/pdf/2012.12718v1,2020-12-23T14:46:51Z,"David Restrepo Amariles, Aurore Clément Troussel, Rajaa El Hamdani","**Simplifying Compliance with Data Protection Laws**

The General Data Protection Regulation (GDPR) is a set of rules that protects people's personal data. Companies and law firms must follow these rules to avoid fines and maintain trust with their customers. However, complying with GDPR can be a daunting task, especially when it comes to creating and managing complex privacy documents.

Researchers have proposed a new approach to help companies and law firms comply with GDPR. The Privatech project focuses on automating and streamlining the compliance process using machine learning and automation. The goal is to make it easier for companies to assess and document their compliance with GDPR.

The researchers surveyed current research on GDPR automation and identified the challenges companies face in complying with GDPR. They then created a roadmap for compliance assessment and generation, breaking down the process into tasks that can be addressed through machine learning and automation.

**Key Takeaways:**

* The Privatech project aims to help companies and law firms comply with GDPR through automation and machine learning.
* The researchers identified the challenges companies face in complying with GDPR and created a roadmap for compliance assessment and generation.
* The goal is to simplify the compliance process and reduce the burden on companies and law firms.

**What does this mean for you?**

If you're a business owner or work in a law firm, this research could lead to more efficient and effective ways to comply with GDPR. This could result in cost savings, reduced administrative burdens, and improved data protection for your customers. Ultimately, this could help build trust and confidence in the use of personal data."
cs.AI,PaXNet: Dental Caries Detection in Panoramic X-ray using Ensemble Transfer Learning and Capsule Classifier,"Dental caries is one of the most chronic diseases involving the majority of the population during their lifetime. Caries lesions are typically diagnosed by radiologists relying only on their visual inspection to detect via dental x-rays. In many cases, dental caries is hard to identify using x-rays and can be misinterpreted as shadows due to different reasons such as low image quality. Hence, developing a decision support system for caries detection has been a topic of interest in recent years. Here, we propose an automatic diagnosis system to detect dental caries in Panoramic images for the first time, to the best of authors' knowledge. The proposed model benefits from various pretrained deep learning models through transfer learning to extract relevant features from x-rays and uses a capsule network to draw prediction results. On a dataset of 470 Panoramic images used for features extraction, including 240 labeled images for classification, our model achieved an accuracy score of 86.05\% on the test set. The obtained score demonstrates acceptable detection performance and an increase in caries detection speed, as long as the challenges of using Panoramic x-rays of real patients are taken into account. Among images with caries lesions in the test set, our model acquired recall scores of 69.44\% and 90.52\% for mild and severe ones, confirming the fact that severe caries spots are more straightforward to detect and efficient mild caries detection needs a more robust and larger dataset. Considering the novelty of current research study as using Panoramic images, this work is a step towards developing a fully automated efficient decision support system to assist domain experts.",https://arxiv.org/pdf/2012.13666v1,2020-12-26T03:00:35Z,"Arman Haghanifar, Mahdiyar Molahasani Majdabadi, Seok-Bum Ko","**Breakthrough in Dental Caries Detection: AI-Powered System Shows Promise**

Dental caries, also known as tooth decay, is a common chronic disease affecting a large portion of the population. Detecting caries lesions using dental X-rays can be challenging, even for experienced radiologists. To address this issue, researchers have developed an innovative AI-powered system called PaXNet, which uses machine learning to automatically detect dental caries in panoramic X-ray images.

**How it works**

PaXNet leverages pre-trained deep learning models to extract relevant features from X-ray images and a capsule network to make predictions. In a test dataset of 470 panoramic images, the system achieved an accuracy score of 86.05%, demonstrating promising detection performance. The model was particularly effective in detecting severe caries lesions, but struggled to identify mild cases, highlighting the need for a more robust and larger dataset.

**What's next**

This study marks a significant step towards developing a fully automated decision support system to assist dental experts in detecting caries lesions. The use of panoramic X-ray images is a novel approach, and the results show that PaXNet has the potential to improve caries detection speed and accuracy. Further research is needed to refine the system and address the challenges of using real-world patient data. If successful, PaXNet could become a valuable tool for dental professionals, enabling them to diagnose and treat dental caries more effectively."
cs.AI,Toward Compact Data from Big Data,Bigdata is a dataset of which size is beyond the ability of handling a valuable raw material that can be refined and distilled into valuable specific insights. Compact data is a method that optimizes the big dataset that gives best assets without handling complex bigdata. The compact dataset contains the maximum knowledge patterns at fine grained level for effective and personalized utilization of bigdata systems without bigdata. The compact data method is a tailor-made design which depends on problem situations. Various compact data techniques have been demonstrated into various data-driven research area in the paper.,https://arxiv.org/pdf/2012.13677v1,2020-12-26T04:45:40Z,"Song-Kyoo, Kim","Here's a summary of the research paper for a general audience:

**From Big Data to Smart Data: Making Insights More Accessible**

Imagine having a huge library with an overwhelming number of books. You want to extract valuable information, but it's hard to sift through everything. That's where ""big data"" comes in - massive amounts of information that are difficult to manage.

Researchers have come up with a solution called ""compact data,"" which aims to distill big data into a more manageable and useful form. Compact data is a tailored approach that extracts the most important insights from big data, allowing for more efficient and personalized use.

Think of it like refining crude oil into specific, valuable products. By applying compact data techniques, researchers can uncover hidden patterns and insights without having to handle the massive amounts of raw data. This approach has been successfully applied to various fields, making it easier to extract valuable information from big data.

In short, compact data is a way to make big data more accessible, efficient, and useful, enabling us to gain deeper insights without getting overwhelmed by the sheer volume of information."
cs.AI,Towards sample-efficient episodic control with DAC-ML,"The sample-inefficiency problem in Artificial Intelligence refers to the inability of current Deep Reinforcement Learning models to optimize action policies within a small number of episodes. Recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in Episodic Reinforcement Learning. However, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. In this paper, we capitalize on the design principles of the Distributed Adaptive Control (DAC) theory of mind and brain to build a novel cognitive architecture (DAC-ML) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.",https://arxiv.org/pdf/2012.13779v1,2020-12-26T16:38:08Z,"Ismael T. Freire, Adrián F. Amil, Vasiliki Vouloutsi, Paul F. M. J. Verschure","**Breakthrough in Artificial Intelligence: A New Approach to Learning**

Imagine being able to learn a new skill or task with just a few tries, like humans do. Currently, artificial intelligence (AI) systems require a huge amount of data and practice to master a task, which is a major limitation. Researchers have been trying to improve this by adding memory systems to AI models, but progress has been slow.

In a new study, scientists have developed a novel approach called DAC-ML, inspired by the human brain's ability to learn quickly. By incorporating a memory system similar to the hippocampus, a part of the brain that helps us form memories, DAC-ML can rapidly learn to make effective decisions in a challenging task. This breakthrough has the potential to make AI systems more efficient and capable of learning like humans do. The study's findings could lead to significant advancements in areas like robotics, gaming, and autonomous vehicles."
cs.AI,My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism,"Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We find that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups' where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. We also find that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as ``the world is flat'' actually increases the score instead of decreasing it.",https://arxiv.org/pdf/2012.13872v1,2020-12-27T06:19:20Z,"Swapnil Parekh, Yaman Kumar Singla, Changyou Chen, Junyi Jessy Li, Rajiv Ratn Shah","**The Dark Side of Automated Essay Scoring: A Study Reveals Flaws**

Imagine taking a test and having your essay graded by a computer. Sounds efficient, right? But what if that computer doesn't truly understand what you're writing? A recent study titled ""My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism"" explored the inner workings of Automated Essay Scoring (AES) systems, which are used to grade essays. The study aimed to understand how these systems work and whether they can be tricked.

The researchers found some surprising flaws. They discovered that these systems don't truly comprehend the meaning of the text, but instead focus on a few key words. For example, if you write an essay about a topic but remove the context around a few important words, the system may still give you a good grade, even if the essay doesn't make sense. This is because the systems don't consider the essay as a cohesive piece of writing with a natural flow of speech and grammatical structure. Instead, they treat it like a ""word-soup"" where a few words are more important than others.

But that's not all. The study also found that these systems can be tricked by adding false information. For instance, if you write an essay that includes the phrase ""the world is flat,"" the system might actually give you a higher score, rather than a lower one. This is because the systems are not grounded in real-world knowledge and common sense.

The study's findings have significant implications. With millions of people relying on these systems for important decisions, such as college admissions or job applications, it's crucial that we understand their limitations. The researchers hope that their work will lead to the development of more sophisticated and accurate AES systems that truly understand the meaning of the text.

**What does this mean for students and educators?**

* It highlights the need for caution when relying on automated scoring systems.
* It emphasizes the importance of human evaluation and feedback in the grading process.
* It suggests that educators and test administrators should be aware of the potential biases and flaws in these systems.

Overall, the study reveals that Automated Essay Scoring systems are not as infallible as we might think. By understanding their limitations, we can work towards creating more accurate and fair systems that truly support students and educators."
cs.AI,Neural document expansion for ad-hoc information retrieval,"Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",https://arxiv.org/pdf/2012.14005v1,2020-12-27T20:00:08Z,"Cheng Tang, Andrew Arnold","Here's a summary of the research paper for a general audience:

**Improving Search Results with Artificial Intelligence**

Researchers have made a breakthrough in improving search results using artificial intelligence. They've developed a new method called ""neural document expansion,"" which helps computers better understand the content of documents and return more accurate search results.

The method uses a type of AI model that can analyze text and generate new text based on what it's learned. This approach has shown great promise in searching short texts, but it requires a lot of training data to work well.

The good news is that the researchers have found a way to adapt this approach to work with longer documents and limited training data. This is important because it means that the method can be used in a wider range of applications, such as searching large databases or retrieving information from the web.

Overall, this research has the potential to improve the accuracy and relevance of search results, making it easier for people to find what they're looking for online."
cs.AI,Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations,"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.",https://arxiv.org/pdf/1904.07934v2,2019-04-16T19:16:57Z,"David Acuna, Amlan Kar, Sanja Fidler","**Improving Object Boundary Detection with Noisy Annotations**

Computer vision researchers have made significant progress in identifying objects within images, but accurately detecting the boundaries of these objects remains a challenge. Object boundaries are crucial for various applications, such as self-driving cars, medical imaging, and robotics. However, obtaining precise annotations of these boundaries is a labor-intensive and time-consuming process. To address this issue, researchers have developed a new method that enables computers to learn from noisy annotations and improve object boundary detection.

**The Problem: Noisy Annotations**

Annotations are labels assigned to specific parts of an image to help computers learn what they represent. However, these annotations can be noisy or inaccurate, which can hinder the performance of object boundary detection algorithms. The researchers behind this study aimed to develop a method that can effectively learn from noisy annotations and produce accurate object boundary detections.

**The Solution: A New Layer and Loss Function**

The researchers proposed a simple yet effective solution that can be integrated with existing object boundary detection algorithms. They introduced a new layer and loss function that encourage the computer to predict sharp and precise boundaries, even when the annotations are noisy. The new layer and loss function work by:

1. **Enforcing maximum response along the normal direction**: The computer is encouraged to predict a strong response along the direction of the boundary, which helps to produce sharp and precise boundaries.
2. **Regularizing direction**: The computer is also encouraged to predict a consistent direction for the boundary, which helps to reduce errors.

**Level Set Formulation: Reasoning about True Object Boundaries**

The researchers also used a level set formulation, which is a mathematical technique for representing boundaries. This allowed the computer to reason about the true object boundaries during training, even when the annotations are noisy or misaligned.

**Results and Implications**

The researchers tested their method on several datasets and achieved state-of-the-art results, outperforming existing methods by a significant margin (more than 4% in terms of MF(ODS) and 18.61% in terms of AP). Furthermore, they demonstrated that their learned network can be used to improve coarse segmentation labels, making it an efficient way to label new data.

**Conclusion**

In conclusion, the researchers have developed a novel method that enables computers to learn from noisy annotations and improve object boundary detection. Their approach has significant implications for various applications, including self-driving cars, medical imaging, and robotics. By improving object boundary detection, we can enable computers to better understand and interact with their environment."
cs.AI,How to define co-occurrence in different domains of study?,"This position paper presents a comparative study of co-occurrences. Some similarities and differences in the definition exist depending on the research domain (e.g. linguistics, NLP, computer science). This paper discusses these points, and deals with the methodological aspects in order to identify co-occurrences in a multidisciplinary paradigm.",https://arxiv.org/pdf/1904.08010v1,2019-04-16T23:16:56Z,Mathieu Roche,"Here's a summary of the research paper for a general audience:

**Understanding Co-Occurrence: A Concept with Many Meanings**

When we talk about two things happening together, we use the term ""co-occurrence"". But did you know that this concept has different meanings in different fields of study? Researchers from various domains, such as linguistics, computer science, and natural language processing (NLP), have been using the term ""co-occurrence"" in their own ways, often without a unified definition.

This paper explores the similarities and differences in how co-occurrence is defined and studied across different research areas. The authors aim to create a common framework for understanding co-occurrence, which can help researchers from diverse fields communicate and work together more effectively. By clarifying the methodological aspects of co-occurrence, this study paves the way for a more multidisciplinary approach to understanding this complex concept."
cs.AI,Decision Making with Machine Learning and ROC Curves,The Receiver Operating Characteristic (ROC) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data science. This paper studies the statistical properties of ROC curves and its implication on model selection. We analyze the implications of different models of incentive heterogeneity and information asymmetry on the relation between human decisions and the ROC curves. Our theoretical discussion is illustrated in the context of a large data set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy Checkups of reproductive age couples in Henan Province provided by the Chinese Ministry of Health.,https://arxiv.org/pdf/1905.02810v1,2019-05-05T08:01:23Z,"Kai Feng, Han Hong, Ke Tang, Jingyuan Wang","Here's a summary of the research paper for a general audience:

**Using Machine Learning to Make Better Decisions**

Imagine you're a doctor trying to decide whether a pregnant woman is at risk for complications. You have to make a decision based on limited information, and you want to make sure you're making the right call. That's where machine learning comes in - a type of artificial intelligence that helps computers make predictions based on data.

One important tool in machine learning is the ROC curve (Receiver Operating Characteristic curve). It's a graph that shows how well a computer model can distinguish between two things, like a healthy pregnancy and a high-risk pregnancy. The curve helps us understand how accurate the model is and how it can be improved.

In this study, researchers looked at how ROC curves can be used to make better decisions in situations where there's uncertainty and incomplete information. They used a large dataset of pregnancy outcomes and doctor diagnoses from China to test their ideas.

The researchers found that ROC curves can be influenced by factors like how doctors make decisions and how much information they have. By understanding these factors, we can create better machine learning models that take into account the complexities of real-world decision-making.

The study's findings have important implications for fields like medicine, finance, and social services, where decision-making is critical and data-driven insights can make a big difference. By combining machine learning with ROC curves, we can make more informed decisions and improve outcomes for people and organizations."
cs.AI,Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review,"Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.",https://arxiv.org/pdf/1905.02845v1,2019-05-07T23:41:34Z,"Benyamin Ghojogh, Maria N. Samad, Sayema Asif Mashhadi, Tania Kapoor, Wahab Ali, Fakhri Karray, Mark Crowley","**Simplifying Complex Data: A Review of Feature Selection and Extraction Methods**

When analyzing complex data, it's often helpful to simplify it by selecting or extracting the most important features. This process, known as feature selection and extraction, enables better classification, prediction, or clustering of data. In a recent literature review, researchers examined various methods for feature selection and extraction, including their underlying theory, motivations, and applications.

The review highlights the importance of pre-processing data to make it more manageable and interpretable. By selecting or extracting the most relevant features, researchers can improve the accuracy and efficiency of their analyses. The authors also provide examples of numerical implementations of these methods and compare their strengths and weaknesses.

**In Simple Terms:** Imagine trying to understand a large, messy dataset. Feature selection and extraction are like cleaning and organizing the data, so that you can focus on the most important information. This helps you make better predictions, classify data, or identify patterns. This review provides an overview of different methods for doing just that."
cs.AI,AI-Powered Text Generation for Harmonious Human-Machine Interaction: Current State and Future Directions,"In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation.",https://arxiv.org/pdf/1905.01984v1,2019-05-01T23:26:38Z,"Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang, Shaoyang Hao, Zhiwen Yu","Here's a summary of the research paper for a general audience:

**The Future of Text Generation: How AI is Changing the Way We Interact with Machines**

In recent years, artificial intelligence (AI) has made tremendous progress in generating human-like text. This technology has the potential to revolutionize the way we interact with machines, making them more conversational and personalized. Researchers have been working on developing new methods for text generation, shifting from simple template-based approaches to more advanced neural network-based models.

The goal of text generation has also evolved. Instead of just generating smooth and coherent sentences, researchers now aim to create content that is personalized and diverse. This technology has many potential applications, such as chatbots, virtual assistants, and content creation tools.

This survey paper provides an overview of the current state of text generation, including the different models and techniques used, as well as their various applications. The authors aim to provide a comprehensive summary of the achievements in this field and identify future directions for research. Ultimately, the goal is to create harmonious human-machine interaction, where machines can understand and respond to human needs in a more natural and intuitive way."
cs.CL,End-to-End Speaker Diarization as Post-Processing,"This paper investigates the utilization of an end-to-end diarization model as post-processing of conventional clustering-based diarization. Clustering-based diarization methods partition frames into clusters of the number of speakers; thus, they typically cannot handle overlapping speech because each frame is assigned to one speaker. On the other hand, some end-to-end diarization methods can handle overlapping speech by treating the problem as multi-label classification. Although some methods can treat a flexible number of speakers, they do not perform well when the number of speakers is large. To compensate for each other's weakness, we propose to use a two-speaker end-to-end diarization method as post-processing of the results obtained by a clustering-based method. We iteratively select two speakers from the results and update the results of the two speakers to improve the overlapped region. Experimental results show that the proposed algorithm consistently improved the performance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II datasets.",https://arxiv.org/pdf/2012.10055v2,2020-12-18T05:31:07Z,"Shota Horiguchi, Paola Garcia, Yusuke Fujita, Shinji Watanabe, Kenji Nagamatsu","**Improving Speaker Identification in Recordings**

Researchers have made progress in developing technology to identify and distinguish between multiple speakers in audio recordings. Currently, there are two main approaches: one that groups audio frames into clusters based on the number of speakers, and another that uses artificial intelligence to classify audio segments as belonging to one or more speakers.

However, these approaches have limitations. The clustering-based method struggles with overlapping speech, where multiple speakers talk at the same time, while the AI-based method can handle overlapping speech but may not perform well with a large number of speakers.

To overcome these limitations, the researchers propose a new approach that combines the strengths of both methods. They use an AI-based model that can handle overlapping speech between two speakers to refine the results of the clustering-based method. By iteratively selecting pairs of speakers and updating the results, the proposed algorithm improves the accuracy of speaker identification in recordings.

**Key findings:**

* The proposed algorithm consistently improved the performance of state-of-the-art methods across multiple datasets (CALLHOME, AMI, and DIHARD II).
* The approach can effectively handle overlapping speech and improve speaker identification accuracy.

**Implications:**

* This research has the potential to improve the accuracy of speaker identification in various applications, such as speech recognition, voice assistants, and audio analysis."
cs.CL,Regularized Attentive Capsule Network for Overlapped Relation Extraction,"Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.",https://arxiv.org/pdf/2012.10187v1,2020-12-18T12:17:08Z,"Tianyi Liu, Xiangyu Lin, Weijia Jia, Mingliang Zhou, Wei Zhao","**Improving Relation Extraction with AI: A Breakthrough in Knowledge Base Construction**

Researchers have developed a new artificial intelligence (AI) model, called Regularized Attentive Capsule Network (RA-CapNet), to improve the accuracy of relation extraction in knowledge base construction. Relation extraction is the process of identifying relationships between entities in text, such as ""Person A is a colleague of Person B"".

The challenge lies in automatically extracting high-quality relationships from large datasets, which often contain noisy words and overlapping relations. For instance, a single sentence may describe multiple relationships between entities, making it difficult for AI models to accurately identify them.

The RA-CapNet model addresses this challenge by using a novel approach that combines attention mechanisms and capsule networks. This allows the model to:

1. Focus on relevant features in the text, regardless of their position.
2. Identify multiple relation features in a single instance.
3. Distinguish between overlapping relations.

The researchers tested their model on widely used datasets and achieved significant improvements in relation extraction. This breakthrough has the potential to enhance the construction of knowledge bases, which are essential for various applications, such as search engines, chatbots, and recommendation systems.

**In simple terms:** The RA-CapNet model is a new AI tool that helps computers better understand relationships between entities in text, even when the text is complex or contains multiple relationships. This can lead to more accurate and comprehensive knowledge bases, which can benefit various applications and industries."
cs.CL,Should I visit this place? Inclusion and Exclusion Phrase Mining from Reviews,"Although several automatic itinerary generation services have made travel planning easy, often times travellers find themselves in unique situations where they cannot make the best out of their trip. Visitors differ in terms of many factors such as suffering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowledge this is the first work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classifier provides a binary overlap F1 of $\sim$80 and $\sim$82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classifier provides an F1 of $\sim$98 and $\sim$97 for 11-class inclusion and exclusion classification respectively. We believe that our work can significantly improve the quality of an automatic itinerary generation service.",https://arxiv.org/pdf/2012.10226v1,2020-12-18T13:43:13Z,"Omkar Gurjar, Manish Gupta","**Making Travel More Accessible: A New Way to Analyze Tourist Reviews**

When planning a trip, it's essential to choose destinations that cater to your needs, whether you have a disability, dietary restrictions, or are traveling with a toddler. However, it's not always easy to find out if a tourist spot is suitable for you. A new study proposes a solution to this problem by analyzing online reviews of tourist spots to identify phrases that indicate whether a place is inclusive or not for people with specific needs.

The researchers collected 2000 reviews from 1000 tourist spots and developed a system to classify phrases as either inclusive or exclusive for 11 factors, such as disability accessibility or family-friendliness. Their system achieved high accuracy, correctly identifying inclusive and exclusive phrases about 80-98% of the time.

This research has the potential to improve automatic itinerary generation services, making it easier for travelers to plan trips that meet their needs. By analyzing reviews in this way, tourist spots can be more easily identified as suitable or not suitable for specific types of travelers, helping to create a more inclusive and accessible travel experience for everyone."
cs.CL,Speech Synthesis as Augmentation for Low-Resource ASR,"Speech synthesis might hold the key to low-resource speech recognition. Data augmentation techniques have become an essential part of modern speech recognition training. Yet, they are simple, naive, and rarely reflect real-world conditions. Meanwhile, speech synthesis techniques have been rapidly getting closer to the goal of achieving human-like speech. In this paper, we investigate the possibility of using synthesized speech as a form of data augmentation to lower the resources necessary to build a speech recognizer. We experiment with three different kinds of synthesizers: statistical parametric, neural, and adversarial. Our findings are interesting and point to new research directions for the future.",https://arxiv.org/pdf/2012.13004v1,2020-12-23T22:19:42Z,"Deblin Bagchi, Shannon Wotherspoon, Zhuolin Jiang, Prasanna Muthukumar","Here's a summary of the research paper for a general audience:

**Can Computer-Generated Voices Help Improve Speech Recognition?**

Speech recognition technology, like Siri or Alexa, requires a lot of data to learn and improve. However, collecting and labeling large amounts of data can be time-consuming and expensive, especially for languages or dialects with limited resources. Researchers are exploring new ways to augment speech recognition training data, and one promising approach is to use computer-generated voices, also known as speech synthesis.

In this study, researchers investigated whether using synthesized speech can help reduce the amount of data needed to build a speech recognizer. They tested three different types of speech synthesizers and found encouraging results. The study suggests that computer-generated voices could be used to supplement real speech data, potentially making it easier to develop speech recognition systems for languages or dialects with limited resources. This research opens up new possibilities for improving speech recognition technology and could have significant implications for the future of voice-activated systems."
cs.CL,QUACKIE: A NLP Classification Task With Ground Truth Explanations,"NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP Interpretability, but their dependence on human provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a specific classification task by diverting question-answering datasets. For this custom classification task, the interpretability ground-truth arises directly from the definition of the classification problem. We use this method to propose a benchmark and lay the groundwork for future research in NLP interpretability by evaluating a wide range of current state of the art methods.",https://arxiv.org/pdf/2012.13190v2,2020-12-24T10:43:20Z,"Yves Rychener, Xavier Renard, Djamé Seddah, Pascal Frossard, Marcin Detyniecki","Here's a summary of the research paper for a general audience:

**Making AI More Transparent: A New Benchmark for NLP Interpretability**

Imagine you're talking to a virtual assistant, and it gives you an answer to a question. You might wonder, ""How did it come up with that answer?"" This is where interpretability comes in - making AI models more transparent and trustworthy.

Researchers have been working on evaluating how well AI models can explain their decisions, but it's been challenging to do so fairly. Existing methods rely on human-provided explanations, which can be biased.

In this study, the authors propose a new approach to create a benchmark for evaluating AI interpretability. They took a dataset designed for question-answering and turned it into a classification task, where the correct explanations are built-in. This allows them to test various state-of-the-art AI models and see how well they can explain their decisions.

The goal of this research is to lay the groundwork for future studies on making AI more transparent and trustworthy. By developing a reliable benchmark, researchers can better evaluate and improve AI models' ability to provide clear explanations for their decisions."
cs.CL,"I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling","To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.",https://arxiv.org/pdf/2012.13391v2,2020-12-24T18:47:49Z,"Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston","Here's a summary of the research paper for a general audience:

**Improving Chatbots' Consistency**

Imagine you're having a conversation with a chatbot, and it says something that contradicts what it said earlier. This can be frustrating and make the conversation feel unnatural. Researchers have developed a new way to detect these kinds of contradictions and improve chatbots' consistency.

The researchers created a dataset of conversations, both between humans and between humans and chatbots, where contradictory statements were made. They then compared two approaches to detecting these contradictions: one that looks at the conversation as a whole, and another that breaks down the conversation into individual statements.

Their results showed that the approach that breaks down the conversation into individual statements is more effective at detecting contradictions. They also found that their new dataset is more useful for training chatbots to detect contradictions than existing datasets.

The researchers' work has important implications for improving the consistency of chatbots. By detecting contradictions, chatbots can be designed to avoid making inconsistent statements, leading to more natural and engaging conversations. This research has the potential to improve the way we interact with chatbots and make our conversations with them more enjoyable and productive."
cs.CL,Panarchy: ripples of a boundary concept,"How do social-ecological systems change over time? In 2002 Holling and colleagues proposed the concept of Panarchy, which presented social-ecological systems as an interacting set of adaptive cycles, each of which is produced by the dynamic tensions between novelty and efficiency at multiple scales. Initially introduced as a conceptual framework and set of metaphors, panarchy has gained the attention of scholars across many disciplines and its ideas continue to inspire further conceptual developments. Almost twenty years after this concept was introduced we review how it has been used, tested, extended and revised. We do this by combining qualitative methods and machine learning. Document analysis was used to code panarchy features that are commonly used in the scientific literature (N = 42), a qualitative analysis that was complemented with topic modeling of 2177 documents. We find that the adaptive cycle is the feature of panarchy that has attracted the most attention. Challenges remain in empirically grounding the metaphor, but recent theoretical and empirical work offers some avenues for future research.",https://arxiv.org/pdf/2012.14312v1,2020-12-28T15:47:45Z,"Juan Rocha, Linda Luvuno, Jesse Rieb, Erin Crockett, Katja Malmborg, Michael Schoon, Garry Peterson","**Understanding How Social and Environmental Systems Change Over Time**

Imagine a lake ecosystem where fish populations, water quality, and local communities are all interconnected. How do these complex systems change and adapt over time? In 2002, a team of researchers introduced the concept of ""Panarchy"" to help understand these dynamic relationships. Panarchy proposes that social-ecological systems, like the lake ecosystem, are made up of multiple cycles of change that interact with each other. These cycles are driven by tensions between innovation and efficiency, and occur at different scales, from local to global.

**Key Findings**

A recent review of nearly 2,200 research papers on Panarchy has shed light on how this concept has been used and developed over the past two decades. The review found that:

* The idea of an ""adaptive cycle"" - a cycle of growth, conservation, release, and reorganization - has been the most widely used and studied aspect of Panarchy.
* While the concept has inspired many researchers, there are still challenges in using it to understand real-world systems.
* Recent studies have made progress in testing and applying Panarchy, and offer new ideas for future research.

**What Does This Mean?**

The Panarchy concept has helped researchers think about complex systems in a new way, and has inspired a wide range of studies across many fields. While there is still work to be done, the review suggests that Panarchy remains a useful framework for understanding how social and environmental systems change and adapt over time. By continuing to develop and test this concept, researchers can gain a deeper understanding of how to manage and protect complex systems like the lake ecosystem."
cs.CL,My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism,"Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We find that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups' where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. We also find that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as ``the world is flat'' actually increases the score instead of decreasing it.",https://arxiv.org/pdf/2012.13872v1,2020-12-27T06:19:20Z,"Swapnil Parekh, Yaman Kumar Singla, Changyou Chen, Junyi Jessy Li, Rajiv Ratn Shah","**The Hidden Flaws of Automated Essay Scoring Systems**

Imagine taking a test that determines your future, only to find out that the scoring system is flawed. Researchers have made a startling discovery about automated essay scoring systems, which are used to evaluate millions of essays every year. These systems, powered by artificial intelligence, are supposed to assess writing skills, but they have a major weakness: they don't truly understand the meaning of the text.

In a recent study, researchers tested these systems with essays that contained false information, such as the claim that ""the world is flat."" Surprisingly, some systems gave higher scores to essays with false information, rather than lower scores. This is because the systems focus on a few key words, rather than understanding the overall meaning and context of the essay.

The researchers also found that removing the context surrounding these key words had little impact on the predicted score, even if it made the essay harder to understand. This suggests that the systems are not evaluating essays as cohesive pieces of writing, but rather as collections of individual words.

These findings raise concerns about the reliability of automated essay scoring systems, particularly when used for high-stakes decisions, such as college admissions or job applications. The researchers argue that these systems need to be improved to truly understand the meaning and context of essays, rather than just relying on superficial features. By doing so, we can ensure that these systems provide accurate and fair evaluations of students' writing abilities."
cs.CL,Measuring University Impact: Wikipedia approach,"The impact of Universities on the social, economic and political landscape is one of the key directions in contemporary educational evaluation. In this paper, we discuss the new methodological technique that evaluates the impact of university based on popularity (number of page-views) of their alumni's pages on Wikipedia. It allows revealing the alumni popularity dynamics and tracking its state. Preliminary analysis shows that the number of page-views is higher for the contemporary persons that prove the perspectives of this approach. Then, universities were ranked based on the methodology and compared to the famous international university rankings ARWU and QS based only on alumni scales: for the top 10 universities, there is an intersection of two universities (Columbia University, Stanford University). The correlation coefficients between different university rankings are provided in the paper. Finally, the ranking based on the alumni popularity was compared with the ranking of universities based on the popularity of their webpages on Wikipedia: there is a strong connection between these indicators.",https://arxiv.org/pdf/2012.13980v1,2020-12-27T17:41:56Z,"Tatiana Kozitsina, Viacheslav Goiko, Roman Palkin, Valentin Khomutenko, Yulia Mundrievskaya, Maria Sukhareva, Isak Froumin, Mikhail Myagkov","Here's a summary of the research paper for a general audience:

**Measuring University Impact through Wikipedia**

Universities have a significant impact on society, economy, and politics. But how can we measure this impact? A new study proposes a creative approach: using Wikipedia to evaluate a university's influence. The researchers looked at the popularity of alumni pages on Wikipedia, measured by the number of page views. They found that this approach can reveal interesting trends and patterns.

The study ranked universities based on the popularity of their alumni's Wikipedia pages and compared the results to well-known international university rankings. Interestingly, two of the top universities - Columbia and Stanford - appeared in both lists. The researchers also found a strong connection between the popularity of a university's alumni pages and the popularity of its own Wikipedia page.

This innovative approach offers a new way to assess a university's impact and reputation. By leveraging online data, researchers can gain insights into a university's influence on society and its alumni's achievements. This method could complement traditional university rankings and provide a more nuanced understanding of a university's contributions."
cs.CL,Neural document expansion for ad-hoc information retrieval,"Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",https://arxiv.org/pdf/2012.14005v1,2020-12-27T20:00:08Z,"Cheng Tang, Andrew Arnold","Here's a summary of the research paper for a general audience:

**Improving Search Results with Artificial Intelligence**

Researchers have made a breakthrough in improving search results using artificial intelligence. They've developed a new method called ""neural document expansion"" that helps computers better understand the content of documents and return more accurate search results.

The method uses a type of AI model that can analyze and expand on the text in a document, making it easier for the computer to find relevant information. Previously, this approach required a large amount of labeled data to work effectively, which can be time-consuming and expensive to obtain.

The good news is that the researchers have found a way to adapt this approach to work with standard search tasks, where there may not be as much labeled data available. This means that the method can be applied to a wider range of search tasks, including those involving long documents.

The potential impact of this research is significant, as it could lead to more accurate and relevant search results in a variety of applications, from web search engines to specialized databases."
cs.CL,Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation,"Spoken question answering (SQA) is challenging due to complex reasoning on top of the spoken documents. The recent studies have also shown the catastrophic impact of automatic speech recognition (ASR) errors on SQA. Therefore, this work proposes to mitigate the ASR errors by aligning the mismatch between ASR hypotheses and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively utilize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score.",https://arxiv.org/pdf/1904.07904v1,2019-04-16T18:13:39Z,"Chia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee","Here's a summary of the research paper for a general audience:

**Improving Spoken Question Answering: A New Approach**

Imagine you're asking a voice assistant a question, but the assistant doesn't quite understand what you said. This can lead to incorrect answers. Researchers have found that errors made by speech recognition systems can have a big impact on spoken question answering (SQA). To tackle this problem, a team of researchers proposed a new approach that uses a technique called adversarial domain adaptation.

Their method helps to reduce the errors made by speech recognition systems by teaching a model to focus on the most important features of the spoken text, rather than getting bogged down by mistakes. The researchers tested their approach and found that it improved the accuracy of SQA by 2%, beating the previous best results.

In simple terms, this research aims to make voice assistants better at understanding and answering questions by reducing the impact of speech recognition errors. The new approach has shown promising results and could lead to more accurate and helpful voice assistants in the future."
cs.CL,Query Expansion for Cross-Language Question Re-Ranking,"Community question-answering (CQA) platforms have become very popular forums for asking and answering questions daily. While these forums are rich repositories of community knowledge, they present challenges for finding relevant answers and similar questions, due to the open-ended nature of informal discussions. Further, if the platform allows questions and answers in multiple languages, we are faced with the additional challenge of matching cross-lingual information. In this work, we focus on the cross-language question re-ranking shared task, which aims to find existing questions that may be written in different languages. Our contribution is an exploration of query expansion techniques for this problem. We investigate expansions based on Word Embeddings, DBpedia concepts linking, and Hypernym, and show that they outperform existing state-of-the-art methods.",https://arxiv.org/pdf/1904.07982v1,2019-04-16T20:55:59Z,"Muhammad Mahbubur Rahman, Sorami Hisamoto, Kevin Duh","**Improving Cross-Language Question Answering on Community Forums**

Imagine you're looking for an answer to a question on a online forum, but the responses are in a different language. Community question-answering platforms, where people ask and answer questions, can be treasure troves of information. However, finding relevant answers and similar questions can be tricky, especially when discussions are informal and span multiple languages.

Researchers have developed a new approach to tackle this challenge. They focused on a task called cross-language question re-ranking, which aims to find existing questions that may be written in different languages. The researchers explored a technique called query expansion, which involves adding more context to a search query to get better results.

They tested three different query expansion methods:

1. **Word Embeddings**: a way to represent words in a mathematical space to capture their meanings.
2. **DBpedia concepts linking**: a way to connect questions to specific concepts and entities, like people or places.
3. **Hypernym**: a way to identify more general concepts related to a question.

The researchers found that these query expansion techniques outperform existing methods, making it easier to find relevant questions and answers across languages. This work has the potential to improve the way we search for information on community forums, making it more accessible and useful for people around the world."
cs.CL,Posterior-regularized REINFORCE for Instance Selection in Distant Supervision,"This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.",https://arxiv.org/pdf/1904.08051v1,2019-04-17T02:21:51Z,"Qi Zhang, Siliang Tang, Xiang Ren, Fei Wu, Shiliang Pu, Yueting Zhuang","**Improving Machine Learning Efficiency with Posterior-Regularized REINFORCE**

Researchers have developed a new method to make machine learning more efficient, specifically for a task called instance selection in distant supervision. Distant supervision is a technique used to train machine learning models on large datasets, but these datasets can be noisy and contain irrelevant information. Instance selection helps to filter out this noise.

The researchers modeled instance selection as a series of decisions, where a machine learning agent determines whether a particular piece of data is valuable or not. They used a technique called REINFORCE to train this agent, but found that it took a long time to train.

To speed up the training process, the researchers applied a technique called posterior regularization (PR). This involves incorporating domain-specific rules into the training process, allowing the agent to make more informed decisions.

The results showed that this new method, called posterior-regularized REINFORCE, significantly improved the performance of a relation classifier (a type of machine learning model) trained on the cleaned dataset. Additionally, it greatly reduced the time required to train the model.

In simple terms, this research provides a more efficient way to train machine learning models to filter out noise in large datasets, which can lead to better performance and faster training times."
cs.CL,End-to-End Speech Translation with Knowledge Distillation,"End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years. Compared to conventional pipeline systems, end-to-end ST models have advantages of lower latency, smaller model size and less error propagation. However, the combination of speech recognition and text translation in one model is more difficult than each of these two tasks. In this paper, we propose a knowledge distillation approach to improve ST model by transferring the knowledge from text translation model. Specifically, we first train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation. Experiments on English- French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the instruction of teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points.",https://arxiv.org/pdf/1904.08075v1,2019-04-17T04:00:52Z,"Yuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, Hua Wu, Haifeng Wang, Chengqing Zong","Here's a summary of the research paper for a general audience:

**Breaking Down Language Barriers with AI-Powered Speech Translation**

Imagine being able to have a conversation with someone who speaks a different language, without needing an interpreter. Researchers are working on developing artificial intelligence (AI) models that can translate speech in real-time, directly from one language to another. This is known as end-to-end speech translation.

The challenge is that speech translation requires combining two complex tasks: speech recognition (understanding what's being said) and text translation (converting the text into another language). To overcome this challenge, researchers propose a new approach called knowledge distillation.

Here's how it works: they first train a separate model that excels at text translation (the ""teacher"" model). Then, they train a speech translation model (the ""student"" model) to learn from the teacher model. This allows the speech translation model to benefit from the teacher's expertise and improve its performance.

In experiments, the researchers found that their approach significantly improved the accuracy of speech translation models, achieving gains of over 3.5 points in a standard evaluation metric (BLEU). This work demonstrates that end-to-end speech translation is possible for both similar and dissimilar language pairs, bringing us closer to seamless communication across languages."
cs.CL,FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance,"Frequently Asked Question (FAQ) retrieval is an important task where the objective is to retrieve an appropriate Question-Answer (QA) pair from a database based on a user's query. We propose a FAQ retrieval system that considers the similarity between a user's query and a question as well as the relevance between the query and an answer. Although a common approach to FAQ retrieval is to construct labeled data for training, it takes annotation costs. Therefore, we use a traditional unsupervised information retrieval system to calculate the similarity between the query and question. On the other hand, the relevance between the query and answer can be learned by using QA pairs in a FAQ database. The recently-proposed BERT model is used for the relevance calculation. Since the number of QA pairs in FAQ page is not enough to train a model, we cope with this issue by leveraging FAQ sets that are similar to the one in question. We evaluate our approach on two datasets. The first one is localgovFAQ, a dataset we construct in a Japanese administrative municipality domain. The second is StackExchange dataset, which is the public dataset in English. We demonstrate that our proposed method outperforms baseline methods on these datasets.",https://arxiv.org/pdf/1905.02851v2,2019-05-08T00:33:37Z,"Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, Sadao Kurohashi","**Improving Question Answering Systems: A New Approach**

Imagine you're searching for answers to common questions on a website, but the search results aren't quite right. Researchers have developed a new system to improve the accuracy of these question-answering systems, known as Frequently Asked Question (FAQ) retrieval.

The system works by considering two key factors: 

1. **How similar is your question to the ones already on the website?** 
2. **How relevant is the answer to your question?**

The researchers used a traditional method to compare the similarity between questions, but they used a more advanced artificial intelligence (AI) model called BERT to assess the relevance of the answers. BERT is a powerful tool that can understand the context and meaning of text.

The challenge was that there weren't enough labeled examples to train the AI model. To overcome this, the researchers used similar sets of questions and answers from other sources to help train the model.

The results were impressive: the new system outperformed existing methods on two different datasets, one in Japanese and one in English. This approach has the potential to improve the accuracy of question-answering systems, making it easier for people to find the answers they're looking for online."
cs.CL,Automatic Inference of Minimalist Grammars using an SMT-Solver,"We introduce (1) a novel parser for Minimalist Grammars (MG), encoded as a system of first-order logic formulae that may be evaluated using an SMT-solver, and (2) a novel procedure for inferring Minimalist Grammars using this parser. The input to this procedure is a sequence of sentences that have been annotated with syntactic relations such as semantic role labels (connecting arguments to predicates) and subject-verb agreement. The output of this procedure is a set of minimalist grammars, each of which is able to parse the sentences in the input sequence such that the parse for a sentence has the same syntactic relations as those specified in the annotation for that sentence. We applied this procedure to a set of sentences annotated with syntactic relations and evaluated the inferred grammars using cost functions inspired by the Minimum Description Length principle and the Subset principle. Inferred grammars that were optimal with respect to certain combinations of these cost functions were found to align with contemporary theories of syntax.",https://arxiv.org/pdf/1905.02869v1,2019-05-08T02:12:18Z,Sagar Indurkhya,"Here's a summary of the research paper for a general audience:

**Unlocking the Secrets of Language: A New Way to Understand Grammar**

Researchers have made a breakthrough in understanding how language works by developing a new method to automatically infer the rules of grammar. This method uses a powerful computer tool called an SMT-solver to analyze a set of sentences and their meanings, and then figure out the underlying grammar rules that govern those sentences.

The researchers started with a set of sentences that had been annotated with information about how the words relate to each other, such as which words are subjects and verbs, and what roles different words play in a sentence. They then used their new method to infer a set of grammar rules that could accurately parse these sentences and match the annotated relationships.

The inferred grammar rules were evaluated using two principles: the Minimum Description Length principle, which favors simpler explanations, and the Subset principle, which prefers more general rules. The results showed that the inferred grammars that were optimal according to these principles aligned with current theories of syntax, providing new insights into how language works.

This research has the potential to revolutionize our understanding of language and could lead to improvements in natural language processing, language teaching, and linguistic research."
cs.CL,ShapeGlot: Learning Language for Shape Differentiation,"In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We first build a large scale, carefully controlled dataset of human utterances that each refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",https://arxiv.org/pdf/1905.02925v1,2019-05-08T06:01:33Z,"Panos Achlioptas, Judy Fan, Robert X. D. Hawkins, Noah D. Goodman, Leonidas J. Guibas","**Understanding How Language Describes Object Shapes**

Imagine you're trying to describe a chair to someone, but there are many similar chairs around. How would you explain which one you're referring to? Researchers have created a new system called ShapeGlot to understand how people use language to differentiate between similar objects.

To develop ShapeGlot, the researchers created a large dataset of human descriptions of objects, such as chairs and lamps, and their 3D models. They then built computer models that can understand and generate language to describe these objects.

The study found that these computer models can accurately identify and describe objects, even when they're similar or when the descriptions are given by humans. The models can also learn to describe new objects they haven't seen before, such as transferring knowledge from chairs to lamps.

The researchers discovered that the models rely heavily on words related to object parts, such as ""leg"" or ""backrest,"" to understand and describe objects. This shows that language is closely tied to the visual features of objects.

This research has practical applications in areas like robotics and computer vision, where understanding language and object shapes is crucial. It also provides insights into how humans use language to communicate about objects and their differences."
cs.CL,Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy,"Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of different coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classification are unscalable and cannot accommodate large scale EHR datasets.   In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). The system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: first, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and finally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed the robustness and generalisation properties of Text2Node by mapping ICD-9-CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy.   This system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in healthcare. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models.",https://arxiv.org/pdf/1905.01958v1,2019-04-11T17:31:23Z,"Rohollah Soltani, Alexandre Tomberg","**Unlocking the Power of Electronic Health Records: A New System for Mapping Medical Phrases**

Electronic Health Records (EHRs) are digital versions of a patient's medical history, containing valuable information about their health. However, different EHR systems use different coding standards, making it difficult to share and compare data between them. A team of researchers has developed a new system called Text2Node, which can map medical phrases to concepts in a large taxonomy, such as SNOMED CT. This system has the potential to improve data interchangeability in healthcare, enabling the sharing and comparison of EHR data between different systems.

**The Problem: Different Coding Standards**

Imagine trying to compare medical data from different hospitals, but each hospital uses a different language to describe the same medical conditions. This is a major challenge in healthcare, as different EHR systems use different coding standards. For example, one hospital might use ICD-9-CM Diagnosis codes, while another hospital uses SNOMED CT codes. This makes it difficult to share and compare data between hospitals.

**The Solution: Text2Node**

Text2Node is a cross-domain mapping system that can map medical phrases to concepts in a large taxonomy. The system works in three stages:

1. **Mapping words to numbers**: Text2Node converts medical phrases into numerical representations, called word embeddings. This allows the system to understand the meaning of each phrase.
2. **Vectorizing the taxonomy**: The system converts the taxonomy into numerical representations, called node embeddings. This allows the system to understand the relationships between different concepts in the taxonomy.
3. **Connecting the two**: Text2Node trains a mapping function to connect the word embeddings to the node embeddings. This enables the system to map medical phrases to concepts in the taxonomy.

**How it Works**

Text2Node uses machine learning algorithms to learn from a limited set of training data. The system can then generalize to new, unseen data, making it scalable and robust. For example, if the system is trained on a dataset of ICD-9-CM Diagnosis phrases, it can map new, unseen phrases to concepts in the SNOMED CT taxonomy.

**The Benefits**

The Text2Node system has several benefits:

* **Improved data sharing**: Text2Node enables the sharing and comparison of EHR data between different systems, which can lead to better healthcare outcomes.
* **More accurate predictive models**: By incorporating taxonomical medical knowledge, Text2Node can improve the accuracy of clinical predictive models.
* **Scalability**: The system can handle large datasets and is robust to variations in wording between coding systems.

**Conclusion**

In conclusion, Text2Node is a novel system that has the potential to revolutionize the way we share and compare EHR data. By mapping medical phrases to concepts in a large taxonomy, Text2Node can improve data interchangeability in healthcare and enable the development of more accurate predictive models. With its scalability and robustness, Text2Node is an important step towards unlocking the full potential of EHRs."
cs.CL,Who wrote this book? A challenge for e-commerce,"Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.",https://arxiv.org/pdf/1905.01973v1,2019-04-19T10:13:07Z,"Béranger Dumont, Simona Maggio, Ghiles Sidi Said, Quoc-Tien Au","**The Author Identification Challenge in E-commerce**

Imagine you're browsing an online bookstore and searching for books by your favorite author, F. Scott Fitzgerald. However, what if the website lists his books under different names, such as ""F. Scott Fitzgerald"", ""Fitzgerald, F. Scott"", or even ""F Scott Fitzgerald""? This can make it difficult to find all the books you're looking for.

Researchers have tackled this problem by developing a system that can accurately identify and standardize author names in e-commerce catalogs. Their approach combines data from open sources, such as book databases, with advanced machine learning techniques. Specifically, they use neural networks to compare and match author names, and to correct errors or inconsistencies.

**The Results**

The researchers tested their system on data from the e-commerce website Rakuten France and achieved a promising result: in 72% of cases, the system's top suggestion was the correct, standardized author name. This breakthrough has the potential to improve the online shopping experience, making it easier for customers to find books by their favorite authors. By solving the author identification challenge, e-commerce platforms can provide more accurate and consistent information, leading to a better user experience."
cs.CL,AI-Powered Text Generation for Harmonious Human-Machine Interaction: Current State and Future Directions,"In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation.",https://arxiv.org/pdf/1905.01984v1,2019-05-01T23:26:38Z,"Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang, Shaoyang Hao, Zhiwen Yu","**The Future of Text Generation: How AI is Revolutionizing Human-Machine Interaction**

Imagine a world where computers can generate text that sounds natural and conversational, just like a human. This is becoming a reality thanks to advancements in artificial intelligence (AI) and deep learning. Over the past 20 years, text generation technology has made tremendous progress, shifting from simple template-based methods to more sophisticated neural network-based approaches.

Researchers are no longer just focused on generating smooth sentences, but also on creating personalized and diverse content. For example, AI-powered chatbots can now be designed to have unique personalities, making interactions with machines feel more natural and engaging.

This survey paper provides an overview of the current state of text generation, highlighting the key technologies, models, and applications that are driving this field forward. From chatbots and virtual assistants to automated content creation, AI-powered text generation is transforming the way humans interact with machines.

As this technology continues to evolve, we can expect to see even more innovative applications in the future, such as personalized language learning tools, AI-generated content for entertainment and education, and more. With this survey, researchers and developers can stay up-to-date on the latest advancements and explore new directions for harmonious human-machine interaction."
stat.ML,Data Assimilation in the Latent Space of a Neural Network,"There is an urgent need to build models to tackle Indoor Air Quality issue. Since the model should be accurate and fast, Reduced Order Modelling technique is used to reduce the dimensionality of the problem. The accuracy of the model, that represent a dynamic system, is improved integrating real data coming from sensors using Data Assimilation techniques. In this paper, we formulate a new methodology called Latent Assimilation that combines Data Assimilation and Machine Learning. We use a Convolutional neural network to reduce the dimensionality of the problem, a Long-Short-Term-Memory to build a surrogate model of the dynamic system and an Optimal Interpolated Kalman Filter to incorporate real data. Experimental results are provided for CO2 concentration within an indoor space. This methodology can be used for example to predict in real-time the load of virus, such as the SARS-COV-2, in the air by linking it to the concentration of CO2.",https://arxiv.org/pdf/2012.12056v1,2020-12-22T14:43:50Z,"Maddalena Amendola, Rossella Arcucci, Laetitia Mottet, Cesar Quilodran Casas, Shiwei Fan, Christopher Pain, Paul Linden, Yi-Ke Guo","**Improving Indoor Air Quality with AI-Powered Forecasting**

Indoor air quality is a pressing concern, especially with the risk of airborne viruses like SARS-COV-2. Researchers have developed a new method called Latent Assimilation, which combines machine learning and data assimilation techniques to create accurate and fast models for predicting indoor air quality.

The method uses a neural network to reduce the complexity of the problem, a type of recurrent neural network (LSTM) to model the dynamic system, and a Kalman filter to incorporate real-time data from sensors. The researchers tested their approach by predicting CO2 concentrations in an indoor space.

This innovative approach has the potential to revolutionize indoor air quality monitoring and prediction. For example, it could be used to forecast the load of viruses like SARS-COV-2 in the air, enabling swift action to prevent outbreaks. By providing accurate and timely predictions, Latent Assimilation can help create healthier and safer indoor environments."
stat.ML,Upper Confidence Bounds for Combining Stochastic Bandits,"We provide a simple method to combine stochastic bandit algorithms. Our approach is based on a ""meta-UCB"" procedure that treats each of $N$ individual bandit algorithms as arms in a higher-level $N$-armed bandit problem that we solve with a variant of the classic UCB algorithm. Our final regret depends only on the regret of the base algorithm with the best regret in hindsight. This approach provides an easy and intuitive alternative strategy to the CORRAL algorithm for adversarial bandits, without requiring the stability conditions imposed by CORRAL on the base algorithms. Our results match lower bounds in several settings, and we provide empirical validation of our algorithm on misspecified linear bandit and model selection problems.",https://arxiv.org/pdf/2012.13115v1,2020-12-24T05:36:29Z,"Ashok Cutkosky, Abhimanyu Das, Manish Purohit","**Improving Decision-Making with a Simple and Effective Method**

Imagine you're trying to find the best option among several choices, but you don't know which one is the best. This is a classic problem in decision-making, known as a ""bandit problem"". Researchers have developed algorithms to help solve this problem, but they can be limited when dealing with multiple options or uncertain situations.

A new study proposes a simple and intuitive method to combine multiple algorithms, called ""stochastic bandit algorithms"", to improve decision-making. This method, called ""meta-UCB"", treats each individual algorithm as an option in a higher-level problem, and uses a variant of the classic UCB algorithm to solve it.

The key benefit of this approach is that it allows for easy combination of different algorithms, without requiring complex conditions or assumptions. The results show that this method performs well, matching theoretical limits in several situations, and outperforms other methods in certain scenarios.

The study also tested the method on real-world problems, such as optimizing linear models and selecting the best model, with promising results. Overall, this research provides a practical and effective solution for improving decision-making in uncertain situations."
stat.ML,QUACKIE: A NLP Classification Task With Ground Truth Explanations,"NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP Interpretability, but their dependence on human provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a specific classification task by diverting question-answering datasets. For this custom classification task, the interpretability ground-truth arises directly from the definition of the classification problem. We use this method to propose a benchmark and lay the groundwork for future research in NLP interpretability by evaluating a wide range of current state of the art methods.",https://arxiv.org/pdf/2012.13190v2,2020-12-24T10:43:20Z,"Yves Rychener, Xavier Renard, Djamé Seddah, Pascal Frossard, Marcin Detyniecki","Here's a summary of the research paper for a general audience:

**Making AI More Transparent: A New Benchmark for NLP Interpretability**

Imagine you're talking to a virtual assistant, and it gives you an answer to a question. You might wonder, ""How did it come up with that answer?"" This is where Natural Language Processing (NLP) interpretability comes in - it's about making AI models more transparent and trustworthy.

Researchers have been working on ways to evaluate how well NLP models explain their decisions, but existing methods rely on human-provided explanations, which can be biased. To address this issue, a team of researchers created a new classification task using question-answering datasets. They designed a benchmark, called QUACKIE, where the explanations for the model's decisions are built-in, providing a more objective way to evaluate interpretability methods.

The researchers used QUACKIE to test various state-of-the-art NLP interpretability methods and laid the groundwork for future research in this area. This work has the potential to improve the transparency and trustworthiness of AI models, making them more reliable and accountable."
stat.ML,Learning Structures in Earth Observation Data with Gaussian Processes,"Gaussian Processes (GPs) has experienced tremendous success in geoscience in general and for bio-geophysical parameter retrieval in the last years. GPs constitute a solid Bayesian framework to formulate many function approximation problems consistently. This paper reviews the main theoretical GP developments in the field. We review new algorithms that respect the signal and noise characteristics, that provide feature rankings automatically, and that allow applicability of associated uncertainty intervals to transport GP models in space and time. All these developments are illustrated in the field of geoscience and remote sensing at a local and global scales through a set of illustrative examples.",https://arxiv.org/pdf/2012.11922v1,2020-12-22T10:46:37Z,"Fernando Mateo, Jordi Munoz-Mari, Valero Laparra, Jochem Verrelst, Gustau Camps-Valls","**Unlocking Hidden Patterns in Earth Observation Data**

Scientists have made significant progress in analyzing data from Earth observation, such as satellite and sensor readings, using a powerful statistical tool called Gaussian Processes (GPs). GPs are a type of machine learning algorithm that helps identify patterns and relationships in complex data.

In a recent review, researchers highlighted the latest advancements in GPs and their applications in geoscience and remote sensing. They discussed new algorithms that can:

1. **Handle noisy data**: GPs can now effectively separate signal from noise, leading to more accurate results.
2. **Identify key factors**: The algorithms can automatically rank features, such as variables like temperature or humidity, to determine their importance in predicting certain outcomes.
3. **Provide uncertainty estimates**: GPs can now generate uncertainty intervals, which help quantify the reliability of predictions and can be applied to different locations and times.

The researchers demonstrated the effectiveness of these developments through various examples, ranging from local to global scales. These advancements have the potential to improve our understanding of the Earth and its systems, enabling more accurate predictions and better decision-making in fields like climate modeling, agriculture, and environmental monitoring."
stat.ML,"Fairness, Welfare, and Equity in Personalized Pricing","We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a ""triple bottom line"": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.",https://arxiv.org/pdf/2012.11066v2,2020-12-21T01:01:56Z,"Nathan Kallus, Angela Zhou","**The Fairness of Personalized Pricing: A Delicate Balance**

Imagine being offered a customized price for a product or service based on your individual characteristics, such as your income level or shopping habits. This practice, known as personalized pricing, is becoming increasingly common. But is it fair?

Researchers have been studying the impact of personalized pricing on fairness, welfare, and equity. They found that personalized pricing can have both positive and negative effects, depending on the context and goals. For example, it can increase access to essential goods and services, such as vaccines or microloans, while also raising concerns about unequal treatment and price burdens on certain groups.

The researchers propose a framework to evaluate the fairness and effectiveness of personalized pricing. They suggest that, in some cases, personalized pricing can achieve a ""triple bottom line"": expanding access, increasing overall welfare, and improving revenue or budget utilization.

To illustrate their findings, the researchers analyzed two real-world examples: pricing subsidies for an elective vaccine and personalized interest rates for microcredit. They found that personalized pricing can lead to significant benefits, such as increased access to vaccines and improved repayment rates for microloans.

Overall, the study highlights the need for careful consideration of fairness, welfare, and equity when implementing personalized pricing strategies. By understanding the potential benefits and drawbacks, businesses and policymakers can design more effective and equitable pricing systems that balance competing objectives."
stat.ML,Towards sample-efficient episodic control with DAC-ML,"The sample-inefficiency problem in Artificial Intelligence refers to the inability of current Deep Reinforcement Learning models to optimize action policies within a small number of episodes. Recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in Episodic Reinforcement Learning. However, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. In this paper, we capitalize on the design principles of the Distributed Adaptive Control (DAC) theory of mind and brain to build a novel cognitive architecture (DAC-ML) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.",https://arxiv.org/pdf/2012.13779v1,2020-12-26T16:38:08Z,"Ismael T. Freire, Adrián F. Amil, Vasiliki Vouloutsi, Paul F. M. J. Verschure","**Breakthrough in Artificial Intelligence: A New Approach to Learning**

Imagine being able to learn a new skill or task with just a few tries. Current artificial intelligence (AI) systems struggle to do this, requiring thousands of attempts to master even simple tasks. Researchers have made progress in improving learning speed by adding memory systems to AI models, but they still can't match human learning abilities.

In a new study, scientists have developed a novel AI system called DAC-ML, inspired by the human brain's Distributed Adaptive Control (DAC) theory. This system mimics the brain's hippocampus, a region crucial for forming memories, to rapidly learn and adapt to new situations. When tested on a challenging virtual foraging task, DAC-ML quickly learned to make effective decisions, outperforming existing AI models.

This breakthrough has significant implications for the development of more efficient and human-like AI systems, which could lead to advancements in areas like robotics, gaming, and decision-making. By learning from the brain's efficient learning mechanisms, researchers can create AI systems that learn faster and more effectively, bringing us closer to developing intelligent machines that can think and act like humans."
stat.ML,3D Shape Synthesis for Conceptual Design and Optimization Using Variational Autoencoders,"We propose a data-driven 3D shape design method that can learn a generative model from a corpus of existing designs, and use this model to produce a wide range of new designs. The approach learns an encoding of the samples in the training corpus using an unsupervised variational autoencoder-decoder architecture, without the need for an explicit parametric representation of the original designs. To facilitate the generation of smooth final surfaces, we develop a 3D shape representation based on a distance transformation of the original 3D data, rather than using the commonly utilized binary voxel representation. Once established, the generator maps the latent space representations to the high-dimensional distance transformation fields, which are then automatically surfaced to produce 3D representations amenable to physics simulations or other objective function evaluation modules. We demonstrate our approach for the computational design of gliders that are optimized to attain prescribed performance scores. Our results show that when combined with genetic optimization, the proposed approach can generate a rich set of candidate concept designs that achieve prescribed functional goals, even when the original dataset has only a few or no solutions that achieve these goals.",https://arxiv.org/pdf/1904.07964v1,2019-04-16T20:26:53Z,"Wentai Zhang, Zhangsihao Yang, Haoliang Jiang, Suyash Nigam, Soji Yamakawa, Tomotake Furuhata, Kenji Shimada, Levent Burak Kara","Here's a summary of the research paper for a general audience:

**Designing New 3D Shapes with AI**

Imagine being able to create new designs for products, like gliders or other objects, using artificial intelligence (AI). Researchers have developed a method that uses AI to learn from existing designs and generate new ones that are optimized for specific performance goals.

The approach uses a type of AI called a variational autoencoder, which analyzes a collection of existing designs and learns to represent them in a compact, mathematical form. This allows the AI to generate new designs by combining and modifying the features of the existing designs.

The researchers tested their method by designing gliders that need to achieve certain performance goals, such as flying a certain distance or staying aloft for a certain amount of time. They found that their AI-powered approach can generate a wide range of new designs that meet these goals, even when there are few or no existing designs that achieve them.

This method has the potential to revolutionize the design process by allowing engineers and designers to quickly generate and test many different design options, rather than relying on trial and error or manual design. The resulting designs can then be optimized using additional AI tools, such as genetic algorithms, to achieve specific performance goals."
stat.ML,DNN Architecture for High Performance Prediction on Natural Videos Loses Submodule's Ability to Learn Discrete-World Dataset,"Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.",https://arxiv.org/pdf/1904.07969v1,2019-04-16T20:35:09Z,"Lana Sinapayen, Atsushi Noda","Here's a summary of the research paper for a general audience:

**Can a Single Brain-Like Algorithm Learn Everything?**

Imagine a super-smart computer program that can learn and predict anything, from how a ball bounces to how a person walks. Researchers have been wondering if such a program exists, and if it can adapt to completely different worlds with different rules.

To test this idea, scientists studied a specific program called PredNet, which is great at predicting what happens next in videos of the real world. They found that PredNet is really good at predicting natural videos, but when they tried to train it on a completely different type of video - one that followed the rules of a simple game called the Game of Life - it struggled to learn.

Surprisingly, they also found that a smaller part of the PredNet program, a type of artificial neural network, was actually really good at learning the Game of Life video, but not so good at predicting natural videos. This suggests that the PredNet program's architecture is designed to excel in one area, but at the cost of flexibility.

The findings imply that there may be a trade-off between being really good at predicting things in the real world and being able to adapt to completely different situations. This has interesting implications for the development of artificial intelligence and our understanding of how the human brain works."
stat.ML,SynC: A Unified Framework for Generating Synthetic Population with Gaussian Copula,"Synthetic population generation is the process of combining multiple socioeconomic and demographic datasets from different sources and/or granularity levels, and downscaling them to an individual level. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this study, we propose a multi-stage framework called SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first removes potential outliers in the data and then fits the filtered data with a Gaussian copula model to correctly capture dependencies and marginal distributions of sampled survey data. Finally, SynC leverages predictive models to merge datasets into one and then scales them accordingly to match the marginal constraints. We make three key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 3) release an easy-to-use framework implementation for reproducibility, and 4) ensure the methodology is scalable at the production level and can easily incorporate new data.",https://arxiv.org/pdf/1904.07998v2,2019-04-16T22:10:19Z,"Colin Wan, Zheng Li, Alicia Guo, Yue Zhao","**Creating Realistic Populations with Synthetic Data**

Imagine being able to combine information from different sources, like census data and survey results, to create a detailed picture of a population. This is called synthetic population generation, and it's a crucial step in many data science tasks. However, until now, there hasn't been a standard and efficient way to do it.

Researchers have developed a new framework called SynC, which uses advanced statistical and machine learning techniques to generate synthetic populations. SynC works by first cleaning the data to remove errors, then using a mathematical model to capture the relationships between different variables. Finally, it combines the data into a single, detailed picture of the population.

The benefits of SynC are threefold. Firstly, it provides a novel and efficient way to generate individual-level data from aggregated sources. Secondly, it can be used as a feature engineering tool, which means it can help improve the accuracy of machine learning models. Thirdly, it offers an alternative to collecting data in situations where gathering information is difficult or expensive.

The researchers tested SynC on two real-world datasets and found it to be effective and scalable. They also made the framework easy to use and publicly available, which means others can reproduce their results and build upon their work. Overall, SynC has the potential to revolutionize the way we work with data, making it easier to generate realistic populations and make informed decisions."
stat.ML,People infer recursive visual concepts from just a few examples,"Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal ""programs"" -- latent generating processes with nontrivial algorithmic properties -- from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pre-trained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.",https://arxiv.org/pdf/1904.08034v2,2019-04-17T00:45:05Z,"Brenden M. Lake, Steven T. Piantadosi","**Unlocking Human Learning: How We Infer Complex Concepts from Few Examples**

Imagine being shown just one or two pictures of a new type of animal, and then being able to recognize and even generate new images of similar animals. This ability to learn and reason from very few examples is a hallmark of human intelligence, but it's a challenge for even the best machine learning algorithms.

Researchers have found that people can infer complex visual concepts, including the underlying ""programs"" or rules that generate them, from just one, two, or three examples. This is because humans don't just learn by memorizing images, but by understanding the causal relationships and patterns that underlie them.

In a recent study, participants were shown a few examples of a visual concept and then asked to extrapolate and generate new examples. The results showed that people's judgments were consistent with a Bayesian program learning model, which searches for the best explanation of the observations. This model outperformed alternative approaches, including a state-of-the-art deep neural network.

These findings have significant implications for our understanding of human learning and intelligence. They suggest that people have a unique ability to learn and reason with rich algorithmic abstractions from sparse input data, which is a key aspect of human cognition. By studying how humans learn and reason, researchers can develop more advanced machine learning algorithms that mimic human intelligence."
stat.ML,Neural Message Passing for Multi-Label Classification,"Multi-label classification (MLC) is the task of assigning a set of target labels for a given sample. Modeling the combinatorial label interactions in MLC has been a long-haul challenge. We propose Label Message Passing (LaMP) Neural Networks to efficiently model the joint prediction of multiple labels. LaMP treats labels as nodes on a label-interaction graph and computes the hidden representation of each label node conditioned on the input using attention-based neural message passing. Attention enables LaMP to assign different importance to neighbor nodes per label, learning how labels interact (implicitly). The proposed models are simple, accurate, interpretable, structure-agnostic, and applicable for predicting dense labels since LaMP is incredibly parallelizable. We validate the benefits of LaMP on seven real-world MLC datasets, covering a broad spectrum of input/output types and outperforming the state-of-the-art results. Notably, LaMP enables intuitive interpretation of how classifying each label depends on the elements of a sample and at the same time rely on its interaction with other labels. We provide our code and datasets at https://github.com/QData/LaMP",https://arxiv.org/pdf/1904.08049v1,2019-04-17T01:58:17Z,"Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi","**Breakthrough in Multi-Label Classification: Introducing LaMP Neural Networks**

Imagine you're trying to categorize a piece of content, like a news article or a social media post, into multiple categories at once (e.g., sports, politics, and entertainment). This task, known as multi-label classification, is challenging because it requires understanding how different labels interact with each other. Researchers have now developed a new method called Label Message Passing (LaMP) Neural Networks, which efficiently models these interactions to improve accuracy.

**How LaMP Works**

LaMP treats labels as nodes in a graph and uses a technique called attention-based neural message passing to compute the relationships between labels. This allows LaMP to assign different importance to each label interaction, effectively learning how labels relate to each other. The result is a simple, accurate, and interpretable model that can be applied to a wide range of multi-label classification tasks.

**Key Benefits**

* **Improved Accuracy**: LaMP outperforms state-of-the-art results on seven real-world datasets.
* **Interpretability**: LaMP provides insights into how each label depends on the input data and its interactions with other labels.
* **Flexibility**: LaMP is structure-agnostic and can be applied to various input/output types.

**What's Next**

The LaMP code and datasets are now available open-source, making it easy for researchers and developers to build upon this innovation. This breakthrough has the potential to enhance various applications, such as content categorization, recommendation systems, and more."
stat.ML,Sparseout: Controlling Sparsity in Deep Networks,"Dropout is commonly used to help reduce overfitting in deep neural networks. Sparsity is a potentially important property of neural networks, but is not explicitly controlled by Dropout-based regularization. In this work, we propose Sparseout a simple and efficient variant of Dropout that can be used to control the sparsity of the activations in a neural network. We theoretically prove that Sparseout is equivalent to an $L_q$ penalty on the features of a generalized linear model and that Dropout is a special case of Sparseout for neural networks. We empirically demonstrate that Sparseout is computationally inexpensive and is able to control the desired level of sparsity in the activations. We evaluated Sparseout on image classification and language modelling tasks to see the effect of sparsity on these tasks. We found that sparsity of the activations is favorable for language modelling performance while image classification benefits from denser activations. Sparseout provides a way to investigate sparsity in state-of-the-art deep learning models. Source code for Sparseout could be found at \url{https://github.com/najeebkhan/sparseout}.",https://arxiv.org/pdf/1904.08050v1,2019-04-17T02:10:25Z,"Najeeb Khan, Ian Stavness","**Controlling Sparsity in Deep Learning Models**

Deep learning models are powerful tools for making predictions and classifications, but they can sometimes become too complex and ""overfit"" to the training data. A common technique used to prevent overfitting is called ""dropout,"" which randomly drops out certain neurons during training. However, dropout doesn't directly control the sparsity of the model's activations, which is the number of neurons that are actually firing.

Researchers have proposed a new technique called ""Sparseout,"" which is a variant of dropout that allows for explicit control over the sparsity of a neural network's activations. In simple terms, Sparseout helps to regulate how many neurons are firing at any given time, which can improve the model's performance.

The study found that Sparseout is a useful tool for investigating the role of sparsity in deep learning models. When tested on image classification and language modeling tasks, the researchers found that:

* Language models performed better when they had sparse activations (i.e., fewer neurons firing)
* Image classification models performed better when they had denser activations (i.e., more neurons firing)

The Sparseout technique is computationally efficient and easy to implement, making it a valuable tool for researchers and practitioners working with deep learning models. The source code for Sparseout is also publicly available, making it easy for others to try out and build upon this work."
stat.ML,Collaborative and Privacy-Preserving Machine Teaching via Consensus Optimization,"In this work, we define a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers. We focus on consensus super teaching. It aims at organizing distributed teachers to jointly select a compact while informative training subset from data hosted by the teachers to make a learner learn better. The challenges arise from three perspectives. First, the state-of-the-art pool-based super teaching method applies mixed-integer non-linear programming (MINLP) which does not scale well to very large data sets. Second, it is desirable to restrict data access of the teachers to only their own data during the collaboration stage to mitigate privacy leaks. Finally, the teaching collaboration should be communication-efficient since large communication overheads can cause synchronization delays between teachers.   To address these challenges, we formulate collaborative teaching as a consensus and privacy-preserving optimization process to minimize teaching risk. We theoretically demonstrate the necessity of collaboration between teachers for improving the learner's learning. Furthermore, we show that the proposed method enjoys a similar property as the Oracle property of adaptive Lasso. The empirical study illustrates that our teaching method can deliver significantly more accurate teaching results with high speed, while the non-collaborative MINLP-based super teaching becomes prohibitively expensive to compute.",https://arxiv.org/pdf/1905.02796v1,2019-05-07T20:15:31Z,"Yufei Han, Yuzhe Ma, Christopher Gates, Kevin Roundy, Yun Shen","**Collaborative Machine Teaching: A New Approach to Protecting Data Privacy**

Imagine a scenario where multiple teachers want to work together to help a student learn a new concept, but each teacher has their own private data that they don't want to share with others. This is a challenge in machine learning, where teachers (or data providers) need to collaborate to train a model without revealing their sensitive data.

Researchers have proposed a new approach called ""collaborative and privacy-preserving machine teaching"" to address this challenge. This approach allows multiple teachers to work together to select a small, informative subset of data to teach a learner, while keeping their individual data private.

The new method, called ""consensus super teaching,"" enables teachers to collaborate without sharing their data, reducing the risk of data leaks and minimizing communication overhead. The researchers showed that this approach not only protects data privacy but also leads to more accurate teaching results, and can do so much faster than traditional methods.

This breakthrough has significant implications for various applications, such as healthcare, finance, and education, where data privacy is a major concern. By enabling collaborative machine teaching while preserving data privacy, this approach can lead to more effective and efficient learning models."
stat.ML,Decision Making with Machine Learning and ROC Curves,The Receiver Operating Characteristic (ROC) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data science. This paper studies the statistical properties of ROC curves and its implication on model selection. We analyze the implications of different models of incentive heterogeneity and information asymmetry on the relation between human decisions and the ROC curves. Our theoretical discussion is illustrated in the context of a large data set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy Checkups of reproductive age couples in Henan Province provided by the Chinese Ministry of Health.,https://arxiv.org/pdf/1905.02810v1,2019-05-05T08:01:23Z,"Kai Feng, Han Hong, Ke Tang, Jingyuan Wang","Here's a summary of the research paper for a general audience:

**Using Machine Learning to Make Better Decisions**

Imagine you're a doctor trying to decide whether a pregnant woman is at risk for complications. You have to make a decision based on limited information, and you want to make sure you're making the right call. Machine learning can help with this kind of decision-making, but it's not always easy to know which machine learning model to use.

This study explores a tool called the Receiver Operating Characteristic (ROC) curve, which helps evaluate how well a machine learning model can make predictions. The researchers analyzed how different factors, such as doctor biases and limited information, can affect the accuracy of these predictions.

Using a large dataset of pregnancy outcomes and doctor diagnoses from China, the researchers found that understanding the statistical properties of ROC curves is crucial for selecting the best machine learning model. This is important because it can help doctors and other decision-makers use machine learning to make more accurate predictions and better decisions.

In simple terms, this study helps us understand how to use machine learning to make better decisions, especially in situations where there's uncertainty and limited information. By using ROC curves and understanding their limitations, we can develop more accurate models that lead to better outcomes."
stat.ML,Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review,"Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.",https://arxiv.org/pdf/1905.02845v1,2019-05-07T23:41:34Z,"Benyamin Ghojogh, Maria N. Samad, Sayema Asif Mashhadi, Tania Kapoor, Wahab Ali, Fakhri Karray, Mark Crowley","**Simplifying Data Analysis: A Review of Feature Selection and Extraction Methods**

When analyzing complex data, it's often helpful to simplify it first. This is where feature selection and extraction come in - techniques that help identify the most important parts of the data, making it easier to classify, predict, or group. Think of it like trying to understand a cluttered room; you might focus on specific objects or areas to make sense of the space.

This literature review explores various methods for feature selection and extraction, explaining their theory, motivation, and applications. The authors also provide examples of how these methods work and compare their strengths. By streamlining data analysis, these techniques can improve the accuracy and efficiency of various applications, from predicting outcomes to identifying patterns. The review aims to provide a comprehensive overview of these methods, helping researchers and practitioners choose the best approach for their specific needs."
stat.ML,A Generative Model for Sampling High-Performance and Diverse Weights for Neural Networks,"Recent work on mode connectivity in the loss landscape of deep neural networks has demonstrated that the locus of (sub-)optimal weight vectors lies on continuous paths. In this work, we train a neural network that serves as a hypernetwork, mapping a latent vector into high-performance (low-loss) weight vectors, generalizing recent findings of mode connectivity to higher dimensional manifolds. We formulate the training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We demonstrate how to reduce the number of parameters in the hypernetwork by parameter sharing. Once learned, the hypernetwork allows for a computationally efficient, ancestral sampling of neural network weights, which we recruit to form large ensembles. The improvement in classification accuracy obtained by this ensembling indicates that the generated manifold extends in dimensions other than directions implied by trivial symmetries. For computational efficiency, we distill an ensemble into a single classifier while retaining generalization.",https://arxiv.org/pdf/1905.02898v1,2019-05-07T04:28:46Z,"Lior Deutsch, Erik Nijkamp, Yu Yang","**Unlocking Diverse and High-Performing AI Models with a New Generative Model**

Imagine being able to generate a vast array of highly accurate artificial intelligence (AI) models, each with its own unique strengths and weaknesses. Researchers have made a breakthrough in achieving this goal by developing a new type of AI model called a ""generative model"" or ""hypernetwork."" This model can produce a wide range of high-performing AI models, similar to how a master chef can whip up a variety of delicious dishes using a single recipe.

The key innovation here is that the hypernetwork can generate multiple AI models that are not only highly accurate but also diverse in their internal workings. This diversity is crucial because it allows the generated models to excel in different areas, much like how different people have different strengths and weaknesses.

The researchers trained the hypernetwork to balance two competing goals: accuracy and diversity. They achieved this by introducing a new objective that encourages the hypernetwork to produce models that are not only accurate but also distinct from one another. The result is a hypernetwork that can efficiently generate a large ensemble of high-performing AI models.

The benefits of this approach are two-fold. Firstly, the generated models can be combined to form a single, highly accurate AI model that outperforms any individual model. Secondly, the hypernetwork can generate new models on the fly, making it a computationally efficient solution for a wide range of applications.

The implications of this research are significant, as it could lead to the development of more robust and adaptable AI systems. For instance, in the field of computer vision, a hypernetwork could generate multiple models that excel in different areas, such as image classification, object detection, and image segmentation. Similarly, in natural language processing, a hypernetwork could generate models that are highly accurate in different languages or domains.

Overall, this breakthrough has the potential to revolutionize the field of AI by providing a powerful tool for generating high-performing and diverse AI models."
stat.ML,Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy,"Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of different coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classification are unscalable and cannot accommodate large scale EHR datasets.   In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). The system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: first, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and finally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed the robustness and generalisation properties of Text2Node by mapping ICD-9-CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy.   This system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in healthcare. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models.",https://arxiv.org/pdf/1905.01958v1,2019-04-11T17:31:23Z,"Rohollah Soltani, Alexandre Tomberg","Here's a summary of the research paper for a general audience:

**Title:** Text2Node: A System to Map Medical Phrases to a Standardized Taxonomy

**Problem:** Electronic health records (EHRs) are digital versions of a patient's medical history. However, different EHR systems use different coding standards, making it difficult to share and compare data between systems.

**Solution:** Researchers have developed a system called Text2Node, which can automatically map medical phrases to a standardized taxonomy, such as SNOMED CT. This taxonomy is a large database of medical concepts that helps healthcare professionals communicate and share information.

**How it works:** Text2Node uses artificial intelligence and natural language processing to learn from a small set of training data and map medical phrases to concepts in the taxonomy. The system consists of three main stages:

1. **Mapping words to numbers**: The system converts words into numerical representations, called word embeddings, which capture their meanings.
2. **Vectorizing the taxonomy**: The system represents the taxonomy as a set of numerical vectors, called node embeddings, which capture the relationships between concepts.
3. **Connecting the two**: The system trains a mapping function to connect the word embeddings to the node embeddings, enabling it to map medical phrases to concepts in the taxonomy.

**Benefits:** Text2Node is scalable, robust, and can handle variations in wording between different coding systems. It can also suggest relevant concepts even when no exact match exists in the taxonomy. This system has the potential to improve data sharing and comparison between EHR systems, leading to better clinical decision-making and research.

**Impact:** By applying Text2Node, researchers can generate embeddings that incorporate medical knowledge from EHRs, which can improve clinical predictive models and ultimately lead to better patient outcomes."
stat.ML,Who wrote this book? A challenge for e-commerce,"Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.",https://arxiv.org/pdf/1905.01973v1,2019-04-19T10:13:07Z,"Béranger Dumont, Simona Maggio, Ghiles Sidi Said, Quoc-Tien Au","**The Author Identification Challenge in E-commerce**

Imagine you're browsing an online bookstore and searching for books by your favorite author, F. Scott Fitzgerald. However, due to variations in how his name is listed - such as abbreviations, misspellings, or different formats - you might not find all his books together. This is a common problem in e-commerce, particularly in the book category.

Researchers have developed a system to tackle this challenge. They combined data from open sources about books with advanced machine learning techniques, a type of artificial intelligence that enables computers to learn from data. Specifically, they used two types of neural networks:

1. **Siamese neural networks**: These help identify similar author names, even if they're not exact matches.
2. **Sequence-to-sequence learning**: This technique corrects errors in author names by suggesting the most likely correct name.

The researchers tested their system on data from the e-commerce website Rakuten France and achieved a 72% accuracy rate in suggesting the correct author name. This means that in most cases, the system was able to correctly identify the author, even when the name was listed differently.

This breakthrough has the potential to improve the online shopping experience by making it easier for customers to find books by their favorite authors. By accurately identifying author names, e-commerce platforms can provide more accurate search results, recommend relevant books, and enhance overall customer satisfaction."
stat.ML,A Content-Based Approach to Email Triage Action Prediction: Exploration and Evaluation,"Email has remained a principal form of communication among people, both in enterprise and social settings. With a deluge of emails crowding our mailboxes daily, there is a dire need of smart email systems that can recover important emails and make personalized recommendations. In this work, we study the problem of predicting user triage actions to incoming emails where we take the reply prediction as a working example. Different from existing methods, we formulate the triage action prediction as a recommendation problem and focus on the content-based approach, where the users are represented using the content of current and past emails. We also introduce additional similarity features to further explore the affinities between users and emails. Experiments on the publicly available Avocado email collection demonstrate the advantages of our proposed recommendation framework and our method is able to achieve better performance compared to the state-of-the-art deep recommendation methods. More importantly, we provide valuable insight into the effectiveness of different textual and user representations and show that traditional bag-of-words approaches, with the help from the similarity features, compete favorably with the more advanced neural embedding methods.",https://arxiv.org/pdf/1905.01991v1,2019-04-30T01:52:57Z,"Sudipto Mukherjee, Ke Jiang","**Smart Email Systems: A New Approach to Managing Your Inbox**

Are you tired of drowning in a sea of emails? Researchers have been working on developing smart email systems that can help you prioritize and manage your inbox more efficiently. A recent study explores a new approach to predicting how users will respond to incoming emails, such as whether they will reply or not.

The researchers formulated this problem as a recommendation task, similar to how online shopping platforms suggest products based on your past purchases. They focused on using the content of emails to understand user behavior, rather than relying on complex algorithms. By analyzing the text of current and past emails, the system can learn to identify which emails are likely to be important to a user.

The study found that this content-based approach is effective in predicting user responses to emails. In fact, it outperformed more advanced methods that use complex neural networks. The researchers also discovered that traditional methods of analyzing text, such as counting the frequency of words, can be just as effective as more modern approaches when combined with additional features that capture the similarities between users and emails.

This research has the potential to lead to the development of smarter email systems that can help users prioritize their emails and reduce the stress of managing a crowded inbox."
stat.ML,A Persona-based Multi-turn Conversation Model in an Adversarial Learning Framework,"In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN ($phredGAN$) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n-gram scores.",https://arxiv.org/pdf/1905.01998v1,2019-04-29T15:09:34Z,"Oluwatobi O. Olabiyi, Anish Khazane, Erik T. Mueller","**Advances in Chatbot Technology: A New Model for More Human-Like Conversations**

Researchers have made a breakthrough in developing chatbots that can engage in more natural and human-like conversations. They've created a new model called persona hredGAN (phredGAN), which enables chatbots to understand and respond to multi-turn conversations more effectively.

The phredGAN model builds on existing technology by incorporating additional information about the speaker, such as their identity, location, and topic of discussion. This allows the chatbot to better understand the context of the conversation and respond more accurately.

In tests, the phredGAN model outperformed other state-of-the-art models on various datasets, including TV show transcripts and customer service interactions. The results showed improvements in measures such as perplexity, BLEU, ROUGE, and Distinct n-gram scores.

This advancement has the potential to lead to more realistic and engaging chatbot interactions, which can be applied to various fields, such as customer service, language learning, and entertainment. The phredGAN model brings us closer to developing chatbots that can converse with humans in a more natural and intuitive way."
