category,title,summary,link,published,authors
cs.LG,"Hierarchical Reinforcement Learning with the MAXQ Value Function
  Decomposition","This paper presents the MAXQ approach to hierarchical reinforcement learning
based on decomposing the target Markov decision process (MDP) into a hierarchy
of smaller MDPs and decomposing the value function of the target MDP into an
additive combination of the value functions of the smaller MDPs. The paper
defines the MAXQ hierarchy, proves formal results on its representational
power, and establishes five conditions for the safe use of state abstractions.
The paper presents an online model-free learning algorithm, MAXQ-Q, and proves
that it converges wih probability 1 to a kind of locally-optimal policy known
as a recursively optimal policy, even in the presence of the five kinds of
state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q
through a series of experiments in three domains and shows experimentally that
MAXQ-Q (with state abstractions) converges to a recursively optimal policy much
faster than flat Q learning. The fact that MAXQ learns a representation of the
value function has an important benefit: it makes it possible to compute and
execute an improved, non-hierarchical policy via a procedure similar to the
policy improvement step of policy iteration. The paper demonstrates the
effectiveness of this non-hierarchical execution experimentally. Finally, the
paper concludes with a comparison to related work and a discussion of the
design tradeoffs in hierarchical reinforcement learning.",http://arxiv.org/abs/cs/9905014v1,1999-05-21T14:26:07Z,['Thomas G. Dietterich']
cs.LG,State Abstraction in MAXQ Hierarchical Reinforcement Learning,"Many researchers have explored methods for hierarchical reinforcement
learning (RL) with temporal abstractions, in which abstract actions are defined
that can perform many primitive actions before terminating. However, little is
known about learning with state abstractions, in which aspects of the state
space are ignored. In previous work, we developed the MAXQ method for
hierarchical RL. In this paper, we define five conditions under which state
abstraction can be combined with the MAXQ value function decomposition. We
prove that the MAXQ-Q learning algorithm converges under these conditions and
show experimentally that state abstraction is important for the successful
application of MAXQ-Q learning.",http://arxiv.org/abs/cs/9905015v1,1999-05-21T14:49:39Z,['Thomas G. Dietterich']
cs.LG,"Multiplicative Algorithm for Orthgonal Groups and Independent Component
  Analysis","The multiplicative Newton-like method developed by the author et al. is
extended to the situation where the dynamics is restricted to the orthogonal
group. A general framework is constructed without specifying the cost function.
Though the restriction to the orthogonal groups makes the problem somewhat
complicated, an explicit expression for the amount of individual jumps is
obtained. This algorithm is exactly second-order-convergent. The global
instability inherent in the Newton method is remedied by a
Levenberg-Marquardt-type variation. The method thus constructed can readily be
applied to the independent component analysis. Its remarkable performance is
illustrated by a numerical simulation.",http://arxiv.org/abs/cs/0001004v1,2000-01-07T06:20:53Z,['Toshinao Akuzawa']
cs.LG,Multiplicative Nonholonomic/Newton -like Algorithm,"We construct new algorithms from scratch, which use the fourth order cumulant
of stochastic variables for the cost function. The multiplicative updating rule
here constructed is natural from the homogeneous nature of the Lie group and
has numerous merits for the rigorous treatment of the dynamics. As one
consequence, the second order convergence is shown. For the cost function,
functions invariant under the componentwise scaling are choosen. By identifying
points which can be transformed to each other by the scaling, we assume that
the dynamics is in a coset space. In our method, a point can move toward any
direction in this coset. Thus, no prewhitening is required.",http://arxiv.org/abs/cs/0002006v1,2000-02-09T06:44:28Z,"['Toshinao Akuzawa', 'Noboru Murata']"
cs.LG,Complexity analysis for algorithmically simple strings,"Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory.",http://arxiv.org/abs/cs/0009001v3,2000-09-05T18:54:58Z,['Andrei N. Soklakov']
cs.LG,Robust Classification for Imprecise Environments,"In real-world environments it usually is difficult to specify target
operating conditions precisely, for example, target misclassification costs.
This uncertainty makes building robust classification systems problematic. We
show that it is possible to build a hybrid classifier that will perform at
least as well as the best available classifier for any target conditions. In
some cases, the performance of the hybrid actually can surpass that of the best
known classifier. This robust performance extends across a wide variety of
comparison frameworks, including the optimization of metrics such as accuracy,
expected cost, lift, precision, recall, and workforce utilization. The hybrid
also is efficient to build, to store, and to update. The hybrid is based on a
method for the comparison of classifier performance that is robust to imprecise
class distributions and misclassification costs. The ROC convex hull (ROCCH)
method combines techniques from ROC analysis, decision analysis and
computational geometry, and adapts them to the particulars of analyzing learned
classifiers. The method is efficient and incremental, minimizes the management
of classifier performance data, and allows for clear visual comparisons and
sensitivity analyses. Finally, we point to empirical evidence that a robust
hybrid classifier indeed is needed for many real-world problems.",http://arxiv.org/abs/cs/0009007v1,2000-09-13T21:09:47Z,"['Foster Provost', 'Tom Fawcett']"
cs.LG,Top-down induction of clustering trees,"An approach to clustering is presented that adapts the basic top-down
induction of decision trees method towards clustering. To this aim, it employs
the principles of instance based learning. The resulting methodology is
implemented in the TIC (Top down Induction of Clustering trees) system for
first order clustering. The TIC system employs the first order logical decision
tree representation of the inductive logic programming system Tilde. Various
experiments with TIC are presented, in both propositional and relational
domains.",http://arxiv.org/abs/cs/0011032v1,2000-11-21T21:51:01Z,"['Hendrik Blockeel', 'Luc De Raedt', 'Jan Ramon']"
cs.LG,Scaling Up Inductive Logic Programming by Learning from Interpretations,"When comparing inductive logic programming (ILP) and attribute-value learning
techniques, there is a trade-off between expressive power and efficiency.
Inductive logic programming techniques are typically more expressive but also
less efficient. Therefore, the data sets handled by current inductive logic
programming systems are small according to general standards within the data
mining community. The main source of inefficiency lies in the assumption that
several examples may be related to each other, so they cannot be handled
independently.
  Within the learning from interpretations framework for inductive logic
programming this assumption is unnecessary, which allows to scale up existing
ILP algorithms. In this paper we explain this learning setting in the context
of relational databases. We relate the setting to propositional data mining and
to the classical ILP setting, and show that learning from interpretations
corresponds to learning from multiple relations and thus extends the
expressiveness of propositional learning, while maintaining its efficiency to a
large extent (which is not the case in the classical ILP setting).
  As a case study, we present two alternative implementations of the ILP system
Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which
loads all data in main memory, and Tilde-LDS, which loads the examples one by
one. We experimentally compare the implementations, showing Tilde-LDS can
handle large data sets (in the order of 100,000 examples or 100 MB) and indeed
scales up linearly in the number of examples.",http://arxiv.org/abs/cs/0011044v1,2000-11-29T12:14:50Z,"['Hendrik Blockeel', 'Luc De Raedt', 'Nico Jacobs', 'Bart Demoen']"
cs.LG,Learning Policies with External Memory,"In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems.",http://arxiv.org/abs/cs/0103003v1,2001-03-02T01:55:46Z,"['Leonid Peshkin', 'Nicolas Meuleau', 'Leslie Kaelbling']"
cs.LG,Efficient algorithms for decision tree cross-validation,"Cross-validation is a useful and generally applicable technique often
employed in machine learning, including decision tree induction. An important
disadvantage of straightforward implementation of the technique is its
computational overhead. In this paper we show that, for decision trees, the
computational overhead of cross-validation can be reduced significantly by
integrating the cross-validation with the normal decision tree induction
process. We discuss how existing decision tree algorithms can be adapted to
this aim, and provide an analysis of the speedups these adaptations may yield.
The analysis is supported by experimental results.",http://arxiv.org/abs/cs/0110036v1,2001-10-17T15:45:23Z,"['Hendrik Blockeel', 'Jan Struyf']"
cs.LG,"Evaluation of the Performance of the Markov Blanket Bayesian Classifier
  Algorithm","The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for
construction of probabilistic classifiers. This paper presents an empirical
comparison of the MBBC algorithm with three other Bayesian classifiers: Naive
Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these
are implemented using the K2 framework of Cooper and Herskovits. The
classifiers are compared in terms of their performance (using simple accuracy
measures and ROC curves) and speed, on a range of standard benchmark data sets.
It is concluded that MBBC is competitive in terms of speed and accuracy with
the other algorithms considered.",http://arxiv.org/abs/cs/0211003v1,2002-11-01T18:09:56Z,['Michael G. Madden']
cs.LG,Approximating Incomplete Kernel Matrices by the em Algorithm,"In biological data, it is often the case that observed data are available
only for a subset of samples. When a kernel matrix is derived from such data,
we have to leave the entries for unavailable samples as missing. In this paper,
we make use of a parametric model of kernel matrices, and estimate missing
entries by fitting the model to existing entries. The parametric model is
created as a set of spectral variants of a complete kernel matrix derived from
another information source. For model fitting, we adopt the em algorithm based
on the information geometry of positive definite matrices. We will report
promising results on bacteria clustering experiments using two marker
sequences: 16S and gyrB.",http://arxiv.org/abs/cs/0211007v1,2002-11-07T07:21:58Z,"['Koji Tsuda', 'Shotaro Akaho', 'Kiyoshi Asai']"
cs.LG,"Reliable and Efficient Inference of Bayesian Networks from Sparse Data
  by Statistical Learning Theory","To learn (statistical) dependencies among random variables requires
exponentially large sample size in the number of observed random variables if
any arbitrary joint probability distribution can occur.
  We consider the case that sparse data strongly suggest that the probabilities
can be described by a simple Bayesian network, i.e., by a graph with small
in-degree \Delta. Then this simple law will also explain further data with high
confidence. This is shown by calculating bounds on the VC dimension of the set
of those probability measures that correspond to simple graphs. This allows to
select networks by structural risk minimization and gives reliability bounds on
the error of the estimated joint measure without (in contrast to a previous
paper) any prior assumptions on the set of possible joint measures.
  The complexity for searching the optimal Bayesian networks of in-degree
\Delta increases only polynomially in the number of random varibales for
constant \Delta and the optimal joint measure associated with a given graph can
be found by convex optimization.",http://arxiv.org/abs/cs/0309015v1,2003-09-10T13:56:41Z,"['Dominik Janzing', 'Daniel Herrmann']"
cs.LG,Toward Attribute Efficient Learning Algorithms,"We make progress on two important problems regarding attribute efficient
learnability.
  First, we give an algorithm for learning decision lists of length $k$ over
$n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time
$n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision
lists that has both subexponential sample complexity and subexponential running
time in the relevant parameters. Our approach establishes a relationship
between attribute efficient learning and polynomial threshold functions and is
based on a new construction of low degree, low weight polynomial threshold
functions for decision lists. For a wide range of parameters our construction
matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives
an essentially optimal tradeoff between polynomial threshold function degree
and weight.
  Second, we give an algorithm for learning an unknown parity function on $k$
out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.
For $k=o(\log n)$ this yields a polynomial time algorithm with sample
complexity $o(n)$. This is the first polynomial time algorithm for learning
parity on a superconstant number of variables with sublinear sample complexity.",http://arxiv.org/abs/cs/0311042v1,2003-11-27T05:34:04Z,"['Adam R. Klivans', 'Rocco A. Servedio']"
cs.LG,"Improving spam filtering by combining Naive Bayes with simple k-nearest
  neighbor searches","Using naive Bayes for email classification has become very popular within the
last few months. They are quite easy to implement and very efficient. In this
paper we want to present empirical results of email classification using a
combination of naive Bayes and k-nearest neighbor searches. Using this
technique we show that the accuracy of a Bayes filter can be improved slightly
for a high number of features and significantly for a small number of features.",http://arxiv.org/abs/cs/0312004v1,2003-11-30T20:41:18Z,['Daniel Etzold']
cs.LG,About Unitary Rating Score Constructing,"It is offered to pool test points of different subjects and different aspects
of the same subject together in order to get the unitary rating score, by the
way of nonlinear transformation of indicator points in accordance with Zipf's
distribution. It is proposed to use the well-studied distribution of
Intellectuality Quotient IQ as the reference distribution for latent variable
""progress in studies"".",http://arxiv.org/abs/cs/0401005v1,2004-01-08T07:50:51Z,['Kromer Victor']
cs.LG,"Mining Heterogeneous Multivariate Time-Series for Learning Meaningful
  Patterns: Application to Home Health Telecare","For the last years, time-series mining has become a challenging issue for
researchers. An important application lies in most monitoring purposes, which
require analyzing large sets of time-series for learning usual patterns. Any
deviation from this learned profile is then considered as an unexpected
situation. Moreover, complex applications may involve the temporal study of
several heterogeneous parameters. In that paper, we propose a method for mining
heterogeneous multivariate time-series for learning meaningful patterns. The
proposed approach allows for mixed time-series -- containing both pattern and
non-pattern data -- such as for imprecise matches, outliers, stretching and
global translating of patterns instances in time. We present the early results
of our approach in the context of monitoring the health status of a person at
home. The purpose is to build a behavioral profile of a person by analyzing the
time variations of several quantitative or qualitative parameters recorded
through a provision of sensors installed in the home.",http://arxiv.org/abs/cs/0412003v1,2004-12-01T16:32:49Z,"['Florence Duchene', 'Catherine Garbay', 'Vincent Rialle']"
cs.LG,Stability Analysis for Regularized Least Squares Regression,"We discuss stability for a class of learning algorithms with respect to noisy
labels. The algorithms we consider are for regression, and they involve the
minimization of regularized risk functionals, such as L(f) := 1/N sum_i
(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when
y_i is a noisy version of f*(x_i) for some function f* in H, the output of the
algorithm converges to f* as the regularization term and noise simultaneously
vanish. We consider two flavors of this problem, one where a data set of N
points remains fixed, and the other where N -> infinity. For the case where N
-> infinity, we give conditions for convergence to f_E (the function which is
the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we
describe the limiting 'non-noisy', 'non-regularized' function f*, and give
conditions for convergence. In the process, we develop a set of tools for
dealing with functionals such as L(f), which are applicable to many other
problems in learning theory.",http://arxiv.org/abs/cs/0502016v1,2005-02-03T19:54:02Z,['Cynthia Rudin']
cs.LG,Probabilistic and Team PFIN-type Learning: General Properties,"We consider the probability hierarchy for Popperian FINite learning and study
the general properties of this hierarchy. We prove that the probability
hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and
p_2 and answers whether PFIN-type learning with the probability of success p_1
is equivalent to PFIN-type learning with the probability of success p_2.
  To prove our result, we analyze the topological structure of the probability
hierarchy. We prove that it is well-ordered in descending ordering and
order-equivalent to ordinal epsilon_0. This shows that the structure of the
hierarchy is very complicated.
  Using similar methods, we also prove that, for PFIN-type learning, team
learning and probabilistic learning are of the same power.",http://arxiv.org/abs/cs/0504001v1,2005-03-31T23:04:28Z,['Andris Ambainis']
cs.LG,Non-asymptotic calibration and resolution,"We analyze a new algorithm for probability forecasting of binary observations
on the basis of the available data, without making any assumptions about the
way the observations are generated. The algorithm is shown to be well
calibrated and to have good resolution for long enough sequences of
observations and for a suitable choice of its parameter, a kernel on the
Cartesian product of the forecast space $[0,1]$ and the data space. Our main
results are non-asymptotic: we establish explicit inequalities, shown to be
tight, for the performance of the algorithm.",http://arxiv.org/abs/cs/0506004v4,2005-06-01T14:03:20Z,['Vladimir Vovk']
cs.CV,A Linear Shift Invariant Multiscale Transform,"This paper presents a multiscale decomposition algorithm. Unlike standard
wavelet transforms, the proposed operator is both linear and shift invariant.
The central idea is to obtain shift invariance by averaging the aligned wavelet
transform projections over all circular shifts of the signal. It is shown how
the same transform can be obtained by a linear filter bank.",http://arxiv.org/abs/cs/9810003v1,1998-10-02T03:34:38Z,['Andreas Siebert']
cs.CV,General Theory of Image Normalization,"We give a systematic, abstract formulation of the image normalization method
as applied to a general group of image transformations, and then illustrate the
abstract analysis by applying it to the hierarchy of viewing transformations of
a planar object.",http://arxiv.org/abs/cs/9810017v1,1998-10-19T20:46:16Z,['Stephen L. Adler']
cs.CV,A Differential Invariant for Zooming,"This paper presents an invariant under scaling and linear brightness change.
The invariant is based on differentials and therefore is a local feature.
Rotationally invariant 2-d differential Gaussian operators up to third order
are proposed for the implementation of the invariant. The performance is
analyzed by simulating a camera zoom-out.",http://arxiv.org/abs/cs/9908017v1,1999-08-26T17:18:49Z,['Andreas Siebert']
cs.CV,A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images,"We describe a simple, but efficient algorithm for the generation of dilated
contours from bilevel images. The initial part of the contour extraction is
explained to be a good candidate for parallel computer code generation. The
remainder of the algorithm is of linear nature.",http://arxiv.org/abs/cs/0001024v1,2000-01-25T16:09:37Z,"['B. R. Schlei', 'L. Prasad']"
cs.CV,"Image Compression with Iterated Function Systems, Finite Automata and
  Zerotrees: Grand Unification","Fractal image compression, Culik's image compression and zerotree prediction
coding of wavelet image decomposition coefficients succeed only because typical
images being compressed possess a significant degree of self-similarity.
Besides the common concept, these methods turn out to be even more tightly
related, to the point of algorithmical reducibility of one technique to
another. The goal of the present paper is to demonstrate these relations.
  The paper offers a plain-term interpretation of Culik's image compression, in
regular image processing terms, without resorting to finite state machines and
similar lofty language. The interpretation is shown to be algorithmically
related to an IFS fractal image compression method: an IFS can be exactly
transformed into Culik's image code. Using this transformation, we will prove
that in a self-similar (part of an) image any zero wavelet coefficient is the
root of a zerotree, or its branch.
  The paper discusses the zerotree coding of (wavelet/projection) coefficients
as a common predictor/corrector, applied vertically through different layers of
a multiresolutional decomposition, rather than within the same view. This
interpretation leads to an insight into the evolution of image compression
techniques: from a causal single-layer prediction, to non-causal same-view
predictions (wavelet decomposition among others) and to a causal cross-layer
prediction (zero-trees, Culik's method).",http://arxiv.org/abs/cs/0003065v1,2000-03-15T19:31:51Z,"['Oleg Kiselyov', 'Paul Fisher']"
cs.CV,Differential Invariants under Gamma Correction,"This paper presents invariants under gamma correction and similarity
transformations. The invariants are local features based on differentials which
are implemented using derivatives of the Gaussian. The use of the proposed
invariant representation is shown to yield improved correlation results in a
template matching scenario.",http://arxiv.org/abs/cs/0003079v1,2000-03-26T23:18:43Z,['Andreas Siebert']
cs.CV,"Assisted Video Sequences Indexing : Motion Analysis Based on Interest
  Points","This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.",http://arxiv.org/abs/cs/0004012v1,2000-04-21T17:32:29Z,"['Emmanuel Etievent', 'Frank Lebourgeois', 'Jean-Michel Jolion']"
cs.CV,Robustness of Regional Matching Scheme over Global Matching Scheme,"The paper has established and verified the theory prevailing widely among
image and pattern recognition specialists that the bottom-up indirect regional
matching process is the more stable and the more robust than the global
matching process against concentrated types of noise represented by clutter,
outlier or occlusion in the imagery. We have demonstrated this by analyzing the
effect of concentrated noise on a typical decision making process of a
simplified two candidate voting model where our theorem establishes the lower
bounds to a critical breakdown point of election (or decision) result by the
bottom-up matching process are greater than the exact bound of the global
matching process implying that the former regional process is capable of
accommodating a higher level of noise than the latter global process before the
result of decision overturns. We present a convincing experimental verification
supporting not only the theory by a white-black flag recognition problem in the
presence of localized noise but also the validity of the conjecture by a facial
recognition problem that the theorem remains valid for other decision making
processes involving an important dimension-reducing transform such as principal
component analysis or a Gabor transform.",http://arxiv.org/abs/cs/0005001v1,2000-05-03T08:49:28Z,"['Liang Chen', 'Naoyuki Tokuda']"
cs.CV,Boosting the Differences: A fast Bayesian classifier neural network,"A Bayesian classifier that up-weights the differences in the attribute values
is discussed. Using four popular datasets from the UCI repository, some
interesting features of the network are illustrated. The network is suitable
for classification problems.",http://arxiv.org/abs/cs/0006001v1,2000-05-31T23:37:48Z,"['Ninan Sajeeth Philip', 'K. Babu Joseph']"
cs.CV,"Distorted English Alphabet Identification : An application of Difference
  Boosting Algorithm","The difference-boosting algorithm is used on letters dataset from the UCI
repository to classify distorted raster images of English alphabets. In
contrast to rather complex networks, the difference-boosting is found to
produce comparable or better classification efficiency on this complex problem.",http://arxiv.org/abs/cs/0006002v1,2000-05-31T23:52:31Z,"['Ninan Sajeeth Philip', 'K. Babu Joseph']"
cs.CV,Geometric Morphology of Granular Materials,"We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials.",http://arxiv.org/abs/cs/0006047v1,2000-06-30T22:17:42Z,"['B. R. Schlei', 'L. Prasad', 'A. N. Skourikhine']"
cs.CV,Probabilistic Search for Object Segmentation and Recognition,"The problem of searching for a model-based scene interpretation is analyzed
within a probabilistic framework. Object models are formulated as generative
models for range data of the scene. A new statistical criterion, the truncated
object probability, is introduced to infer an optimal sequence of object
hypotheses to be evaluated for their match to the data. The truncated
probability is partly determined by prior knowledge of the objects and partly
learned from data. Some experiments on sequence quality and object segmentation
and recognition from stereo data are presented. The article recovers classic
concepts from object recognition (grouping, geometric hashing, alignment) from
the probabilistic perspective and adds insight into the optimal ordering of
object hypotheses for evaluation. Moreover, it introduces point-relation
densities, a key component of the truncated probability, as statistical models
of local surface shape.",http://arxiv.org/abs/cs/0208005v1,2002-08-05T10:57:09Z,"['Ulrich Hillenbrand', 'Gerd Hirzinger']"
cs.CV,Least squares fitting of circles and lines,"We study theoretical and computational aspects of the least squares fit (LSF)
of circles and circular arcs. First we discuss the existence and uniqueness of
LSF and various parametrization schemes. Then we evaluate several popular
circle fitting algorithms and propose a new one that surpasses the existing
methods in reliability. We also discuss and compare direct (algebraic) circle
fits.",http://arxiv.org/abs/cs/0301001v1,2003-01-01T19:58:03Z,"['N. Chernov', 'C. Lesort']"
cs.CV,Statistical efficiency of curve fitting algorithms,"We study the problem of fitting parametrized curves to noisy data. Under
certain assumptions (known as Cartesian and radial functional models), we
derive asymptotic expressions for the bias and the covariance matrix of the
parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower
bound, which he proved for unbiased estimates only, to more general estimates
that include many popular algorithms (most notably, the orthogonal least
squares and algebraic fits). We then show that the gradient-weighted algebraic
fit is statistically efficient and describe all other statistically efficient
algebraic fits.",http://arxiv.org/abs/cs/0303015v1,2003-03-18T21:30:36Z,"['N. Chernov', 'C. Lesort']"
cs.CV,"Flexible Camera Calibration Using a New Analytical Radial Undistortion
  Formula with Application to Mobile Robot Localization","Most algorithms in 3D computer vision rely on the pinhole camera model
because of its simplicity, whereas virtually all imaging devices introduce
certain amount of nonlinear distortion, where the radial distortion is the most
severe part. Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved. An application of the new radial distortion model is non-iterative
yellow line alignment with a calibrated camera on ODIS, a robot built in our
CSOIS.",http://arxiv.org/abs/cs/0307045v1,2003-07-20T02:35:38Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.CV,A New Analytical Radial Distortion Model for Camera Calibration,"Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved.",http://arxiv.org/abs/cs/0307046v1,2003-07-20T05:18:59Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.CV,Rational Radial Distortion Models with Analytical Undistortion Formulae,"The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
class of rational radial distortion models with easy analytical undistortion
formulae. Experimental results are presented to show that with this class of
rational radial distortion models, satisfactory and comparable accuracy is
achieved.",http://arxiv.org/abs/cs/0307047v1,2003-07-20T05:54:42Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.CV,"An Analytical Piecewise Radial Distortion Model for Precision Camera
  Calibration","The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
piecewise radial distortion model with easy analytical undistortion formula.
The motivation for seeking a piecewise radial distortion model is that, when a
camera is resulted in a low quality during manufacturing, the nonlinear radial
distortion can be complex. Using low order polynomials to approximate the
radial distortion might not be precise enough. On the other hand, higher order
polynomials suffer from the inverse problem. With the new piecewise radial
distortion function, more flexibility is obtained and the radial undistortion
can be performed analytically. Experimental results are presented to show that
with this new piecewise radial distortion model, better performance is achieved
than that using the single function. Furthermore, a comparable performance with
the conventional polynomial model using 2 coefficients can also be
accomplished.",http://arxiv.org/abs/cs/0307051v1,2003-07-21T16:30:11Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.CV,Camera Calibration: a USU Implementation,"The task of camera calibration is to estimate the intrinsic and extrinsic
parameters of a camera model. Though there are some restricted techniques to
infer the 3-D information about the scene from uncalibrated cameras, effective
camera calibration procedures will open up the possibility of using a wide
range of existing algorithms for 3-D reconstruction and recognition.
  The applications of camera calibration include vision-based metrology, robust
visual platooning and visual docking of mobile robots where the depth
information is important.",http://arxiv.org/abs/cs/0307072v1,2003-07-31T19:33:48Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.CV,"A Family of Simplified Geometric Distortion Models for Camera
  Calibration","The commonly used radial distortion model for camera calibration is in fact
an assumption or a restriction. In practice, camera distortion could happen in
a general geometrical manner that is not limited to the radial sense. This
paper proposes a simplified geometrical distortion modeling method by using two
different radial distortion functions in the two image axes. A family of
simplified geometric distortion models is proposed, which are either simple
polynomials or the rational functions of polynomials. Analytical geometric
undistortion is possible using two of the distortion functions discussed in
this paper and their performance can be improved by applying a piecewise
fitting idea. Our experimental results show that the geometrical distortion
models always perform better than their radial distortion counterparts.
Furthermore, the proposed geometric modeling method is more appropriate for
cameras whose distortion is not perfectly radially symmetric around the center
of distortion.",http://arxiv.org/abs/cs/0308003v1,2003-08-02T01:39:38Z,"['Lili Ma', 'YangQuan Chen', 'Kevin L. Moore']"
cs.AI,Dynamic Backtracking,"Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches.",http://arxiv.org/abs/cs/9308101v1,1993-08-01T00:00:00Z,['M. L. Ginsberg']
cs.AI,"A Market-Oriented Programming Environment and its Application to
  Distributed Multicommodity Flow Problems","Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms.",http://arxiv.org/abs/cs/9308102v1,1993-08-01T00:00:00Z,['M. P. Wellman']
cs.AI,An Empirical Analysis of Search in GSAT,"We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms.",http://arxiv.org/abs/cs/9309101v1,1993-09-01T00:00:00Z,"['I. P. Gent', 'T. Walsh']"
cs.AI,The Difficulties of Learning Logic Programs with Cut,"As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages.",http://arxiv.org/abs/cs/9311101v1,1993-11-01T00:00:00Z,"['F. Bergadano', 'D. Gunetti', 'U. Trinchero']"
cs.AI,Software Agents: Completing Patterns and Constructing User Interfaces,"To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime.",http://arxiv.org/abs/cs/9311102v1,1993-11-01T00:00:00Z,"['J. C. Schlimmer', 'L. A. Hermens']"
cs.AI,Decidable Reasoning in Terminological Knowledge Representation Systems,"Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted.",http://arxiv.org/abs/cs/9312101v1,1993-12-01T00:00:00Z,"['M. Buchheit', 'F. M. Donini', 'A. Schaerf']"
cs.AI,Teleo-Reactive Programs for Agent Control,"A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots.",http://arxiv.org/abs/cs/9401101v1,1994-01-01T00:00:00Z,['N. Nilsson']
cs.AI,"Learning the Past Tense of English Verbs: The Symbolic Pattern
  Associator vs. Connectionist Models","Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms.",http://arxiv.org/abs/cs/9402101v1,1994-02-01T00:00:00Z,['C. X. Ling']
cs.AI,"Substructure Discovery Using Minimum Description Length and Background
  Knowledge","The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value.",http://arxiv.org/abs/cs/9402102v1,1994-02-01T00:00:00Z,"['D. J. Cook', 'L. B. Holder']"
cs.AI,Bias-Driven Revision of Logical Domain Theories,"The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.",http://arxiv.org/abs/cs/9402103v1,1994-02-01T00:00:00Z,"['M. Koppel', 'R. Feldman', 'A. M. Segre']"
cs.AI,"Exploring the Decision Forest: An Empirical Investigation of Occam's
  Razor in Decision Tree Induction","We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees.",http://arxiv.org/abs/cs/9403101v1,1994-03-01T00:00:00Z,"['P. M. Murphy', 'M. J. Pazzani']"
cs.AI,"A Semantics and Complete Algorithm for Subsumption in the CLASSIC
  Description Logic","This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC, a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions, the developers have had to use an
algorithm that is incomplete with respect to the standard, model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete, and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs,
which are an abstracted version of the implementation structures used in
CLASSIC, and are of independent interest.",http://arxiv.org/abs/cs/9406101v1,1994-06-01T00:00:00Z,"['A. Borgida', 'P. F. Patel-Schneider']"
cs.AI,Applying GSAT to Non-Clausal Formulas,"In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far.",http://arxiv.org/abs/cs/9406102v1,1994-06-01T00:00:00Z,['R. Sebastiani']
cs.AI,Random Worlds and Maximum Entropy,"Given a knowledge base KB containing first-order and statistical facts, we
consider a principled method, called the random-worlds method, for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals, then we can consider all
possible worlds, or first-order models, with domain {1,...,N} that satisfy KB,
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only, we can naturally associate an entropy with each world. As N
grows larger, there are many more worlds with higher entropy. Therefore, we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence,
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly, the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general, the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods.",http://arxiv.org/abs/cs/9408101v1,1994-08-01T00:00:00Z,"['A. J. Grove', 'J. Y. Halpern', 'D. Koller']"
cs.AI,"Pattern Matching and Discourse Processing in Information Extraction from
  Japanese Text","Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First, sentence level processing locates relevant
pieces of information scattered throughout the text; second, discourse
processing merges coreferential information to generate the output. In the
first step, pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step, partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format, complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance.",http://arxiv.org/abs/cs/9408102v1,1994-08-01T00:00:00Z,"['T. Kitani', 'Y. Eriguchi', 'M. Hara']"
cs.AI,A System for Induction of Oblique Decision Trees,"This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees.",http://arxiv.org/abs/cs/9408103v1,1994-08-01T00:00:00Z,"['S. K. Murthy', 'S. Kasif', 'S. Salzberg']"
cs.AI,On Planning while Learning,"This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner.",http://arxiv.org/abs/cs/9409101v1,1994-09-01T00:00:00Z,"['S. Safra', 'M. Tennenholtz']"
cs.AI,Wrap-Up: a Trainable Discourse Module for Information Extraction,"The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text,
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging, lexical disambiguation, and dictionary construction.
Wrap-Up is fully trainable, and not only automatically decides what classifiers
are needed, but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain.",http://arxiv.org/abs/cs/9412101v1,1994-12-01T00:00:00Z,"['S. Soderland', 'Lehnert. W']"
cs.AI,Operations for Learning with Graphical Models,"This paper is a multidisciplinary review of empirical, statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks, directed graphs representing a Markov chain, and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition, differentiation, and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework: Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas, some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression, techniques for feed-forward networks,
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms.",http://arxiv.org/abs/cs/9412102v1,1994-12-01T00:00:00Z,['W. L. Buntine']
cs.AI,Total-Order and Partial-Order Planning: A Comparative Analysis,"For many years, the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper, we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners.",http://arxiv.org/abs/cs/9412103v1,1994-12-01T00:00:00Z,"['S. Minton', 'J. Bresina', 'M. Drummond']"
cs.CL,Linear Segmentation and Segment Significance,"We present a new method for discovering a segmental discourse structure of a
document while categorizing segment function. We demonstrate how retrieval of
noun phrases and pronominal forms, along with a zero-sum weighting scheme,
determines topicalized segmentation. Futhermore, we use term distribution to
aid in identifying the role that the segment performs in the document. Finally,
we present results of evaluation in terms of precision and recall which surpass
earlier approaches.",http://arxiv.org/abs/cs/9809020v1,1998-09-15T23:49:32Z,"['Min-Yen Kan', 'Judith L. Klavans', 'Kathleen R. McKeown']"
cs.CL,"Modelling Users, Intentions, and Structure in Spoken Dialog","We outline how utterances in dialogs can be interpreted using a partial first
order logic. We exploit the capability of this logic to talk about the truth
status of formulae to define a notion of coherence between utterances and
explain how this coherence relation can serve for the construction of AND/OR
trees that represent the segmentation of the dialog. In a BDI model we
formalize basic assumptions about dialog and cooperative behaviour of
participants. These assumptions provide a basis for inferring speech acts from
coherence relations between utterances and attitudes of dialog participants.
Speech acts prove to be useful for determining dialog segments defined on the
notion of completing expectations of dialog participants. Finally, we sketch
how explicit segmentation signalled by cue phrases and performatives is covered
by our dialog model.",http://arxiv.org/abs/cs/9809022v1,1998-09-17T11:10:14Z,"['Bernd Ludwig', 'Guenther Goerz', 'Heinrich Niemann']"
cs.CL,A Lexicalized Tree Adjoining Grammar for English,"This document describes a sizable grammar of English written in the TAG
formalism and implemented for use with the XTAG system. This report and the
grammar described herein supersedes the TAG grammar described in an earlier
1995 XTAG technical report. The English grammar described in this report is
based on the TAG formalism which has been extended to include lexicalization,
and unification-based feature structures. The range of syntactic phenomena that
can be handled is large and includes auxiliaries (including inversion), copula,
raising and small clause constructions, topicalization, relative clauses,
infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO
constructions, noun-noun modifications, extraposition, determiner sequences,
genitives, negation, noun-verb contractions, sentential adjuncts and
imperatives. This technical report corresponds to the XTAG Release 8/31/98. The
XTAG grammar is continuously updated with the addition of new analyses and
modification of old ones, and an online version of this report can be found at
the XTAG web page at http://www.cis.upenn.edu/~xtag/",http://arxiv.org/abs/cs/9809024v2,1998-09-18T00:33:47Z,['XTAG Research Group']
cs.CL,Prefix Probabilities from Stochastic Tree Adjoining Grammars,"Language models for speech recognition typically use a probability model of
the form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other
hand, are typically used to assign structure to utterances. A language model of
the above form is constructed from such grammars by computing the prefix
probability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all
possible terminations of the prefix a_1 ... a_n. The main result in this paper
is an algorithm to compute such prefix probabilities given a stochastic Tree
Adjoining Grammar (TAG). The algorithm achieves the required computation in
O(n^6) time. The probability of subderivations that do not derive any words in
the prefix, but contribute structurally to its derivation, are precomputed to
achieve termination. This algorithm enables existing corpus-based estimation
techniques for stochastic TAGs to be used for language modelling.",http://arxiv.org/abs/cs/9809026v1,1998-09-18T03:45:45Z,"['Mark-Jan Nederhof', 'Anoop Sarkar', 'Giorgio Satta']"
cs.CL,Conditions on Consistency of Probabilistic Tree Adjoining Grammars,"Much of the power of probabilistic methods in modelling language comes from
their ability to compare several derivations for the same string in the
language. An important starting point for the study of such cross-derivational
properties is the notion of _consistency_. The probability model defined by a
probabilistic grammar is said to be _consistent_ if the probabilities assigned
to all the strings in the language sum to one. From the literature on
probabilistic context-free grammars (CFGs), we know precisely the conditions
which ensure that consistency is true for a given CFG. This paper derives the
conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can
be shown to be consistent. It gives a simple algorithm for checking consistency
and gives the formal justification for its correctness. The conditions derived
here can be used to ensure that probability models that use TAGs can be checked
for _deficiency_ (i.e. whether any probability mass is assigned to strings that
cannot be generated).",http://arxiv.org/abs/cs/9809027v1,1998-09-18T03:58:57Z,['Anoop Sarkar']
cs.CL,Separating Dependency from Constituency in a Tree Rewriting System,"In this paper we present a new tree-rewriting formalism called Link-Sharing
Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using
LSTAG we define an approach towards coordination where linguistic dependency is
distinguished from the notion of constituency. Such an approach towards
coordination that explicitly distinguishes dependencies from constituency gives
a better formal understanding of its representation when compared to previous
approaches that use tree-rewriting systems which conflate the two issues.",http://arxiv.org/abs/cs/9809028v1,1998-09-18T04:44:02Z,['Anoop Sarkar']
cs.CL,Incremental Parser Generation for Tree Adjoining Grammars,"This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far.",http://arxiv.org/abs/cs/9809029v1,1998-09-18T05:03:48Z,['Anoop Sarkar']
cs.CL,"A Freely Available Morphological Analyzer, Disambiguator and Context
  Sensitive Lemmatizer for German","In this paper we present Morphy, an integrated tool for German morphology,
part-of-speech tagging and context-sensitive lemmatization. Its large lexicon
of more than 320,000 word forms plus its ability to process German compound
nouns guarantee a wide morphological coverage. Syntactic ambiguities can be
resolved with a standard statistical part-of-speech tagger. By using the output
of the tagger, the lemmatizer can determine the correct root even for ambiguous
word forms. The complete package is freely available and can be downloaded from
the World Wide Web.",http://arxiv.org/abs/cs/9809050v1,1998-09-23T12:59:39Z,"['Wolfgang Lezius', 'Reinhard Rapp', 'Manfred Wettler']"
cs.CL,Processing Unknown Words in HPSG,"The lexical acquisition system presented in this paper incrementally updates
linguistic properties of unknown words inferred from their surrounding context
by parsing sentences with an HPSG grammar for German. We employ a gradual,
information-based concept of ``unknownness'' providing a uniform treatment for
the range of completely known to maximally unknown lexical entries. ``Unknown''
information is viewed as revisable information, which is either generalizable
or specializable. Updating takes place after parsing, which only requires a
modified lexical lookup. Revisable pieces of information are identified by
grammar-specified declarations which provide access paths into the parse
feature structure. The updating mechanism revises the corresponding places in
the lexical feature structures iff the context actually provides new
information. For revising generalizable information, type union is required. A
worked-out example demonstrates the inferential capacity of our implemented
system.",http://arxiv.org/abs/cs/9809106v1,1998-09-25T11:02:08Z,"['Petra Barg', 'Markus Walther']"
cs.CL,Computing Declarative Prosodic Morphology,"This paper describes a computational, declarative approach to prosodic
morphology that uses inviolable constraints to denote small finite candidate
sets which are filtered by a restrictive incremental optimization mechanism.
The new approach is illustrated with an implemented fragment of Modern Hebrew
verbs couched in MicroCUF, an expressive constraint logic formalism. For
generation and parsing of word forms, I propose a novel off-line technique to
eliminate run-time optimization. It produces a finite-state oracle that
efficiently restricts the constraint interpreter's search space. As a
byproduct, unknown words can be analyzed without special mechanisms. Unlike
pure finite-state transducer approaches, this hybrid setup allows for more
expressivity in constraints to specify e.g. token identity for reduplication or
arithmetic constraints for phonetics.",http://arxiv.org/abs/cs/9809107v1,1998-09-25T14:32:38Z,['Markus Walther']
cs.CL,"On the Evaluation and Comparison of Taggers: The Effect of Noise in
  Testing Corpora","This paper addresses the issue of {\sc pos} tagger evaluation. Such
evaluation is usually performed by comparing the tagger output with a reference
test corpus, which is assumed to be error-free. Currently used corpora contain
noise which causes the obtained performance to be a distortion of the real
value. We analyze to what extent this distortion may invalidate the comparison
between taggers or the measure of the improvement given by a new system. The
main conclusion is that a more rigorous testing experimentation
setting/designing is needed to reliably evaluate and compare tagger accuracies.",http://arxiv.org/abs/cs/9809112v1,1998-09-28T07:49:11Z,"['L. Padro', 'L. Marquez']"
cs.CL,Improving Tagging Performance by Using Voting Taggers,"We present a bootstrapping method to develop an annotated corpus, which is
specially useful for languages with few available resources. The method is
being applied to develop a corpus of Spanish of over 5Mw. The method consists
on taking advantage of the collaboration of two different POS taggers. The
cases in which both taggers agree present a higher accuracy and are used to
retrain the taggers.",http://arxiv.org/abs/cs/9809113v1,1998-09-28T07:50:55Z,"['L. Marquez', 'L. Padro', 'H. Rodriguez']"
cs.CL,Resources for Evaluation of Summarization Techniques,"We report on two corpora to be used in the evaluation of component systems
for the tasks of (1) linear segmentation of text and (2) summary-directed
sentence extraction. We present characteristics of the corpora, methods used in
the collection of user judgments, and an overview of the application of the
corpora to evaluating the component system. Finally, we discuss the problems
and issues with construction of the test set which apply broadly to the
construction of evaluation resources for language technologies.",http://arxiv.org/abs/cs/9810014v1,1998-10-13T20:33:05Z,"['Judith L. Klavans', 'Kathleen R. McKeown', 'Min-Yen Kan', 'Susan Lee']"
cs.CL,Restrictions on Tree Adjoining Languages,"Several methods are known for parsing languages generated by Tree Adjoining
Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate
which restrictions on TAGs and TAG derivations are needed in order to lower
this O(n^6) time complexity, without introducing large runtime constants, and
without losing any of the generative power needed to capture the syntactic
constructions in natural language that can be handled by unrestricted TAGs. In
particular, we describe an algorithm for parsing a strict subclass of TAG in
O(n^5), and attempt to show that this subclass retains enough generative power
to make it useful in the general case.",http://arxiv.org/abs/cs/9810015v1,1998-10-13T21:17:13Z,"['Giorgio Satta', 'William Schuler']"
cs.CL,"Translating near-synonyms: Possibilities and preferences in the
  interlingua","This paper argues that an interlingual representation must explicitly
represent some parts of the meaning of a situation as possibilities (or
preferences), not as necessary or definite components of meaning (or
constraints). Possibilities enable the analysis and generation of nuance,
something required for faithful translation. Furthermore, the representation of
the meaning of words, especially of near-synonyms, is crucial, because it
specifies which nuances words can convey in which contexts.",http://arxiv.org/abs/cs/9811008v1,1998-11-02T21:29:41Z,['Philip Edmonds']
cs.CL,"Choosing the Word Most Typical in Context Using a Lexical Co-occurrence
  Network","This paper presents a partial solution to a component of the problem of
lexical choice: choosing the synonym most typical, or expected, in context. We
apply a new statistical approach to representing the context of a word through
lexical co-occurrence networks. The implementation was trained and evaluated on
a large corpus, and results show that the inclusion of second-order
co-occurrence relations improves the performance of our implemented lexical
choice program.",http://arxiv.org/abs/cs/9811009v1,1998-11-02T23:06:19Z,['Philip Edmonds']
cs.CL,Comparing a statistical and a rule-based tagger for German,"In this paper we present the results of comparing a statistical tagger for
German based on decision trees and a rule-based Brill-Tagger for German. We
used the same training corpus (and therefore the same tag-set) to train both
taggers. We then applied the taggers to the same test corpus and compared their
respective behavior and in particular their error rates. Both taggers perform
similarly with an error rate of around 5%. From the detailed error analysis it
can be seen that the rule-based tagger has more problems with unknown words
than the statistical tagger. But the results are opposite for tokens that are
many-ways ambiguous. If the unknown words are fed into the taggers with the
help of an external lexicon (such as the Gertwol system) the error rate of the
rule-based tagger drops to 4.7%, and the respective rate of the statistical
taggers drops to around 3.7%. Combining the taggers by using the output of one
tagger to help the other did not lead to any further improvement.",http://arxiv.org/abs/cs/9811016v1,1998-11-11T11:06:34Z,"['Martin Volk', 'Gerold Schneider']"
cs.CL,Expoiting Syntactic Structure for Language Modeling,"The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words--binary-parse-structure with headword annotation and
operates in a left-to-right manner --- therefore usable for automatic speech
recognition. The model, its probabilistic parameterization, and a set of
experiments meant to evaluate its predictive power are presented; an
improvement over standard trigram modeling is achieved.",http://arxiv.org/abs/cs/9811022v2,1998-11-12T17:31:17Z,"['Ciprian Chelba', 'Frederick Jelinek']"
cs.CL,A Structured Language Model,"The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words - binary-parse-structure with headword annotation. The
model, its probabilistic parametrization, and a set of experiments meant to
evaluate its predictive power are presented.",http://arxiv.org/abs/cs/9811025v2,1998-11-13T16:53:15Z,['Ciprian Chelba']
cs.CL,"A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S
  tructural Disambiguation","In this thesis, I address the problem of automatically acquiring lexical
semantic knowledge, especially that of case frame patterns, from large corpus
data and using the acquired knowledge in structural disambiguation. The
approach I adopt has the following characteristics: (1) dividing the problem
into three subproblems: case slot generalization, case dependency learning, and
word clustering (thesaurus construction). (2) viewing each subproblem as that
of statistical estimation and defining probability models for each subproblem,
(3) adopting the Minimum Description Length (MDL) principle as learning
strategy, (4) employing efficient learning algorithms, and (5) viewing the
disambiguation problem as that of statistical prediction. Major contributions
of this thesis include: (1) formalization of the lexical knowledge acquisition
problem, (2) development of a number of learning methods for lexical knowledge
acquisition, and (3) development of a high-performance disambiguation method.",http://arxiv.org/abs/cs/9812001v3,1998-12-01T11:43:32Z,['Hang LI']
stat.ML,Lasso type classifiers with a reject option,"We consider the problem of binary classification where one can, for a
particular cost, choose not to classify an observation. We present a simple
proof for the oracle inequality for the excess risk of structural risk
minimizers using a lasso type penalty.",http://arxiv.org/abs/0705.2363v1,2007-05-16T14:23:17Z,['Marten Wegkamp']
stat.ML,Metric Embedding for Nearest Neighbor Classification,"The distance metric plays an important role in nearest neighbor (NN)
classification. Usually the Euclidean distance metric is assumed or a
Mahalanobis distance metric is optimized to improve the NN performance. In this
paper, we study the problem of embedding arbitrary metric spaces into a
Euclidean space with the goal to improve the accuracy of the NN classifier. We
propose a solution by appealing to the framework of regularization in a
reproducing kernel Hilbert space and prove a representer-like theorem for NN
classification. The embedding function is then determined by solving a
semidefinite program which has an interesting connection to the soft-margin
linear binary support vector machine classifier. Although the main focus of
this paper is to present a general, theoretical framework for metric embedding
in a NN setting, we demonstrate the performance of the proposed method on some
benchmark datasets and show that it performs better than the Mahalanobis metric
learning algorithm in terms of leave-one-out and generalization errors.",http://arxiv.org/abs/0706.3499v1,2007-06-24T06:50:24Z,"['Bharath K. Sriperumbudur', 'Gert R. G. Lanckriet']"
stat.ML,Degenerating families of dendrograms,"Dendrograms used in data analysis are ultrametric spaces, hence objects of
nonarchimedean geometry. It is known that there exist $p$-adic representation
of dendrograms. Completed by a point at infinity, they can be viewed as
subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.
The implications are that certain moduli spaces known in algebraic geometry are
$p$-adic parameter spaces of (families of) dendrograms, and stochastic
classification can also be handled within this framework. At the end, we
calculate the topology of the hidden part of a dendrogram.",http://arxiv.org/abs/0707.3536v1,2007-07-24T12:45:39Z,['Patrick Erik Bradley']
stat.ML,Families of dendrograms,"A conceptual framework for cluster analysis from the viewpoint of p-adic
geometry is introduced by describing the space of all dendrograms for n
datapoints and relating it to the moduli space of p-adic Riemannian spheres
with punctures using a method recently applied by Murtagh (2004b). This method
embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the
p-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.
After explaining the definitions, the concept of classifiers is discussed in
the context of moduli spaces, and upper bounds for the number of hidden
vertices in dendrograms are given.",http://arxiv.org/abs/0707.4072v1,2007-07-27T09:37:28Z,['Patrick Erik Bradley']
stat.ML,Online Learning in Discrete Hidden Markov Models,"We present and analyse three online algorithms for learning in discrete
Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.
Using the Kullback-Leibler divergence as a measure of generalisation error we
draw learning curves in simplified situations. The performance for learning
drifting concepts of one of the presented algorithms is analysed and compared
with the Baldi-Chauvin algorithm in the same situations. A brief discussion
about learning and symmetry breaking based on our results is also presented.",http://arxiv.org/abs/0708.2377v1,2007-08-17T14:41:58Z,"['Roberto C. Alamino', 'Nestor Caticha']"
stat.ML,Supervised Machine Learning with a Novel Kernel Density Estimator,"In recent years, kernel density estimation has been exploited by computer
scientists to model machine learning problems. The kernel density estimation
based approaches are of interest due to the low time complexity of either O(n)
or O(n*log(n)) for constructing a classifier, where n is the number of sampling
instances. Concerning design of kernel density estimators, one essential issue
is how fast the pointwise mean square error (MSE) and/or the integrated mean
square error (IMSE) diminish as the number of sampling instances increases. In
this article, it is shown that with the proposed kernel function it is feasible
to make the pointwise MSE of the density estimator converge at O(n^-2/3)
regardless of the dimension of the vector space, provided that the probability
density function at the point of interest meets certain conditions.",http://arxiv.org/abs/0709.2760v3,2007-09-18T06:45:30Z,"['Yen-Jen Oyang', 'Darby Tien-Hao Chang', 'Yu-Yen Ou', 'Hao-Geng Hung', 'Chih-Peng Wu', 'Chien-Yu Chen']"
stat.ML,Bayesian Classification and Regression with High Dimensional Features,"This thesis responds to the challenges of using a large number, such as
thousands, of features in regression and classification problems.
  There are two situations where such high dimensional features arise. One is
when high dimensional measurements are available, for example, gene expression
data produced by microarray techniques. For computational or other reasons,
people may select only a small subset of features when modelling such data, by
looking at how relevant the features are to predicting the response, based on
some measure such as correlation with the response in the training data.
Although it is used very commonly, this procedure will make the response appear
more predictable than it actually is. In Chapter 2, we propose a Bayesian
method to avoid this selection bias, with application to naive Bayes models and
mixture models.
  High dimensional features also arise when we consider high-order
interactions. The number of parameters will increase exponentially with the
order considered. In Chapter 3, we propose a method for compressing a group of
parameters into a single one, by exploiting the fact that many predictor
variables derived from high-order interactions have the same values for all the
training cases. The number of compressed parameters may have converged before
considering the highest possible order. We apply this compression method to
logistic sequence prediction models and logistic classification models.
  We use both simulated data and real data to test our methods in both
chapters.",http://arxiv.org/abs/0709.2936v1,2007-09-18T23:56:17Z,['Longhai Li']
stat.ML,"Simulated Annealing: Rigorous finite-time guarantees for optimization on
  continuous domains","Simulated annealing is a popular method for approaching the solution of a
global optimization problem. Existing results on its performance apply to
discrete combinatorial optimization where the optimization variables can assume
only a finite set of possible values. We introduce a new general formulation of
simulated annealing which allows one to guarantee finite-time performance in
the optimization of functions of continuous variables. The results hold
universally for any optimization problem on a bounded domain and establish a
connection between simulated annealing and up-to-date theory of convergence of
Markov chain Monte Carlo methods on continuous domains. This work is inspired
by the concept of finite-time learning with known accuracy and confidence
developed in statistical learning theory.",http://arxiv.org/abs/0709.2989v1,2007-09-19T10:56:13Z,"['A. Lecchini-Visintini', 'J. Lygeros', 'J. Maciejowski']"
stat.ML,"The nested Chinese restaurant process and Bayesian nonparametric
  inference of topic hierarchies","We present the nested Chinese restaurant process (nCRP), a stochastic process
which assigns probability distributions to infinitely-deep,
infinitely-branching trees. We show how this stochastic process can be used as
a prior distribution in a Bayesian nonparametric model of document collections.
Specifically, we present an application to information retrieval in which
documents are modeled as paths down a random tree, and the preferential
attachment dynamics of the nCRP leads to clustering of documents according to
sharing of topics at multiple levels of abstraction. Given a corpus of
documents, a posterior inference algorithm finds an approximation to a
posterior distribution over trees, topics and allocations of words to levels of
the tree. We demonstrate this algorithm on collections of scientific abstracts
from several journals. This model exemplifies a recent trend in statistical
machine learning--the use of Bayesian nonparametric methods to infer
distributions on flexible data structures.",http://arxiv.org/abs/0710.0845v3,2007-10-03T17:32:21Z,"['David M. Blei', 'Thomas L. Griffiths', 'Michael I. Jordan']"
stat.ML,Probabilistic coherence and proper scoring rules,"We provide self-contained proof of a theorem relating probabilistic coherence
of forecasts to their non-domination by rival forecasts with respect to any
proper scoring rule. The theorem appears to be new but is closely related to
results achieved by other investigators.",http://arxiv.org/abs/0710.3183v1,2007-10-16T21:16:29Z,"['Joel Predd', 'Robert Seiringer', 'Elliott H. Lieb', 'Daniel Osherson', 'Vincent Poor', 'Sanjeev Kulkarni']"
stat.ML,Bayesian Online Changepoint Detection,"Changepoints are abrupt variations in the generative parameters of a data
sequence. Online detection of changepoints is useful in modelling and
prediction of time series in application areas such as finance, biometrics, and
robotics. While frequentist methods have yielded online filtering and
prediction techniques, most Bayesian papers have focused on the retrospective
segmentation problem. Here we examine the case where the model parameters
before and after the changepoint are independent and we derive an online
algorithm for exact inference of the most recent changepoint. We compute the
probability distribution of the length of the current ``run,'' or time since
the last changepoint, using a simple message-passing algorithm. Our
implementation is highly modular so that the algorithm may be applied to a
variety of types of data. We illustrate this modularity by demonstrating the
algorithm on three different real-world data sets.",http://arxiv.org/abs/0710.3742v1,2007-10-19T17:18:30Z,"['Ryan Prescott Adams', 'David J. C. MacKay']"
stat.ML,Variable importance in binary regression trees and forests,"We characterize and study variable importance (VIMP) and pairwise variable
associations in binary regression trees. A key component involves the node mean
squared error for a quantity we refer to as a maximal subtree. The theory
naturally extends from single trees to ensembles of trees and applies to
methods like random forests. This is useful because while importance values
from random forests are used to screen variables, for example they are used to
filter high throughput genomic data in Bioinformatics, very little theory
exists about their properties.",http://arxiv.org/abs/0711.2434v1,2007-11-15T15:09:41Z,['Hemant Ishwaran']
stat.ML,"Pac-Bayesian Supervised Classification: The Thermodynamics of
  Statistical Learning","This monograph deals with adaptive supervised classification, using tools
borrowed from statistical mechanics and information theory, stemming from the
PACBayesian approach pioneered by David McAllester and applied to a conception
of statistical learning theory forged by Vladimir Vapnik. Using convex analysis
on the set of posterior probability measures, we show how to get local measures
of the complexity of the classification model involving the relative entropy of
posterior distributions with respect to Gibbs posterior measures. We then
discuss relative bounds, comparing the generalization error of two
classification rules, showing how the margin assumption of Mammen and Tsybakov
can be replaced with some empirical measure of the covariance structure of the
classification model.We show how to associate to any posterior distribution an
effective temperature relating it to the Gibbs prior distribution with the same
level of expected error rate, and how to estimate this effective temperature
from data, resulting in an estimator whose expected error rate converges
according to the best possible power of the sample size adaptively under any
margin and parametric complexity assumptions. We describe and study an
alternative selection scheme based on relative bounds between estimators, and
present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and
use this to improve Vapnik's generalization bounds, extending them to the case
when the sample is made of independent non-identically distributed pairs of
patterns and labels. Finally we review briefly the construction of Support
Vector Machines and show how to derive generalization bounds for them,
measuring the complexity either through the number of support vectors or
through the value of the transductive or inductive margin.",http://arxiv.org/abs/0712.0248v1,2007-12-03T13:49:36Z,['Olivier Catoni']
stat.ML,Classification Constrained Dimensionality Reduction,"Dimensionality reduction is a topic of recent interest. In this paper, we
present the classification constrained dimensionality reduction (CCDR)
algorithm to account for label information. The algorithm can account for
multiple classes as well as the semi-supervised setting. We present an
out-of-sample expressions for both labeled and unlabeled data. For unlabeled
data, we introduce a method of embedding a new point as preprocessing to a
classifier. For labeled data, we introduce a method that improves the embedding
during the training phase using the out-of-sample extension. We investigate
classification performance using the CCDR algorithm on hyper-spectral satellite
imagery data. We demonstrate the performance gain for both local and global
classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors
algorithm performance. We present a connection between intrinsic dimension
estimation and the optimal embedding dimension obtained using the CCDR
algorithm.",http://arxiv.org/abs/0802.2906v2,2008-02-20T18:26:31Z,"['Raviv Raich', 'Jose A. Costa', 'Steven B. Damelin', 'Alfred O. Hero III']"
stat.ML,Component models for large networks,"Being among the easiest ways to find meaningful structure from discrete data,
Latent Dirichlet Allocation (LDA) and related component models have been
applied widely. They are simple, computationally fast and scalable,
interpretable, and admit nonparametric priors. In the currently popular field
of network modeling, relatively little work has taken uncertainty of data
seriously in the Bayesian sense, and component models have been introduced to
the field only recently, by treating each node as a bag of out-going links. We
introduce an alternative, interaction component model for communities (ICMc),
where the whole network is a bag of links, stemming from different components.
The former finds both disassortative and assortative structure, while the
alternative assumes assortativity and finds community-like structures like the
earlier methods motivated by physics. With Dirichlet Process priors and an
efficient implementation the models are highly scalable, as demonstrated with a
social network from the Last.fm web site, with 670,000 nodes and 1.89 million
links.",http://arxiv.org/abs/0803.1628v1,2008-03-11T18:38:52Z,"['Janne Sinkkonen', 'Janne Aukia', 'Samuel Kaski']"
stat.ML,Testing for Homogeneity with Kernel Fisher Discriminant Analysis,"We propose to investigate test statistics for testing homogeneity in
reproducing kernel Hilbert spaces. Asymptotic null distributions under null
hypothesis are derived, and consistency against fixed and local alternatives is
assessed. Finally, experimental evidence of the performance of the proposed
approach on both artificial data and a speaker verification task is provided.",http://arxiv.org/abs/0804.1026v1,2008-04-07T13:46:27Z,"['Zaid Harchaoui', 'Francis Bach', 'Eric Moulines']"
stat.ML,"On the underestimation of model uncertainty by Bayesian K-nearest
  neighbors","When using the K-nearest neighbors method, one often ignores uncertainty in
the choice of K. To account for such uncertainty, Holmes and Adams (2002)
proposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN
(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain
Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams
(2002) focused on the performance of BKNN in terms of misclassification error
but did not assess its ability to quantify uncertainty. We present some
evidence to show that BKNN still significantly underestimates model
uncertainty.",http://arxiv.org/abs/0804.1325v1,2008-04-08T16:58:11Z,"['Wanhua Su', 'Hugh Chipman', 'Mu Zhu']"
stat.ML,"Information Preserving Component Analysis: Data Projections for Flow
  Cytometry Analysis","Flow cytometry is often used to characterize the malignant cells in leukemia
and lymphoma patients, traced to the level of the individual cell. Typically,
flow cytometric data analysis is performed through a series of 2-dimensional
projections onto the axes of the data set. Through the years, clinicians have
determined combinations of different fluorescent markers which generate
relatively known expression patterns for specific subtypes of leukemia and
lymphoma -- cancers of the hematopoietic system. By only viewing a series of
2-dimensional projections, the high-dimensional nature of the data is rarely
exploited. In this paper we present a means of determining a low-dimensional
projection which maintains the high-dimensional relationships (i.e.
information) between differing oncological data sets. By using machine learning
techniques, we allow clinicians to visualize data in a low dimension defined by
a linear combination of all of the available markers, rather than just 2 at a
time. This provides an aid in diagnosing similar forms of cancer, as well as a
means for variable selection in exploratory flow cytometric research. We refer
to our method as Information Preserving Component Analysis (IPCA).",http://arxiv.org/abs/0804.2848v1,2008-04-17T16:25:48Z,"['Kevin M. Carter', 'Raviv Raich', 'William G. Finn', 'Alfred O. Hero III']"
stat.ML,Random projection trees for vector quantization,"A simple and computationally efficient scheme for tree-structured vector
quantization is presented. Unlike previous methods, its quantization error
depends only on the intrinsic dimension of the data distribution, rather than
the apparent dimension of the space in which the data happen to lie.",http://arxiv.org/abs/0805.1390v1,2008-05-09T17:52:42Z,"['Sanjoy Dasgupta', 'Yoav Freund']"
stat.ML,Manifold Learning: The Price of Normalization,"We analyze the performance of a class of manifold-learning algorithms that
find their output by minimizing a quadratic form under some normalization
constraints. This class consists of Locally Linear Embedding (LLE), Laplacian
Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and
Diffusion maps. We present and prove conditions on the manifold that are
necessary for the success of the algorithms. Both the finite sample case and
the limit case are analyzed. We show that there are simple manifolds in which
the necessary conditions are violated, and hence the algorithms cannot recover
the underlying manifolds. Finally, we present numerical results that
demonstrate our claims.",http://arxiv.org/abs/0806.2646v1,2008-06-16T19:54:49Z,"['Y. Goldberg', 'A. Zakai', 'D. Kushnir', 'Y. Ritov']"
